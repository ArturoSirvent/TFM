{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#librerias \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "import os \n",
    "import re \n",
    "\n",
    "#import model\n",
    "import sys \n",
    "sys.path.append(\"../../1.1-Break_pruebasTranformer/\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from model import AnomalyTransformer\n",
    "import gc\n",
    "from torch import optim\n",
    "\n",
    "from torch.utils.data import DataLoader,Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DATA_DIR=\"../data2apply/\"\n",
    "data_dir=os.path.join(BASE_DATA_DIR,\"pump_data\")\n",
    "train_dir=os.path.join(data_dir,\"sensor.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(train_dir,index_col=0)\n",
    "df[\"machine_status\"]=df[\"machine_status\"].map({\"NORMAL\":0,\"RECOVERING\":-1,\"BROKEN\":-2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset de los datos \n",
    "\n",
    "class PumpData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#código del modelo\n",
    "\n",
    "\n",
    "class AnomalyModel:\n",
    "    def __init__(self, AnomalyTransformer, dataset, batch_size=16,window_size=100,enc_in=1,enc_out=1, d_model=64, n_heads=2, e_layers=2, d_ff=32,\n",
    "                 dropout=0.1, activation='relu',  lambda_=1e-3,max_norm=0.1,norm_type=2):\n",
    "        self.model = AnomalyTransformer(window_size, enc_in, enc_out, d_model, n_heads, e_layers, d_ff, dropout, activation, output_attention=True)\n",
    "        self.model.cuda()\n",
    "        self.N=e_layers\n",
    "        self.model = self.xavier_initialization(self.model)\n",
    "        self.dataset=dataset\n",
    "        self.dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        self.lambda_ = lambda_\n",
    "        self.max_norm=max_norm\n",
    "        self.norm_type=norm_type\n",
    "\n",
    "    @staticmethod\n",
    "    def xavier_initialization(model):\n",
    "        for module in model.modules():\n",
    "            if hasattr(module, 'weight') and module.weight is not None and module.weight.dim() >= 2:\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if hasattr(module, 'bias') and module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def my_kl_loss(p, q):\n",
    "        res = p * (torch.log(p + 0.0001) - torch.log(q + 0.0001))\n",
    "        return torch.mean(torch.sum(res, dim=-1), dim=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def layer_association_discrepancy(Pl, Sl):\n",
    "        B, Head, Height, _ = Pl.shape\n",
    "        kl1 = AnomalyModel.my_kl_loss(Pl.view(B, Head, Height, -1), Sl.view(B, Head, Height, -1))\n",
    "        kl2 = AnomalyModel.my_kl_loss(Sl.view(B, Head, Height, -1), Pl.view(B, Head, Height, -1))\n",
    "        ad_vector = kl1 + kl2\n",
    "        return ad_vector\n",
    "\n",
    "    @staticmethod\n",
    "    def association_discrepancy(P_list, S_list):\n",
    "        return torch.stack([AnomalyModel.layer_association_discrepancy(j/torch.unsqueeze(torch.sum(j, dim=-1), dim=-1).repeat(1, 1, 1, 100),i) for i, j in zip(S_list,P_list)]).mean(axis=[0])\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_function(x_hat, P_list, S_list, lambda_, x):\n",
    "        frob_norm = ((x_hat - x)**2).sum()\n",
    "        diss_norm = torch.mean(AnomalyModel.association_discrepancy(P_list, S_list))\n",
    "        print(frob_norm, diss_norm)\n",
    "        return (frob_norm.item(), diss_norm.item()), frob_norm - (lambda_ * diss_norm)\n",
    "\n",
    "    @staticmethod\n",
    "    def min_loss(output, P_layers, S_layers, x, lambda_):\n",
    "        P_list = P_layers\n",
    "        S_list = [S.detach() for S in S_layers]\n",
    "        lambda_ = -lambda_\n",
    "        _, loss_value = AnomalyModel.loss_function(output, P_list, S_list, lambda_, x)\n",
    "        return loss_value\n",
    "\n",
    "    @staticmethod\n",
    "    def max_loss(output, P_layers, S_layers, x, lambda_):\n",
    "        P_list = [P.detach() for P in P_layers]\n",
    "        S_list = S_layers\n",
    "        lambda_ = lambda_\n",
    "        details, loss_value = AnomalyModel.loss_function(output, P_list, S_list, lambda_, x)\n",
    "        return details, loss_value\n",
    "    \n",
    "    def cosine_lr_schedule_with_warmup(self, optimizer, epoch, initial_lr, total_epochs, warmup_epochs):\n",
    "        if epoch < warmup_epochs:\n",
    "            lr = initial_lr * (epoch + 1) / warmup_epochs\n",
    "        else:\n",
    "            adjusted_epoch = epoch - warmup_epochs\n",
    "            adjusted_total_epochs = total_epochs - warmup_epochs\n",
    "            lr = 0.5 * initial_lr * (1 + math.cos(math.pi * adjusted_epoch / adjusted_total_epochs))\n",
    "\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "\n",
    "    def clip_gradients(self):\n",
    "        \"\"\"\n",
    "        Clip gradients of the model parameters.\n",
    "\n",
    "        Args:\n",
    "            model (torch.nn.Module): The neural network model.\n",
    "            max_norm (float): The maximum allowed norm for the gradients.\n",
    "            norm_type (float): The type of the norm calculation (default: 2 for L2 norm).\n",
    "        \"\"\"\n",
    "        # Recupera todos los gradientes de los parámetros del modelo\n",
    "        gradients = [param.grad for param in self.model.parameters() if param.grad is not None]\n",
    "\n",
    "        # Calcula la norma total de los gradientes\n",
    "        total_norm = torch.norm(torch.stack([torch.norm(grad, self.norm_type) for grad in gradients]), self.norm_type)\n",
    "\n",
    "        # Calcula el factor de escalado para recortar los gradientes\n",
    "        clip_coef = self.max_norm / (total_norm + 1e-6)\n",
    "        if clip_coef < 1:\n",
    "            # Aplica el factor de escalado a los gradientes\n",
    "            for grad in gradients:\n",
    "                grad.mul_(clip_coef)\n",
    "\n",
    "\n",
    "\n",
    "    def schedule_lambda(self,epoch,num_epochs,init_lambda,final_lambda):\n",
    "        self.lambda_=(final_lambda-init_lambda)/num_epochs * epoch + init_lambda\n",
    "        \n",
    "    def train(self, num_epochs, initial_lr, warmup_epochs,init_lambda,final_lambda):\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=initial_lr)\n",
    "\n",
    "        self.model.train()\n",
    "        loss_frob_diss = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch: {epoch}\")\n",
    "            self.schedule_lambda(epoch,num_epochs,init_lambda,final_lambda)\n",
    "            for i, (inputs) in enumerate(self.dataloader):\n",
    "                inputs = inputs.float().to(\"cuda:0\")\n",
    "                outputs, series, prior, _ = self.model(inputs)\n",
    "                prior = [j / torch.unsqueeze(torch.sum(j, dim=-1), dim=-1).repeat(1, 1, 1, 100) for j in prior]\n",
    "\n",
    "\n",
    "\n",
    "                loss_min = self.min_loss(outputs, prior, series, inputs, self.lambda_)\n",
    "                loss_min.backward(retain_graph=True)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                details, loss_max = self.max_loss(outputs, prior, series, inputs, self.lambda_)\n",
    "                loss_max.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                loss_frob_diss.append(list(details))\n",
    "\n",
    "                #loss_max.backward(retain_graph=True)\n",
    "                #loss_min.backward()\n",
    "                if self.max_norm is not None:\n",
    "                    self.clip_gradients()\n",
    "                #optimizer.step()\n",
    "                #optimizer.zero_grad()\n",
    "\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(self.dataloader)}], Loss: {loss_max.item():.4f}\")\n",
    "            if warmup_epochs is not None:\n",
    "                self.cosine_lr_schedule_with_warmup(optimizer, epoch, initial_lr, num_epochs, warmup_epochs)\n",
    "\n",
    "        print(\"Entrenamiento finalizado\")\n",
    "        return np.array(loss_frob_diss)\n",
    "    \n",
    "    def predict(self, data=None):\n",
    "        if data is None:\n",
    "            data = next(iter(self.dataloader)).float().to(\"cuda:0\")\n",
    "        else:\n",
    "            data = torch.tensor(data).float()\n",
    "            data = data.to(\"cuda:0\") # .unsqueeze(0) -> esto si solo tiene un canal y no lo hemos puesto\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            out, series, prior, sigmas = self.model(data)\n",
    "\n",
    "        out = out.cpu().numpy()\n",
    "        series = [s.cpu().numpy() for s in series]\n",
    "        prior = [p.cpu().numpy() for p in prior]\n",
    "        sigmas = [sigma.cpu().numpy() for sigma in sigmas]\n",
    "\n",
    "        return data.cpu().detach().numpy(),out, series, prior, sigmas\n",
    "\n",
    "\n",
    "    def anomaly_score(self, input=None, crit_batch_size=32):\n",
    "        if input is None:\n",
    "            input = self.dataset.array\n",
    "\n",
    "        input = input.float().to(\"cuda:0\")\n",
    "        self.model.eval()\n",
    "        num_batches = int(input.size(0) / crit_batch_size) + (1 if input.size(0) % crit_batch_size != 0 else 0)\n",
    "\n",
    "        # Inicializa un tensor vacío para almacenar los resultados\n",
    "        result = torch.empty(0, device=\"cuda:0\")\n",
    "\n",
    "        for i in range(num_batches):\n",
    "            batch_start = i * crit_batch_size\n",
    "            batch_end = min((i + 1) * crit_batch_size, input.size(0))\n",
    "\n",
    "            # Procesa el lote actual\n",
    "            input_batch = input[batch_start:batch_end]\n",
    "            out, series, prior, sigmas = self.model(input_batch)\n",
    "            ad = F.softmax(\n",
    "                -AnomalyModel.association_discrepancy(prior, series), dim=0\n",
    "            )\n",
    "\n",
    "            norm = ((out - input_batch) ** 2).sum(axis=-1)\n",
    "\n",
    "            score = torch.mul(ad, norm)\n",
    "\n",
    "            # Añade el resultado del lote actual al tensor de resultado\n",
    "            result = torch.cat((result, score), dim=0)\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "\n",
    "    def evaluate(self,data):\n",
    "        #esto acepta una serie temporal, y si no se la pasamos usa la que tenia por defecto.\n",
    "        #la vamos a evaluar enventanada y por partes, y luego, vamos a hacer la media de todas las lecturas del assdiss que recojamos \n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_instance = AnomalyModel(AnomalyTransformer.AnomalyTransformer, dataset,enc_in=2,enc_out=2,max_norm=None)\n",
    "#model_instance.lambda_=schedule_lambda()\n",
    "# Parámetros de entrenamiento\n",
    "num_epochs = 40\n",
    "initial_lr = 0.001\n",
    "warmup_epochs = 7\n",
    "\n",
    "# Entrenar la instancia de la clase AnomalyModel\n",
    "\n",
    "loss_frob_diss = model_instance.train(num_epochs, initial_lr,warmup_epochs,0.001,0.01)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
