{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly benchmark UCR archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"../../1.2-HyperParameterTuning/\")\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import re\n",
    "from model import AnomalyTransformer\n",
    "import gc\n",
    "from torch import optim\n",
    "\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCRData(object):\n",
    "    def __init__(self,data_dir,stride=5,winsize=100,norm=True):\n",
    "        list_files=os.listdir(data_dir)\n",
    "        list_data_files=[ os.path.join(data_dir,i) for i in list_files]\n",
    "\n",
    "        self.indexes=[]\n",
    "        for j in list_files:   \n",
    "            self.indexes.append([int(i.split(\".\")[0]) for i in j.split(\"_\")[-3:]])\n",
    "\n",
    "        self.dir_data_files={int(i.split(\"_\")[0]):{\"path\":j,\"index\":ind} for i,j,ind in zip(list_files,list_data_files,self.indexes)}\n",
    "\n",
    "\n",
    "        self.window_size=winsize\n",
    "        self.norm=norm\n",
    "        self.stride=stride\n",
    "\n",
    "        self.train=None\n",
    "        self.start=None\n",
    "        self.end=None\n",
    "        self.ID=None\n",
    "\n",
    "        #pues lo datos no los vamos a cargar aun, lo que vamos a hacer es llamar a una funcion que los carga\n",
    "\n",
    "\n",
    "    def load_ID(self,ID):\n",
    "        #cuando llamamos a esto, cambiamos el estado interno del objeto para que tenga el conjunto de datos\n",
    "        #que queremos \n",
    "        self.current_ID=ID\n",
    "        data_aux=np.loadtxt(self.dir_data_files[ID][\"path\"])[...,np.newaxis]\n",
    "        self.train,self.start,self.end=self.dir_data_files[ID][\"index\"]\n",
    "        if self.norm:\n",
    "            scaler=StandardScaler()\n",
    "            data_aux=scaler.fit_transform(data_aux)\n",
    "        \n",
    "        \n",
    "        self.sine_wave=torch.from_numpy(data_aux) #esto es necesario porque luego lo usa otra clase\n",
    "        self.length=self.sine_wave.shape[0]\n",
    "        #y vamos a enventanar\n",
    "        aux=[]\n",
    "        for i in range(0,data_aux.shape[0]-self.window_size,self.stride):\n",
    "            aux.append(data_aux[i:(i+self.window_size)])\n",
    "        self.array=torch.tensor(np.array(aux))\n",
    "        #self.sensores_data_tensor=torch.from_numpy(self.sensores_data_array)\n",
    "        self.list_anomalies=[list(range(self.start,self.end))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.array.shape[0]\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return self.array[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR=\"..\"\n",
    "data_dir=os.path.join(BASE_DIR,\"data2apply/UCR_dataAnomalyArchive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=UCRData(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 229,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 246,\n",
       " 247,\n",
       " 248,\n",
       " 249,\n",
       " 250]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(data.dir_data_files.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.load_ID(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([79795, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sine_wave.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo el codigo de entrenamiento aqui \n",
    "class AnomalyModel:\n",
    "    def __init__(self, AnomalyTransformer,window_size=100,enc_in=1,enc_out=1, d_model=64, n_heads=3, e_layers=2, d_ff=32,\n",
    "                 dropout=0.1, activation='relu',  lambda_=1e-3,max_norm=0.1,norm_type=2,sigma_a=5,sigma_b=3,clip_sigma=\"abs\"):\n",
    "        self.model = AnomalyTransformer(window_size, enc_in, enc_out, d_model, n_heads, e_layers, d_ff, dropout, activation,\n",
    "                                        sigma_a=sigma_a,sigma_b=sigma_b,clip_sigma=clip_sigma, output_attention=True,)\n",
    "        self.model.cuda()\n",
    "        self.N=e_layers\n",
    "        self.model = self.xavier_initialization(self.model)\n",
    "        #self.dataset=dataset\n",
    "        #self.dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        self.lambda_ = lambda_\n",
    "        self.max_norm=max_norm\n",
    "        #self.batch_size=batch_size\n",
    "        self.norm_type=norm_type\n",
    "        self.window_size=window_size\n",
    "        self.sigma_a=sigma_a\n",
    "        self.sigma_b=sigma_b\n",
    "        self.clip_sigma=clip_sigma\n",
    "    @staticmethod\n",
    "    def xavier_initialization(model):\n",
    "        for module in model.modules():\n",
    "            if hasattr(module, 'weight') and module.weight is not None and module.weight.dim() >= 2:\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if hasattr(module, 'bias') and module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def my_kl_loss(p, q):\n",
    "        res = p * (torch.log(p + 0.0001) - torch.log(q + 0.0001))\n",
    "        return torch.mean(torch.sum(res, dim=-1), dim=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def layer_association_discrepancy(Pl, Sl):\n",
    "        B, Head, Height, _ = Pl.shape\n",
    "        kl1 = AnomalyModel.my_kl_loss(Pl.view(B, Head, Height, -1), Sl.view(B, Head, Height, -1))\n",
    "        kl2 = AnomalyModel.my_kl_loss(Sl.view(B, Head, Height, -1), Pl.view(B, Head, Height, -1))\n",
    "        ad_vector = kl1 + kl2\n",
    "        return ad_vector\n",
    "\n",
    "    @staticmethod\n",
    "    def association_discrepancy(P_list, S_list):\n",
    "        return torch.stack([AnomalyModel.layer_association_discrepancy(j/torch.unsqueeze(torch.sum(j, dim=-1), dim=-1).repeat(1, 1, 1, 100),i) for i, j in zip(S_list,P_list)]).mean(axis=[0])\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_function(x_hat, P_list, S_list, lambda_, x):\n",
    "        frob_norm = (torch.linalg.norm(x_hat- x,dim=(1,2))**2).mean(axis=0) #((x_hat - x)**2).sum()\n",
    "        #diss_norm = torch.mean(AnomalyModel.association_discrepancy(P_list, S_list))\n",
    "        diss_norm = (torch.abs(AnomalyModel.association_discrepancy(P_list, S_list)).mean(dim=1)).mean()\n",
    "        print(frob_norm, diss_norm)\n",
    "        return (frob_norm.item(), diss_norm.item()), frob_norm - (lambda_ * diss_norm)\n",
    "\n",
    "    @staticmethod\n",
    "    def min_loss(output, P_layers, S_layers, x, lambda_):\n",
    "        P_list = P_layers\n",
    "        S_list = [S.detach() for S in S_layers]\n",
    "        lambda_ = -lambda_\n",
    "        _, loss_value = AnomalyModel.loss_function(output, P_list, S_list, lambda_, x)\n",
    "        return loss_value\n",
    "\n",
    "    @staticmethod\n",
    "    def max_loss(output, P_layers, S_layers, x, lambda_):\n",
    "        P_list = [P.detach() for P in P_layers]\n",
    "        S_list = S_layers\n",
    "        lambda_ = lambda_\n",
    "        details, loss_value = AnomalyModel.loss_function(output, P_list, S_list, lambda_, x)\n",
    "        return details, loss_value\n",
    "    \n",
    "    def cosine_lr_schedule_with_warmup(self, optimizer, epoch, initial_lr, total_epochs, warmup_epochs):\n",
    "        if warmup_epochs!=0:\n",
    "            if epoch < warmup_epochs:\n",
    "                lr = initial_lr * (epoch + 1) / warmup_epochs\n",
    "            else:\n",
    "                adjusted_epoch = epoch - warmup_epochs\n",
    "                adjusted_total_epochs = total_epochs - warmup_epochs\n",
    "                lr = 0.5 * initial_lr * (1 + math.cos(math.pi * adjusted_epoch / adjusted_total_epochs))\n",
    "\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "\n",
    "\n",
    "    def clip_gradients(self):\n",
    "        \"\"\"\n",
    "        Clip gradients of the model parameters.\n",
    "\n",
    "        Args:\n",
    "            model (torch.nn.Module): The neural network model.\n",
    "            max_norm (float): The maximum allowed norm for the gradients.\n",
    "            norm_type (float): The type of the norm calculation (default: 2 for L2 norm).\n",
    "        \"\"\"\n",
    "        # Recupera todos los gradientes de los parámetros del modelo\n",
    "        gradients = [param.grad for param in self.model.parameters() if param.grad is not None]\n",
    "\n",
    "        # Calcula la norma total de los gradientes\n",
    "        total_norm = torch.norm(torch.stack([torch.norm(grad, self.norm_type) for grad in gradients]), self.norm_type)\n",
    "\n",
    "        # Calcula el factor de escalado para recortar los gradientes\n",
    "        clip_coef = self.max_norm / (total_norm + 1e-6)\n",
    "        if clip_coef < 1:\n",
    "            # Aplica el factor de escalado a los gradientes\n",
    "            for grad in gradients:\n",
    "                grad.mul_(clip_coef)\n",
    "\n",
    "\n",
    "\n",
    "    def schedule_lambda(self,epoch,num_epochs,init_lambda,final_lambda):\n",
    "        self.lambda_=(final_lambda-init_lambda)/num_epochs * epoch + init_lambda\n",
    "        \n",
    "    def train(self,dataloader, num_epochs, initial_lr, warmup_epochs,init_lambda,final_lambda):\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=initial_lr)\n",
    "        self.num_epochs=num_epochs\n",
    "        self.initial_lr=initial_lr\n",
    "        self.model.train()\n",
    "        loss_frob_diss = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch: {epoch}\")\n",
    "            self.schedule_lambda(epoch,num_epochs,init_lambda,final_lambda)\n",
    "            for i, (inputs) in enumerate(dataloader):\n",
    "                inputs = inputs.float().to(\"cuda:0\")\n",
    "                outputs, series, prior, _ = self.model(inputs)\n",
    "                #esto ya lo hacemos dentro del calculo de la perdida\n",
    "                #prior = [j / torch.unsqueeze(torch.sum(j, dim=-1), dim=-1).repeat(1, 1, 1, 100) for j in prior]\n",
    "\n",
    "\n",
    "\n",
    "                loss_min = self.min_loss(outputs, prior, series, inputs, self.lambda_)\n",
    "                loss_min.backward(retain_graph=True)\n",
    "                #optimizer.step()\n",
    "                #optimizer.zero_grad()\n",
    "\n",
    "                details, loss_max = self.max_loss(outputs, prior, series, inputs, self.lambda_)\n",
    "                loss_max.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                loss_frob_diss.append(list(details))\n",
    "\n",
    "                #loss_max.backward(retain_graph=True)\n",
    "                #loss_min.backward()\n",
    "                if self.max_norm is not None:\n",
    "                    self.clip_gradients()\n",
    "                #optimizer.step()\n",
    "                #optimizer.zero_grad()\n",
    "\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], Loss: {loss_max.item():.4f}\")\n",
    "            if warmup_epochs is not None:\n",
    "                self.cosine_lr_schedule_with_warmup(optimizer, epoch, initial_lr, num_epochs, warmup_epochs)\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        print(\"Entrenamiento finalizado\")\n",
    "        self.loss = np.array(loss_frob_diss)\n",
    "    \n",
    "    # def predict(self, data=None):\n",
    "    #     if data is None:\n",
    "    #         data = next(iter(self.dataloader)).float().to(\"cuda:0\")\n",
    "    #     else:\n",
    "    #         data = torch.tensor(data).float()\n",
    "    #         data = data.to(\"cuda:0\") # .unsqueeze(0) -> esto si solo tiene un canal y no lo hemos puesto\n",
    "\n",
    "    #     self.model.eval()\n",
    "    #     with torch.no_grad():\n",
    "    #         out, series, prior, sigmas = self.model(data)\n",
    "\n",
    "    #     out = out.cpu().numpy()\n",
    "    #     series = [s.cpu().numpy() for s in series]\n",
    "    #     prior = [p.cpu().numpy() for p in prior]\n",
    "    #     sigmas = [sigma.cpu().numpy() for sigma in sigmas]\n",
    "\n",
    "    #     return data.cpu().detach().numpy(),out, series, prior, sigmas\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def anomaly_score(model, input=None, crit_batch_size=32):\n",
    "        input = input.float().to(\"cuda:0\")\n",
    "        model.eval()\n",
    "        num_batches = int(input.size(0) / crit_batch_size) + (1 if input.size(0) % crit_batch_size != 0 else 0)\n",
    "\n",
    "        # Inicializa un tensÇor vacío para almacenar los resultados\n",
    "        result = torch.empty(0, device=\"cuda:0\")\n",
    "\n",
    "        for i in range(num_batches):\n",
    "            batch_start = i * crit_batch_size\n",
    "            batch_end = min((i + 1) * crit_batch_size, input.size(0))\n",
    "\n",
    "            # Procesa el lote actual\n",
    "            input_batch = input[batch_start:batch_end]\n",
    "            out, series, prior, sigmas = model(input_batch)\n",
    "            ad = F.softmax(\n",
    "                -AnomalyModel.association_discrepancy(prior, series), dim=0\n",
    "            )\n",
    "\n",
    "            norm = ((out - input_batch) ** 2).sum(axis=-1)\n",
    "\n",
    "            score = torch.mul(ad, norm)\n",
    "\n",
    "            # Añade el resultado del lote actual al tensor de resultado\n",
    "            result = torch.cat((result, score), dim=0)\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "\n",
    "#tenemos el modelo entrenado en una clase, y otra case el con tipo de datos.\n",
    "#ahora necesitamos algo que nos encapsule la evaluacion de los resultados obtenidos\n",
    "#para ello creamos la clase\n",
    "class EvalModel(object):\n",
    "    def __init__(self,model_instance,dataset_instance,hp=None,save_directory=None,batch_predict=12):\n",
    "        self.model_instance=model_instance\n",
    "        self.dataset=dataset_instance\n",
    "        self.batch_predict=batch_predict\n",
    "        self.save_directory=save_directory\n",
    "        self.window_anomalies()\n",
    "\n",
    "        #self.windowed_data2eval=self.windowed_data2eval.unsqueeze(-1) #leañadimos una dimeninsion porque oslo tiene 1 variable \n",
    "        #ya lo hace la clase dataset\n",
    "        self.hp=hp #esto es una lista con los hiperparametros\n",
    "\n",
    "        self.run_model()\n",
    "\n",
    "        self.evaluate_model()\n",
    "\n",
    "\n",
    "    def run_model(self):\n",
    "        input = self.windowed_data2eval.float().to(\"cuda:0\")\n",
    "        self.model_instance.model.eval()\n",
    "        num_batches = int(input.size(0) / self.batch_predict) + (1 if input.size(0) % self.batch_predict != 0 else 0)\n",
    "\n",
    "        # Inicializa tensores vacíos para almacenar los resultados\n",
    "        out_list =[]\n",
    "        series_list =[]\n",
    "        prior_list =[]\n",
    "        sigmas_list =[]\n",
    "\n",
    "        for i in range(num_batches):\n",
    "            batch_start = i * self.batch_predict\n",
    "            batch_end = min((i + 1) * self.batch_predict, input.size(0))\n",
    "\n",
    "            # Procesa el lote actual\n",
    "            input_batch = input[batch_start:batch_end]\n",
    "            out, series, prior, sigmas = self.model_instance.model(input_batch)\n",
    "            # Añade el resultado del lote actual a los tensores de resultado\n",
    "            out_list.append( out)\n",
    "            series_list.append( torch.stack(series).transpose(0,1))#esto que nos devuelve con listas, asi que los haremos tensores\n",
    "            prior_list.append( torch.stack(prior).transpose(0,1))\n",
    "            sigmas_list.append( torch.stack(sigmas).transpose(0,1))\n",
    "\n",
    "        # Guarda los resultados en los atributos de la clase\n",
    "        self.out_list = torch.cat(out_list,dim=0)\n",
    "        self.series_list = torch.cat(series_list,dim=0)\n",
    "        self.prior_list = torch.cat(prior_list,dim=0) #el output son bloques x muestras x cabezas x Len x Len \n",
    "        self.sigmas_list = torch.cat(sigmas_list,dim=0)\n",
    "\n",
    "    def window_anomalies(self):\n",
    "        #pero en lugar de darle un batch del dataLoader, vamos a darle las ventanas en las que encontramos anomalias\n",
    "        aux=[]\n",
    "        for i in self.dataset.list_anomalies:\n",
    "            if (i[0]-self.dataset.window_size//2)>0:\n",
    "                init_window=i[0]-self.dataset.window_size//2\n",
    "            else:\n",
    "                init_window=0\n",
    "\n",
    "            if (init_window+self.dataset.window_size)>self.dataset.sine_wave.shape[0]:\n",
    "                end_window=self.dataset.sine_wave.shape[0]\n",
    "                init_window=self.dataset.sine_wave.shape[0]-self.dataset.window_size\n",
    "            else:\n",
    "                end_window=init_window+self.dataset.window_size\n",
    "            if (end_window-init_window)==self.dataset.window_size:\n",
    "                aux.append(self.dataset.sine_wave[init_window:end_window])\n",
    "        self.windowed_data2eval=torch.stack(aux)\n",
    "\n",
    "    def evaluate_model(self):\n",
    "        #aqui generamos todos los plot y todo, y los guardamos en el directorio indicado -> save_directory\n",
    "\n",
    "        self.anomaly_score=AnomalyModel.anomaly_score(self.model_instance.model, self.windowed_data2eval)\n",
    "        aux=AnomalyModel.anomaly_score(self.model_instance.model, self.dataset.array)\n",
    "        empt=torch.empty(aux.shape[0],self.dataset.length)\n",
    "        empt[:]=float(\"nan\")\n",
    "        for i,j in enumerate(aux):\n",
    "            i_new=i*self.dataset.stride\n",
    "            empt[i,i_new:(i_new+self.dataset.window_size)]=j\n",
    "        self.full_anomaly_score=np.nan_to_num(np.nanmean(empt.cpu().detach().numpy(),axis=0))\n",
    "        \n",
    "\n",
    "\n",
    "    def generate_log_y_plots(self):\n",
    "        #esto es la ultima funcion encargada de todo el trabajo de guarda registros y tal\n",
    "\n",
    "        if not os.path.exists(self.save_directory):\n",
    "            os.mkdir(self.save_directory)\n",
    "\n",
    "        #variadito\n",
    "        plt.figure(figsize=(19,14))\n",
    "        for i in range(15):\n",
    "            plt.subplot(5,3,i+1)\n",
    "            plt.plot(self.out_list[i].cpu().detach().numpy(),\".-\",label=\"out\")\n",
    "            plt.plot(self.windowed_data2eval.cpu().detach().numpy()[i],label=\"inp\")\n",
    "            plt.plot(self.anomaly_score.cpu().detach().numpy()[i]/self.anomaly_score.cpu().detach().numpy()[i].max(),label=\"AnomScore\")\n",
    "            plt.plot(self.sigmas_list[i,:,:,:,0].mean(dim=[0,1]).cpu().detach().numpy()/self.sigmas_list[i,:,:,:,0].mean(dim=[0,1]).cpu().detach().numpy().max(),label=\"sigma\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        name_fig=\"rec_\"+\"_\".join([ str(m) for m in self.hp])+\".png\"\n",
    "        plt.savefig(os.path.join(self.save_directory,name_fig))\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "#=======================0\n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.subplot(2,1,1)\n",
    "        plt.plot(self.model_instance.loss[:,0])\n",
    "\n",
    "        for i in range(self.model_instance.num_epochs-1):\n",
    "            plt.axvline(i*(self.dataset.length//self.model_instance.batch_size),color=\"red\",alpha=0.3)\n",
    "\n",
    "        plt.title(\"Reconstruction Loss\",fontsize=15)\n",
    "        plt.subplot(2,1,2)\n",
    "        plt.plot(self.model_instance.loss[:,1])\n",
    "\n",
    "        for i in range(self.model_instance.num_epochs-1):\n",
    "            plt.axvline(i*(self.dataset.length//self.model_instance.batch_size),color=\"red\",alpha=0.3)\n",
    "\n",
    "        plt.title(\"Association Discrepancy Loss\",fontsize=15)\n",
    "        plt.tight_layout()\n",
    "        name_fig=\"loss_\"+\"_\".join([ str(m) for m in self.hp])+\".png\"\n",
    "        plt.savefig(os.path.join(self.save_directory,name_fig))\n",
    "        plt.close()\n",
    "\n",
    "#==============================000\n",
    "\n",
    "        plt.figure(figsize=(25,23))\n",
    "        for i in range(5):\n",
    "            plt.subplot(5,6,i*6+1)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            cax=plt.imshow(self.series_list[i,0,0].cpu().detach().numpy())\n",
    "            plt.plot(self.windowed_data2eval[i].cpu().detach().numpy()*2.6-6,color=\"orange\")\n",
    "            plt.plot(self.windowed_data2eval[i].cpu().detach().numpy()*2.6-6,np.arange(self.dataset.window_size),color=\"orange\")\n",
    "            plt.title(\"Block 1, Head 1\")\n",
    "            ax=plt.gca()\n",
    "            divider = make_axes_locatable(ax)\n",
    "            cax2 = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "\n",
    "            # Crea el colorbar ajustado al tamaño del axes\n",
    "            plt.colorbar(cax, cax=cax2)\n",
    "            plt.subplot(5,6,i*6+2)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            cax=plt.imshow(self.series_list[i,0,1].cpu().detach().numpy())\n",
    "            plt.plot(self.windowed_data2eval[i].cpu().detach().numpy()*2.6-6,color=\"orange\")\n",
    "            plt.plot(self.windowed_data2eval[i].cpu().detach().numpy()*2.6-6,np.arange(self.dataset.window_size),color=\"orange\")\n",
    "            plt.title(\"Block 1, Head 2\")\n",
    "            ax=plt.gca()\n",
    "            divider = make_axes_locatable(ax)\n",
    "            cax2 = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "\n",
    "            # Crea el colorbar ajustado al tamaño del axes\n",
    "            plt.colorbar(cax, cax=cax2)\n",
    "            plt.subplot(5,6,i*6+3)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            cax=plt.imshow(self.series_list[i,0,2].cpu().detach().numpy())\n",
    "            plt.plot(self.windowed_data2eval[i].cpu().detach().numpy()*2.6-6,color=\"orange\")\n",
    "            plt.plot(self.windowed_data2eval[i].cpu().detach().numpy()*2.6-6,np.arange(self.dataset.window_size),color=\"orange\")\n",
    "            plt.title(\"Block 1, Head 3\")\n",
    "            ax=plt.gca()\n",
    "            divider = make_axes_locatable(ax)\n",
    "            cax2 = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "\n",
    "            # Crea el colorbar ajustado al tamaño del axes\n",
    "            plt.colorbar(cax, cax=cax2)\n",
    "            plt.subplot(5,6,i*6+4)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            cax=plt.imshow(self.series_list[i,1,0].cpu().detach().numpy())\n",
    "            plt.plot(self.windowed_data2eval[i].cpu().detach().numpy()*2.6-6,color=\"orange\")\n",
    "            plt.plot(self.windowed_data2eval[i].cpu().detach().numpy()*2.6-6,np.arange(self.dataset.window_size),color=\"orange\")\n",
    "            plt.title(\"Block 2, Head 1\")\n",
    "            ax=plt.gca()\n",
    "            divider = make_axes_locatable(ax)\n",
    "            cax2 = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "\n",
    "            # Crea el colorbar ajustado al tamaño del axes\n",
    "            plt.colorbar(cax, cax=cax2)\n",
    "            plt.subplot(5,6,i*6+5)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            cax=plt.imshow(self.series_list[i,1,1].cpu().detach().numpy())\n",
    "            plt.plot(self.windowed_data2eval[i].cpu().detach().numpy()*2.6-6,color=\"orange\")\n",
    "            plt.plot(self.windowed_data2eval[i].cpu().detach().numpy()*2.6-6,np.arange(self.dataset.window_size),color=\"orange\")\n",
    "            plt.title(\"Block 2, Head 2\")\n",
    "            ax=plt.gca()\n",
    "            divider = make_axes_locatable(ax)\n",
    "            cax2 = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "\n",
    "            # Crea el colorbar ajustado al tamaño del axes\n",
    "            plt.colorbar(cax, cax=cax2)\n",
    "            plt.subplot(5,6,i*6+6)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            cax=plt.imshow(self.series_list[i,1,2].cpu().detach().numpy())\n",
    "            plt.plot(self.windowed_data2eval[i].cpu().detach().numpy()*2.6-6,color=\"orange\")\n",
    "            plt.plot(self.windowed_data2eval[i].cpu().detach().numpy()*2.6-6,np.arange(self.dataset.window_size),color=\"orange\")\n",
    "            plt.title(\"Block 2, Head 3\")\n",
    "            ax=plt.gca()\n",
    "            divider = make_axes_locatable(ax)\n",
    "            cax2 = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "\n",
    "            # Crea el colorbar ajustado al tamaño del axes\n",
    "            plt.colorbar(cax, cax=cax2)\n",
    "        plt.tight_layout()\n",
    "        name_fig=\"series_\"+\"_\".join([ str(m) for m in self.hp])+\".png\"\n",
    "        plt.savefig(os.path.join(self.save_directory,name_fig))\n",
    "        plt.close()\n",
    "\n",
    "#===============\n",
    "\n",
    "        plt.figure(figsize=(25,23))\n",
    "        for i in range(5):\n",
    "            plt.subplot(5,6,i*6+1)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            cax=plt.imshow(self.prior_list[i,0,0].cpu().detach().numpy())\n",
    "            plt.plot(self.windowed_data2eval[i].cpu().detach().numpy()*2.6-6,color=\"orange\")\n",
    "            plt.plot(self.windowed_data2eval[i].cpu().detach().numpy()*2.6-6,np.arange(self.dataset.window_size),color=\"orange\")\n",
    "            plt.title(\"Block 1, Head 1\")\n",
    "            ax=plt.gca()\n",
    "            divider = make_axes_locatable(ax)\n",
    "            cax2 = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "\n",
    "            # Crea el colorbar ajustado al tamaño del axes\n",
    "            plt.colorbar(cax, cax=cax2)\n",
    "            plt.subplot(5,6,i*6+2)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            cax=plt.imshow(self.prior_list[i,0,1].cpu().detach().numpy())\n",
    "            plt.plot(self.windowed_data2eval[i].cpu().detach().numpy()*2.6-6,color=\"orange\")\n",
    "            plt.plot(self.windowed_data2eval[i].cpu().detach().numpy()*2.6-6,np.arange(self.dataset.window_size),color=\"orange\")\n",
    "            plt.title(\"Block 1, Head 2\")\n",
    "            ax=plt.gca()\n",
    "            divider = make_axes_locatable(ax)\n",
    "            cax2 = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "\n",
    "            # Crea el colorbar ajustado al tamaño del axes\n",
    "            plt.colorbar(cax, cax=cax2)\n",
    "            plt.subplot(5,6,i*6+3)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            cax=plt.imshow(self.prior_list[i,0,2].cpu().detach().numpy())\n",
    "            plt.plot(self.windowed_data2eval[i].cpu().detach().numpy()*2.6-6,color=\"orange\")\n",
    "            plt.plot(self.windowed_data2eval[i].cpu().detach().numpy()*2.6-6,np.arange(self.dataset.window_size),color=\"orange\")\n",
    "            plt.title(\"Block 1, Head 3\")\n",
    "            ax=plt.gca()\n",
    "            divider = make_axes_locatable(ax)\n",
    "            cax2 = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "\n",
    "            # Crea el colorbar ajustado al tamaño del axes\n",
    "            plt.colorbar(cax, cax=cax2)\n",
    "            plt.subplot(5,6,i*6+4)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            cax=plt.imshow(self.prior_list[i,1,0].cpu().detach().numpy())\n",
    "            plt.plot(self.windowed_data2eval[i].cpu().detach().numpy()*2.6-6,color=\"orange\")\n",
    "            plt.plot(self.windowed_data2eval[i].cpu().detach().numpy()*2.6-6,np.arange(self.dataset.window_size),color=\"orange\")\n",
    "            plt.title(\"Block 2, Head 1\")\n",
    "            ax=plt.gca()\n",
    "            divider = make_axes_locatable(ax)\n",
    "            cax2 = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "\n",
    "            # Crea el colorbar ajustado al tamaño del axes\n",
    "            plt.colorbar(cax, cax=cax2)\n",
    "            plt.subplot(5,6,i*6+5)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            cax=plt.imshow(self.prior_list[i,1,1].cpu().detach().numpy())\n",
    "            plt.plot(self.windowed_data2eval[i].cpu().detach().numpy()*2.6-6,color=\"orange\")\n",
    "            plt.plot(self.windowed_data2eval[i].cpu().detach().numpy()*2.6-6,np.arange(self.dataset.window_size),color=\"orange\")\n",
    "            plt.title(\"Block 2, Head 2\")\n",
    "            ax=plt.gca()\n",
    "            divider = make_axes_locatable(ax)\n",
    "            cax2 = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "\n",
    "            # Crea el colorbar ajustado al tamaño del axes\n",
    "            plt.colorbar(cax, cax=cax2)\n",
    "            plt.subplot(5,6,i*6+6)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            cax=plt.imshow(self.prior_list[i,1,2].cpu().detach().numpy())\n",
    "            plt.plot(self.windowed_data2eval[i].cpu().detach().numpy()*2.6-6,color=\"orange\")\n",
    "            plt.plot(self.windowed_data2eval[i].cpu().detach().numpy()*2.6-6,np.arange(self.dataset.window_size),color=\"orange\")\n",
    "            plt.title(\"Block 2, Head 3\")\n",
    "            ax=plt.gca()\n",
    "            divider = make_axes_locatable(ax)\n",
    "            cax2 = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "\n",
    "            # Crea el colorbar ajustado al tamaño del axes\n",
    "            plt.colorbar(cax, cax=cax2)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        name_fig=\"prior_\"+\"_\".join([ str(m) for m in self.hp])+\".png\"\n",
    "        plt.savefig(os.path.join(self.save_directory,name_fig))\n",
    "        plt.close()\n",
    "\n",
    "#================\n",
    "\n",
    "        plt.figure(figsize=(14,8))\n",
    "        plt.plot(self.full_anomaly_score/self.full_anomaly_score.max(),\"-r\",label=\"Ass Dis\")\n",
    "        plt.plot(self.dataset.sine_wave.cpu().detach().numpy()/self.dataset.sine_wave.cpu().detach().numpy().max(),alpha=0.8,label=\"señal\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "        name_fig=\"critScore_\"+\"_\".join([ str(m) for m in self.hp])+\".png\"\n",
    "        plt.savefig(os.path.join(self.save_directory,name_fig))\n",
    "        plt.close()\n",
    "#=====================\n",
    "\n",
    "        plt.figure(figsize=(14,5))\n",
    "\n",
    "        plt.subplot(1,4,1)\n",
    "\n",
    "        plt.hist(self.sigmas_list[:,:, :, :, 0].mean(dim=0).cpu().detach().numpy().flatten(),bins=120,alpha=0.5,histtype=\"stepfilled\")\n",
    "        plt.xlabel(\"sigma\")\n",
    "        plt.title(\"Media sobre todos los \\n sigmas del batch\",fontsize=13)\n",
    "\n",
    "        plt.subplot(1,4,2)\n",
    "        plt.hist(self.sigmas_list[:,0,:, :, 0].cpu().detach().numpy().flatten(),bins=100,alpha=0.4,histtype=\"stepfilled\",label=\"Block 1\")\n",
    "        plt.hist(self.sigmas_list[:,1,:, :, 0].cpu().detach().numpy().flatten(),bins=100,alpha=0.4,histtype=\"stepfilled\",label=\"Block 2\")\n",
    "        plt.xlabel(\"sigma\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Media sobre todos los sigmas del batch,\\n solo segundo bloque\",fontsize=12)\n",
    "        plt.subplot(1,4,3)\n",
    "        plt.hist(self.sigmas_list[:,0,0, :, 0].cpu().detach().numpy().flatten(),bins=80,alpha=0.3,histtype=\"stepfilled\",label=\"Head 1\")\n",
    "        plt.hist(self.sigmas_list[:,0,1, :, 0].cpu().detach().numpy().flatten(),bins=80,alpha=0.3,histtype=\"stepfilled\",label=\"Head 2\")\n",
    "        plt.hist(self.sigmas_list[:,0,2, :, 0].cpu().detach().numpy().flatten(),bins=80,alpha=0.3,histtype=\"stepfilled\",label=\"Head 3\")\n",
    "        plt.xlabel(\"sigma\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Media sobre todos los sigmas del batch,\\n solo primer bloque, y primera cabeza\",fontsize=12)\n",
    "\n",
    "        plt.subplot(1,4,4)\n",
    "        plt.hist(self.sigmas_list[:,1,0, :, 0].cpu().detach().numpy().flatten(),bins=80,alpha=0.3,histtype=\"stepfilled\",label=\"Head 1\")\n",
    "        plt.hist(self.sigmas_list[:,1,1, :, 0].cpu().detach().numpy().flatten(),bins=80,alpha=0.3,histtype=\"stepfilled\",label=\"Head 2\")\n",
    "        plt.hist(self.sigmas_list[:,1,2, :, 0].cpu().detach().numpy().flatten(),bins=80,alpha=0.3,histtype=\"stepfilled\",label=\"Head 3\")\n",
    "        plt.xlabel(\"sigma\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Media sobre todos los sigmas del batch,\\n solo segundo bloque, y primera cabeza\",fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        name_fig=\"sigmas_\"+\"_\".join([ str(m) for m in self.hp])+\".png\"\n",
    "        plt.savefig(os.path.join(self.save_directory,name_fig))\n",
    "        \n",
    "        plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m BASE_DIR\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m..\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m data_dir\u001b[39m=\u001b[39mos\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(BASE_DIR,\u001b[39m\"\u001b[39m\u001b[39mdata2apply/UCR_dataAnomalyArchive\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "BASE_DIR=\"..\"\n",
    "data_dir=os.path.join(BASE_DIR,\"data2apply/UCR_dataAnomalyArchive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'UCRData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#creamos el dataset\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mid\u001b[39m\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m\n\u001b[0;32m----> 3\u001b[0m data\u001b[39m=\u001b[39mUCRData(data_dir,stride\u001b[39m=\u001b[39m\u001b[39m15\u001b[39m,winsize\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m,norm\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m data\u001b[39m.\u001b[39mload_ID(\u001b[39mid\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[39m#creamos el dataloader \u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'UCRData' is not defined"
     ]
    }
   ],
   "source": [
    "#creamos el dataset\n",
    "id=3\n",
    "data=UCRData(data_dir,stride=15,winsize=100,norm=True)\n",
    "data.load_ID(id)\n",
    "\n",
    "#creamos el dataloader \n",
    "dataloader=DataLoader(data,batch_size=16,shuffle=True)\n",
    "\n",
    "#creamos el modelo \n",
    "model_instance = AnomalyModel(AnomalyTransformer.AnomalyTransformer, n_heads=3, d_model=128, enc_in=1, enc_out=1, max_norm=None, sigma_a=3, sigma_b=5, clip_sigma=\"yes\")\n",
    "\n",
    "#entrenamos el modelo\n",
    "model_instance.train(dataloader,1, 1e-4, 1, 0.01, 0.01)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 7.93 GiB total capacity; 7.69 GiB already allocated; 18.19 MiB free; 7.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m gc\u001b[39m.\u001b[39mcollect()\n\u001b[1;32m      5\u001b[0m \u001b[39m#evaluamos el modelo\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m aux\u001b[39m=\u001b[39mEvalModel(model_instance,data,\u001b[39mNone\u001b[39;49;00m,\u001b[39mNone\u001b[39;49;00m)\n",
      "Cell \u001b[0;32mIn[8], line 221\u001b[0m, in \u001b[0;36mEvalModel.__init__\u001b[0;34m(self, model_instance, dataset_instance, hp, save_directory, batch_predict)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhp\u001b[39m=\u001b[39mhp \u001b[39m#esto es una lista con los hiperparametros\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_model()\n\u001b[0;32m--> 221\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate_model()\n",
      "Cell \u001b[0;32mIn[8], line 276\u001b[0m, in \u001b[0;36mEvalModel.evaluate_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mevaluate_model\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    273\u001b[0m     \u001b[39m#aqui generamos todos los plot y todo, y los guardamos en el directorio indicado -> save_directory\u001b[39;00m\n\u001b[1;32m    275\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39manomaly_score\u001b[39m=\u001b[39mAnomalyModel\u001b[39m.\u001b[39manomaly_score(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_instance\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindowed_data2eval)\n\u001b[0;32m--> 276\u001b[0m     aux\u001b[39m=\u001b[39mAnomalyModel\u001b[39m.\u001b[39;49manomaly_score(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_instance\u001b[39m.\u001b[39;49mmodel, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset\u001b[39m.\u001b[39;49marray)\n\u001b[1;32m    277\u001b[0m     empt\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mempty(aux\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m],\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39mlength)\n\u001b[1;32m    278\u001b[0m     empt[:]\u001b[39m=\u001b[39m\u001b[39mfloat\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mnan\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 188\u001b[0m, in \u001b[0;36mAnomalyModel.anomaly_score\u001b[0;34m(model, input, crit_batch_size)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[39m# Procesa el lote actual\u001b[39;00m\n\u001b[1;32m    187\u001b[0m input_batch \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m[batch_start:batch_end]\n\u001b[0;32m--> 188\u001b[0m out, series, prior, sigmas \u001b[39m=\u001b[39m model(input_batch)\n\u001b[1;32m    189\u001b[0m ad \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(\n\u001b[1;32m    190\u001b[0m     \u001b[39m-\u001b[39mAnomalyModel\u001b[39m.\u001b[39massociation_discrepancy(prior, series), dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m    191\u001b[0m )\n\u001b[1;32m    193\u001b[0m norm \u001b[39m=\u001b[39m ((out \u001b[39m-\u001b[39m input_batch) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/otros_repos/TFM/3-ApplyModel/notebooks/../../1.2-HyperParameterTuning/model/AnomalyTransformer.py:87\u001b[0m, in \u001b[0;36mAnomalyTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     86\u001b[0m     enc_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(x)\n\u001b[0;32m---> 87\u001b[0m     enc_out, series, prior, sigmas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(enc_out)\n\u001b[1;32m     88\u001b[0m     enc_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprojection(enc_out)\n\u001b[1;32m     90\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_attention:\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/otros_repos/TFM/3-ApplyModel/notebooks/../../1.2-HyperParameterTuning/model/AnomalyTransformer.py:47\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x, attn_mask)\u001b[0m\n\u001b[1;32m     45\u001b[0m sigma_list \u001b[39m=\u001b[39m []\n\u001b[1;32m     46\u001b[0m \u001b[39mfor\u001b[39;00m attn_layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn_layers:\n\u001b[0;32m---> 47\u001b[0m     x, series, prior, sigma \u001b[39m=\u001b[39m attn_layer(x, attn_mask\u001b[39m=\u001b[39;49mattn_mask)\n\u001b[1;32m     48\u001b[0m     series_list\u001b[39m.\u001b[39mappend(series)\n\u001b[1;32m     49\u001b[0m     prior_list\u001b[39m.\u001b[39mappend(prior)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/otros_repos/TFM/3-ApplyModel/notebooks/../../1.2-HyperParameterTuning/model/AnomalyTransformer.py:23\u001b[0m, in \u001b[0;36mEncoderLayer.forward\u001b[0;34m(self, x, attn_mask)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, attn_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m---> 23\u001b[0m     new_x, attn, mask, sigma \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m     24\u001b[0m         x, x, x,\n\u001b[1;32m     25\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask\n\u001b[1;32m     26\u001b[0m     )\n\u001b[1;32m     27\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(x\u001b[39m+\u001b[39mnew_x)\n\u001b[1;32m     28\u001b[0m     y \u001b[39m=\u001b[39m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/otros_repos/TFM/3-ApplyModel/notebooks/../../1.2-HyperParameterTuning/model/attn.py:104\u001b[0m, in \u001b[0;36mAttentionLayer.forward\u001b[0;34m(self, queries, keys, values, attn_mask)\u001b[0m\n\u001b[1;32m    101\u001b[0m values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue_projection(values)\u001b[39m.\u001b[39mview(B, S, H, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    102\u001b[0m sigma \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msigma_projection(x)\u001b[39m.\u001b[39mview(B, L, H)\n\u001b[0;32m--> 104\u001b[0m out, series, prior, sigma \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minner_attention(\n\u001b[1;32m    105\u001b[0m     queries,\n\u001b[1;32m    106\u001b[0m     keys,\n\u001b[1;32m    107\u001b[0m     values,\n\u001b[1;32m    108\u001b[0m     sigma,\n\u001b[1;32m    109\u001b[0m     attn_mask\n\u001b[1;32m    110\u001b[0m )\n\u001b[1;32m    111\u001b[0m out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mview(B, L, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    113\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_projection(out), series, prior, sigma\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/otros_repos/TFM/3-ApplyModel/notebooks/../../1.2-HyperParameterTuning/model/attn.py:62\u001b[0m, in \u001b[0;36mAnomalyAttention.forward\u001b[0;34m(self, queries, keys, values, sigma, attn_mask)\u001b[0m\n\u001b[1;32m     60\u001b[0m sigma \u001b[39m=\u001b[39m sigma\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mrepeat(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, window_size)  \u001b[39m# B H L L\u001b[39;00m\n\u001b[1;32m     61\u001b[0m prior \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistances\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mrepeat(sigma\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], sigma\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m---> 62\u001b[0m prior \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m \u001b[39m/\u001b[39m (math\u001b[39m.\u001b[39msqrt(\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m math\u001b[39m.\u001b[39mpi) \u001b[39m*\u001b[39m sigma) \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39;49mprior \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m \u001b[39m2\u001b[39;49m \u001b[39m/\u001b[39;49m \u001b[39m2\u001b[39;49m \u001b[39m/\u001b[39;49m (sigma \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m \u001b[39m2\u001b[39;49m))\n\u001b[1;32m     64\u001b[0m series \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(torch\u001b[39m.\u001b[39msoftmax(attn, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m     65\u001b[0m V \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39meinsum(\u001b[39m\"\u001b[39m\u001b[39mbhls,bshd->blhd\u001b[39m\u001b[39m\"\u001b[39m, series, values)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 7.93 GiB total capacity; 7.69 GiB already allocated; 18.19 MiB free; 7.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "del dataloader\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "#evaluamos el modelo\n",
    "aux=EvalModel(model_instance,data,None,None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
