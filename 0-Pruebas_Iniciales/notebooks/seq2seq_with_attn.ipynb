{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invertir un vector con seq2seq with attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  as np \n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "from torch import nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset,Dataset, DataLoader, random_split, IterableDataset\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import gc\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementacion de un seq2seq con atencion\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    #esto nos va a devolver un output y un hidden state\n",
    "    #tenemos tambien un init_hidden para inicializar el hidden state\n",
    "    #nos interesa el tamaño del input y del hidden state (el output tiene tamaño del hidden)\n",
    "    def __init__(self,d_input,d_hidden):\n",
    "        super().__init__()\n",
    "        self.d_input=d_input\n",
    "        self.d_hidden=d_hidden\n",
    "        #vamos a crear un embeding propio con una densa antes de la RNN \n",
    "        self.relu=nn.ReLU()\n",
    "        self.dense1=nn.Linear(d_input,d_hidden)\n",
    "        self.rnn=nn.GRU(d_hidden,d_hidden,batch_first=True) #como no ponemos batch_first=True todo sera de dimension dim: len x hidden size\n",
    "\n",
    "    def forward(self,x,hidden):\n",
    "        x=self.relu(self.dense1(x)) # pasamos B x 1 x input y obtenemos B x 1 x hidden_dim\n",
    "        output,hidden_out=self.rnn(x,hidden) #obtenemos B x 1 x hidden_dim y 1 x B x hidden_dim \n",
    "        return output,hidden_out #obtenemos B x 1 x hidden_dim y 1 x B x hidden_dim\n",
    "    \n",
    "    def init_hidden(self,batch_size=1):\n",
    "        return torch.zeros(1,batch_size,self.d_hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 15]) torch.Size([1, 5, 15])\n"
     ]
    }
   ],
   "source": [
    "#provemos lo anterior\n",
    "encoder=EncoderRNN(1,15)\n",
    "test_vector=torch.randn(5,1,1)\n",
    "output,attn=encoder(test_vector,encoder.init_hidden(5))\n",
    "print(output.shape,attn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    #este es el mecanismo de atencion, el dot product basicamente\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "    def forward(self,hidden_state,encoder_outputs):\n",
    "        #si el encoder_outputs es:  Batch x len_seq x hidden_dim y el hidden state que usamo de Q es 1 x B x Hidden_dim, devolvemos un context que es B x 1 x hidden_dim  y attn_weights B x 1 x len_seq\n",
    "\n",
    "        #transponemos el hidden state para hacer las multiplicaciones sobre los batches\n",
    "\n",
    "        hidden_state_transpose=hidden_state.transpose(0,1) # ahora es B x 1 x hidden\n",
    "        #el objetivo es multiplicarlo por el encoder_outputs que es  Batch x len_seq x hidden_dim, y para ello lo tenemos que transponer\n",
    "        encoder_outputs_transpose=encoder_outputs.transpose(1,2) # Batch x  hidden_dim x len_seq \n",
    "        #aqui calculamos el alignement\n",
    "\n",
    "        e=torch.bmm(hidden_state_transpose,encoder_outputs_transpose) # B x 1 x len_seq\n",
    "        att_weights=F.softmax(e,dim=2) # B x 1 x len_seq y suman 1 sobre los elementos en esa dimension\n",
    "        context=torch.bmm(att_weights,encoder_outputs) # hacemos la multiplicacion para obtener B x 1 x hidden_dim\n",
    "        return context, att_weights # return  B x 1 x hidden_dim y B x 1 x len_seq \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.,  0.,  0., 15.,  0.,  0.,  0.]],\n",
      "\n",
      "        [[ 0.,  0.,  0., 15.,  0.,  0.,  0.]]])\n"
     ]
    }
   ],
   "source": [
    "attn=Attention()\n",
    "emb_outs=torch.zeros(2,7,15)\n",
    "emb_outs[:,3:4,:]=1\n",
    "context,attn=attn(torch.ones(1,2,15),emb_outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    #este decoder va a tener implementada la atencion\n",
    "    #empezaremos el decoder con un hidden_state 0 igual que antes\n",
    "    #el hidden state previo se le va a meter a modo de query al modelo de atencion, que va a recibir tambien los values, que \n",
    "    #son los estados hidden del encoder, de modo que lo ejecutamos en el init de aqui, el encoder\n",
    "    def __init__(self,input_dim,hidden_size):\n",
    "        super().__init__()\n",
    "        self.d_input=input_dim\n",
    "        self.d_output=input_dim\n",
    "        self.d_hidden=hidden_size\n",
    "        #un enmbeding para el input y una densa para el output\n",
    "        self.dense_in=nn.Linear(input_dim,hidden_size)\n",
    "        self.dense_out1=nn.Linear(hidden_size*2,hidden_size)\n",
    "        self.dense_out2=nn.Linear(hidden_size,input_dim) # asumienod dim_input = dim_output\n",
    "\n",
    "        self.rnn=nn.GRU(hidden_size,hidden_size,batch_first=True)#como no ponemos batch_first=True todo sera de dimension dim: len x hidden size\n",
    "\n",
    "        #el mecanismo de atencion y los hidden states del encoder\n",
    "        self.attn=Attention()\n",
    "\n",
    "    def forward(self,prev_input,hidden_state,encoder_outputs):\n",
    "        #hacer forward de esto es partiendo del hidden state anterior, sacar el nuevo\n",
    "        #y con ese calcular la atencion y el context vector, y luego juntarlo con el hidden state y meterlo en la dense y obtener el nuedo hidden\n",
    "\n",
    "        #el input debe de pasar por el embeding\n",
    "        x=F.relu(self.dense_in(prev_input)) # entra B x 1 x input_dim y sale B x 1 x Hidden_dim, el 1 es porque estamos procesando un tocken solo\n",
    "        #ahora calculamos el hidden state\n",
    "        _,new_hidden_state=self.rnn(x,hidden_state) #entran y_(t-1)=B x 1 x Hidden_dim y h_(t-1)= 1 x Batch_size x Hidden_dim y salen y_t = B x 1 x Hidden_dim y h_t = 1 x B x Hidden_dim\n",
    "\n",
    "        #con este hidden state,calculamos la attencion, se calcula en cada dimension batch independiente\n",
    "        #si el encoder_outputs es:  Batch xlen_seq x hidden_dim y el hidden state que usamo de Q es 1 x B x Hidden_dim, devolvemos un context que es B x 1 x hidden_dim  y attn_weights B x 1 x len_seq\n",
    "        context,attn_weigths=self.attn(new_hidden_state,encoder_outputs) # return  B x 1 x hidden_dim y B x 1 x len_seq \n",
    "        new_hidden_state_transpose=new_hidden_state.transpose(0,1)\n",
    "        #con este context y con el hiden state creamos un vector para meter en una dense\n",
    "        out=F.sigmoid(self.dense_out1(torch.cat((new_hidden_state_transpose,context),dim=2)))# le metemos B x 1 x 2*hidden_dim y sacamos B x 1 x hidden_dim\n",
    "        out1=self.dense_out2(out) #metemos B x 1 x hidden_dim y sacamos B x 1 x input_dim\n",
    "\n",
    "        return out1,new_hidden_state, attn_weigths #B x 1 x input_dim , 1 x B x Hidden_dim y B x 1 x len_seq\n",
    "    \n",
    "    def init_hidden(self,batch_size=1):\n",
    "        return torch.zeros(1,batch_size,self.d_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder=DecoderRNN(1,15)\n",
    "\n",
    "a=torch.zeros(2,7,15)\n",
    "a[:,[2],:]=1\n",
    "o,h,atn=decoder(torch.randn(2,1,1),torch.randn(1,2,15),a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.7764,  0.2985,  0.9831, -0.0340, -0.4747,  0.5859,  0.5646,\n",
      "          -0.3377,  0.0232, -1.2997, -0.1950, -0.4325,  0.4108,  0.7329,\n",
      "           0.5236],\n",
      "         [ 0.0895, -0.4322, -0.8604, -0.4994,  0.2019, -0.3698,  0.3595,\n",
      "           0.5239,  0.1578,  0.3372,  0.0829,  1.3714,  0.4039,  1.0230,\n",
      "           0.5676]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
