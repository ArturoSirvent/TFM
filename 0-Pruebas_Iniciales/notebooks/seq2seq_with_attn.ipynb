{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invertir un vector con seq2seq with attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  as np \n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "from torch import nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset,Dataset, DataLoader, random_split, IterableDataset\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import gc\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementacion de un seq2seq con atencion\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    #esto nos va a devolver un output y un hidden state\n",
    "    #tenemos tambien un init_hidden para inicializar el hidden state\n",
    "    #nos interesa el tamaño del input y del hidden state (el output tiene tamaño del hidden)\n",
    "    def __init__(self,d_input,d_hidden):\n",
    "        super().__init__()\n",
    "        self.d_input=d_input\n",
    "        self.d_hidden=d_hidden\n",
    "        #vamos a crear un embeding propio con una densa antes de la RNN \n",
    "        self.relu=nn.ReLU()\n",
    "        self.dense1=nn.Linear(d_input,d_hidden)\n",
    "        self.rnn=nn.GRU(d_hidden,d_hidden,batch_first=True) #como no ponemos batch_first=True todo sera de dimension dim: len x hidden size\n",
    "\n",
    "    def forward(self,x,hidden):\n",
    "        x=self.relu(self.dense1(x)) # pasamos B x 1 x input y obtenemos B x 1 x hidden_dim\n",
    "        output,hidden_out=self.rnn(x,hidden) #obtenemos B x 1 x hidden_dim y 1 x B x hidden_dim \n",
    "        return output,hidden_out #obtenemos B x len_seq x hidden_dim y 1 x B x hidden_dim\n",
    "    \n",
    "    def init_hidden(self,batch_size=1):\n",
    "        return torch.zeros(1,batch_size,self.d_hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 15]) torch.Size([1, 5, 15])\n"
     ]
    }
   ],
   "source": [
    "#provemos lo anterior\n",
    "encoder=EncoderRNN(1,15)\n",
    "test_vector=torch.randn(5,1,1)\n",
    "output,attn=encoder(test_vector,encoder.init_hidden(5))\n",
    "print(output.shape,attn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    #este es el mecanismo de atencion, el dot product basicamente\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "    def forward(self,hidden_state,encoder_outputs):\n",
    "        #si el encoder_outputs es:  Batch x len_seq x hidden_dim y el hidden state que usamo de Q es 1 x B x Hidden_dim, devolvemos un context que es B x 1 x hidden_dim  y attn_weights B x 1 x len_seq\n",
    "\n",
    "        #transponemos el hidden state para hacer las multiplicaciones sobre los batches\n",
    "\n",
    "        hidden_state_transpose=hidden_state.transpose(0,1) # ahora es B x 1 x hidden\n",
    "        #el objetivo es multiplicarlo por el encoder_outputs que es  Batch x len_seq x hidden_dim, y para ello lo tenemos que transponer\n",
    "        encoder_outputs_transpose=encoder_outputs.transpose(1,2) # Batch x  hidden_dim x len_seq \n",
    "        #aqui calculamos el alignement\n",
    "\n",
    "        e=torch.bmm(hidden_state_transpose,encoder_outputs_transpose) # B x 1 x len_seq\n",
    "        att_weights=F.softmax(e,dim=2) # B x 1 x len_seq y suman 1 sobre los elementos en esa dimension\n",
    "        context=torch.bmm(att_weights,encoder_outputs) # hacemos la multiplicacion para obtener B x 1 x hidden_dim\n",
    "        return context, att_weights # return  B x 1 x hidden_dim y B x 1 x len_seq \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429]],\n",
       "\n",
       "        [[0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn=Attention()\n",
    "emb_outs=torch.ones(2,7,15)\n",
    "emb_outs[:,3:4,:]=1\n",
    "context,attn=attn(torch.ones(1,2,15),emb_outs)\n",
    "attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    #este decoder va a tener implementada la atencion\n",
    "    #empezaremos el decoder con un hidden_state 0 igual que antes\n",
    "    #el hidden state previo se le va a meter a modo de query al modelo de atencion, que va a recibir tambien los values, que \n",
    "    #son los estados hidden del encoder, de modo que lo ejecutamos en el init de aqui, el encoder\n",
    "    def __init__(self,input_dim,hidden_size):\n",
    "        super().__init__()\n",
    "        self.d_input=input_dim\n",
    "        self.d_output=input_dim\n",
    "        self.d_hidden=hidden_size\n",
    "        #un enmbeding para el input y una densa para el output\n",
    "        self.dense_in=nn.Linear(input_dim,hidden_size)\n",
    "        self.dense_out1=nn.Linear(hidden_size*2,hidden_size)\n",
    "        self.dense_out2=nn.Linear(hidden_size,input_dim) # asumienod dim_input = dim_output\n",
    "\n",
    "        self.rnn=nn.GRU(hidden_size,hidden_size,batch_first=True)#como no ponemos batch_first=True todo sera de dimension dim: len x hidden size\n",
    "\n",
    "        #el mecanismo de atencion y los hidden states del encoder\n",
    "        self.attn=Attention()\n",
    "\n",
    "    def forward(self,prev_input,hidden_state,encoder_outputs):\n",
    "        #hacer forward de esto es partiendo del hidden state anterior, sacar el nuevo\n",
    "        #y con ese calcular la atencion y el context vector, y luego juntarlo con el hidden state y meterlo en la dense y obtener el nuedo hidden\n",
    "\n",
    "        #el input debe de pasar por el embeding\n",
    "        x=F.relu(self.dense_in(prev_input)) # entra B x 1 x input_dim y sale B x 1 x Hidden_dim, el 1 es porque estamos procesando un tocken solo\n",
    "        #ahora calculamos el hidden state\n",
    "        _,new_hidden_state=self.rnn(x,hidden_state) #entran y_(t-1)=B x 1 x Hidden_dim y h_(t-1)= 1 x Batch_size x Hidden_dim y salen y_t = B x 1 x Hidden_dim y h_t = 1 x B x Hidden_dim\n",
    "\n",
    "        #con este hidden state,calculamos la attencion, se calcula en cada dimension batch independiente\n",
    "        #si el encoder_outputs es:  Batch xlen_seq x hidden_dim y el hidden state que usamo de Q es 1 x B x Hidden_dim, devolvemos un context que es B x 1 x hidden_dim  y attn_weights B x 1 x len_seq\n",
    "        context,attn_weigths=self.attn(new_hidden_state,encoder_outputs) # return  B x 1 x hidden_dim y B x 1 x len_seq \n",
    "        new_hidden_state_transpose=new_hidden_state.transpose(0,1)\n",
    "        #con este context y con el hiden state creamos un vector para meter en una dense\n",
    "        #out=self.dense_out1(torch.cat((new_hidden_state_transpose,context),dim=2))# le metemos B x 1 x 2*hidden_dim y sacamos B x 1 x hidden_dim\n",
    "        out1=self.dense_out2(context) #metemos B x 1 x hidden_dim y sacamos B x 1 x input_dim\n",
    "\n",
    "        return out1,new_hidden_state, attn_weigths #B x 1 x input_dim , 1 x B x Hidden_dim y B x 1 x len_seq\n",
    "    \n",
    "    def init_hidden(self,batch_size=1):\n",
    "        return torch.zeros(1,batch_size,self.d_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder=DecoderRNN(1,15)\n",
    "\n",
    "a=torch.zeros(2,7,15)\n",
    "a[:,[2],:]=1\n",
    "o,h,atn=decoder(torch.randn(2,1,1),torch.randn(1,2,15),a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFlip(Dataset):\n",
    "    def __init__(self,seq_len,input_dim,max_dim):\n",
    "        super().__init__()\n",
    "        self.seq_len=seq_len\n",
    "        self.input_dim=input_dim\n",
    "        self.max_dim=max_dim\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x=torch.randint(0,50,(self.seq_len,self.input_dim)).float()\n",
    "        y=x.flip(1)\n",
    "        return x,y\n",
    "    def __len__(self):\n",
    "        return self.max_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.2476],\n",
      "         [-0.4090],\n",
      "         [-1.2719],\n",
      "         [ 2.5491]]]) tensor([[[ 2.5491],\n",
      "         [-1.2719],\n",
      "         [-0.4090],\n",
      "         [-1.2476]]])\n"
     ]
    }
   ],
   "source": [
    "a=torch.randn(1,4,1)\n",
    "print(a,a.flip(1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.7764,  0.2985,  0.9831, -0.0340, -0.4747,  0.5859,  0.5646,\n",
      "          -0.3377,  0.0232, -1.2997, -0.1950, -0.4325,  0.4108,  0.7329,\n",
      "           0.5236],\n",
      "         [ 0.0895, -0.4322, -0.8604, -0.4994,  0.2019, -0.3698,  0.3595,\n",
      "           0.5239,  0.1578,  0.3372,  0.0829,  1.3714,  0.4039,  1.0230,\n",
      "           0.5676]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# el entrenamiento\n",
    "def train(train_dataloader,encoder, decoder, init_decoder_state,optim_encoder,optim_decoder,epochs,):\n",
    "    #primero inicializamos todo\n",
    "\n",
    "\n",
    "    #bucle de epocas\n",
    "    for epoch in range(epochs):\n",
    "        #para cada epoca recorremos todos los batches\n",
    "\n",
    "        for i,(x_train,y_train) in enumerate(train_dataloader):\n",
    "\n",
    "            batch_size,len_seq,input_dim=x_train.size()\n",
    "\n",
    "            #para cada batch de datos, actualizamos los pesos \n",
    "            loss=torch.tensor(0)\n",
    "            optim_encoder.zero_grad()\n",
    "            optim_decoder.zero_grad()\n",
    "\n",
    "\n",
    "            list_encoder_hidden_states=torch.zeros(batch_size,len_seq,input_dim)\n",
    "            hidden_state_encoder=encoder.init_hidden(batch_size)\n",
    "\n",
    "            #y esto tiene dos fases, la obtener los estados del encoder, y la de procesarlo por el decoder\n",
    "            for j in range(len_seq):\n",
    "                input_aux=x_train[:,j,:]\n",
    "                #calculamoss los estados ocultos de cada instante\n",
    "                _,hidden_state_encoder=encoder(input_aux,hidden_state_encoder)\n",
    "                list_encoder_hidden_states[:,[j],:]=hidden_state_encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 7, 1]) torch.Size([32, 7, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n#y ahora el proceso de prediccion\\ninput_decoder=torch.torch() #aqui ponemos el token input\\nhidden_state_decoder=decoder.init_hidden(batch_size)\\n\\nfor j in range(len_seq):\\n\\n\\n    out1,new_hidden_state, attn_weigths=decoder(input_decoder,hidden_state_decoder,list_encoder_hidden_states)\\n    #tras aplicar\\n'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=DataFlip(7,1,200)\n",
    "dataloader=DataLoader(dataset,batch_size=32)\n",
    "x_train,y_train=next(iter(dataloader))\n",
    "print(x_train.shape,y_train.shape)\n",
    "\n",
    "hidden_dim=15\n",
    "encoder=EncoderRNN(1,hidden_dim)\n",
    "decoder=DecoderRNN(1,hidden_dim)\n",
    "\n",
    "\n",
    "batch_size,len_seq,input_dim=x_train.size()\n",
    "\n",
    "#para cada batch de datos, actualizamos los pesos \n",
    "loss=torch.tensor(0)\n",
    "\n",
    "hidden_state_encoder=encoder.init_hidden(batch_size)\n",
    "\n",
    "#y esto tiene dos fases, la obtener los estados del encoder, y la de procesarlo por el decoder\n",
    "\n",
    "#calculamoss los estados ocultos de cada instante\n",
    "list_encoder_hidden_states,_=encoder(x_train,hidden_state_encoder)\n",
    "hidden_state_encoder_transposed=hidden_state_encoder.transpose(0,1)\n",
    "list_encoder_hidden_states[:,j,:]=hidden_state_encoder\n",
    "\n",
    "\n",
    "#y ahora el proceso de prediccion\n",
    "input_decoder=torch.torch() #aqui ponemos el token input\n",
    "hidden_state_decoder=decoder.init_hidden(batch_size)\n",
    "\n",
    "for j in range(len_seq):\n",
    "\n",
    "\n",
    "    out1,new_hidden_state, attn_weigths=decoder(input_decoder,hidden_state_decoder,list_encoder_hidden_states)\n",
    "    #tras aplicar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "(8, 7)\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "(8, 7)\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "(8, 7)\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "(8, 7)\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "(8, 7)\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "(8, 7)\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "(8, 7)\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "(8, 7)\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "(8, 7)\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "(8, 7)\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "(8, 7)\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "(8, 7)\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 1])\n",
      "(8, 7)\n"
     ]
    }
   ],
   "source": [
    "dataset=DataFlip(7,1,200)\n",
    "dataloader=DataLoader(dataset,batch_size=32)\n",
    "\n",
    "hidden_dim=15\n",
    "encoder=EncoderRNN(1,hidden_dim)\n",
    "decoder=DecoderRNN(1,hidden_dim)\n",
    "\n",
    "optim_enc=optim.Adam(encoder.parameters())\n",
    "optim_dec=optim.Adam(decoder.parameters())\n",
    "\n",
    "\n",
    "batch_size,len_seq,input_dim=x_train.size()\n",
    "for i in range(13):\n",
    "\n",
    "    #para cada batch de datos, actualizamos los pesos \n",
    "    loss=torch.tensor(0)\n",
    "\n",
    "    hidden_state_encoder=encoder.init_hidden(batch_size)\n",
    "\n",
    "    #y esto tiene dos fases, la obtener los estados del encoder, y la de procesarlo por el decoder\n",
    "\n",
    "    #calculamoss los estados ocultos de cada instante\n",
    "    list_encoder_hidden_states,_=encoder(x_train,hidden_state_encoder)\n",
    "\n",
    "    prev_input=torch.ones(batch_size,1,input_dim)*-1.0\n",
    "    result=[]\n",
    "    for i in range(7):\n",
    "        out1,new_hidden_state, attn_weigths=decoder(prev_input,hidden_state_encoder,list_encoder_hidden_states)\n",
    "        prev_input=out1\n",
    "        result.append(out1.detach().numpy().squeeze(-1))\n",
    "        print(out1.shape)\n",
    "\n",
    "    result=np.array(result).squeeze(-1).swapaxes(0,1)\n",
    "    print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=DataFlip(7,1,32*40)\n",
    "dataloader=DataLoader(dataset,batch_size=32)\n",
    "x_train,y_train=next(iter(dataloader))\n",
    "\n",
    "hidden_dim=15\n",
    "encoder=EncoderRNN(1,hidden_dim)\n",
    "decoder=DecoderRNN(1,hidden_dim)\n",
    "\n",
    "optim_enc=optim.Adam(encoder.parameters(),lr=0.001)\n",
    "optim_dec=optim.Adam(decoder.parameters(),lr=0.001)\n",
    "loss_criterion=nn.MSELoss()\n",
    "\n",
    "batch_size,len_seq,input_dim=x_train.size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 209610.3427734375\n",
      "1 192321.716796875\n",
      "2 175830.59057617188\n",
      "3 176296.1962890625\n",
      "4 163382.841796875\n",
      "5 154274.96533203125\n",
      "6 152983.55493164062\n",
      "7 141264.06396484375\n",
      "8 133460.99145507812\n",
      "9 132570.58520507812\n",
      "10 127675.125\n",
      "11 120682.82153320312\n",
      "12 115584.0439453125\n",
      "13 111789.74291992188\n",
      "14 111465.60961914062\n",
      "15 104764.48510742188\n",
      "16 100888.92602539062\n",
      "17 100310.55737304688\n",
      "18 95794.86584472656\n",
      "19 93890.21459960938\n",
      "20 87806.16967773438\n",
      "21 85426.93603515625\n",
      "22 84183.86267089844\n",
      "23 80753.48034667969\n",
      "24 79616.46704101562\n",
      "25 78753.96301269531\n",
      "26 76904.23901367188\n",
      "27 74308.78601074219\n",
      "28 74126.646484375\n",
      "29 72410.24536132812\n",
      "30 70906.36987304688\n",
      "31 68982.98083496094\n",
      "32 68674.54858398438\n",
      "33 67154.53723144531\n",
      "34 65322.45056152344\n",
      "35 65828.6845703125\n",
      "36 65797.76647949219\n",
      "37 63149.86340332031\n",
      "38 62051.468994140625\n",
      "39 61626.66455078125\n",
      "40 60830.54150390625\n",
      "41 61425.84802246094\n",
      "42 61757.26416015625\n",
      "43 60232.4873046875\n",
      "44 61155.92126464844\n",
      "45 60602.77770996094\n",
      "46 59451.576171875\n",
      "47 59451.63342285156\n",
      "48 60103.75451660156\n",
      "49 58910.18701171875\n",
      "50 58368.814697265625\n",
      "51 58687.04992675781\n",
      "52 57325.630859375\n",
      "53 57623.403076171875\n",
      "54 56592.513916015625\n",
      "55 54898.683349609375\n",
      "56 53510.69384765625\n",
      "57 53852.966064453125\n",
      "58 52911.34777832031\n",
      "59 53102.26281738281\n",
      "60 52703.1474609375\n",
      "61 51486.2158203125\n",
      "62 49746.24462890625\n",
      "63 49520.56604003906\n",
      "64 48484.09033203125\n",
      "65 47737.83898925781\n",
      "66 47245.41076660156\n",
      "67 45617.82598876953\n",
      "68 45205.44030761719\n",
      "69 43865.09588623047\n",
      "70 44232.621032714844\n",
      "71 42772.25048828125\n",
      "72 41639.31365966797\n",
      "73 42321.38690185547\n",
      "74 41243.79266357422\n",
      "75 39764.08642578125\n",
      "76 39019.317443847656\n",
      "77 38860.853088378906\n",
      "78 36752.86145019531\n",
      "79 36380.24688720703\n",
      "80 35857.489501953125\n",
      "81 35150.15979003906\n",
      "82 32222.192016601562\n",
      "83 32728.808959960938\n",
      "84 30688.82208251953\n",
      "85 29965.09942626953\n",
      "86 28667.661560058594\n",
      "87 28602.419860839844\n",
      "88 26760.079833984375\n",
      "89 26957.32342529297\n",
      "90 25537.654907226562\n",
      "91 25029.34356689453\n",
      "92 24432.674865722656\n",
      "93 23297.167755126953\n",
      "94 22812.304077148438\n",
      "95 22313.631317138672\n",
      "96 21178.09262084961\n",
      "97 20681.70068359375\n",
      "98 20252.177215576172\n",
      "99 19558.012329101562\n",
      "100 19022.242401123047\n",
      "101 17729.768829345703\n",
      "102 17404.025665283203\n",
      "103 17594.592071533203\n",
      "104 16534.15072631836\n",
      "105 16056.636260986328\n",
      "106 15715.843231201172\n",
      "107 14897.137634277344\n",
      "108 14910.956573486328\n",
      "109 14696.966827392578\n",
      "110 13539.795471191406\n",
      "111 13539.895751953125\n",
      "112 13113.036529541016\n",
      "113 12728.515625\n",
      "114 12009.679336547852\n",
      "115 11881.476333618164\n",
      "116 11113.823532104492\n",
      "117 11246.26171875\n",
      "118 10494.297912597656\n",
      "119 10750.661209106445\n",
      "120 10157.662780761719\n",
      "121 9881.245040893555\n",
      "122 9883.122817993164\n",
      "123 9380.306091308594\n",
      "124 9392.274276733398\n",
      "125 9011.212341308594\n",
      "126 8338.950454711914\n",
      "127 8009.799133300781\n",
      "128 8342.59457397461\n",
      "129 7613.533401489258\n",
      "130 8264.642211914062\n",
      "131 7484.356887817383\n",
      "132 7414.293106079102\n",
      "133 7514.131484985352\n",
      "134 7041.02995300293\n",
      "135 6469.185333251953\n",
      "136 6390.980278015137\n",
      "137 6528.126007080078\n",
      "138 6097.234428405762\n",
      "139 6039.714576721191\n",
      "140 6086.559814453125\n",
      "141 5702.246887207031\n",
      "142 5405.395614624023\n",
      "143 5317.534088134766\n",
      "144 5227.578842163086\n",
      "145 5308.720840454102\n",
      "146 4920.7707595825195\n",
      "147 4694.820343017578\n",
      "148 4510.171485900879\n",
      "149 4505.148040771484\n",
      "150 4147.927391052246\n",
      "151 4380.683982849121\n",
      "152 4267.23494720459\n",
      "153 4247.059432983398\n",
      "154 3834.020492553711\n",
      "155 3846.3449325561523\n",
      "156 3723.6904678344727\n",
      "157 3752.6325073242188\n",
      "158 3570.0199584960938\n",
      "159 3453.4058990478516\n",
      "160 3185.081268310547\n",
      "161 3166.7881622314453\n",
      "162 3033.170166015625\n",
      "163 3021.146312713623\n",
      "164 2939.1759490966797\n",
      "165 2814.6362838745117\n",
      "166 2670.7254066467285\n",
      "167 2745.2909965515137\n",
      "168 2651.2931327819824\n",
      "169 2449.523780822754\n",
      "170 2498.623908996582\n",
      "171 2420.693500518799\n",
      "172 2413.1158485412598\n",
      "173 2384.903694152832\n",
      "174 2299.338939666748\n",
      "175 2058.3793869018555\n",
      "176 2199.0470809936523\n",
      "177 2054.6763191223145\n",
      "178 1964.1122283935547\n",
      "179 1983.6319885253906\n",
      "180 2108.310085296631\n",
      "181 1902.709156036377\n",
      "182 1859.0384330749512\n",
      "183 1661.947265625\n",
      "184 1664.0068550109863\n",
      "185 1595.298053741455\n",
      "186 1578.1110191345215\n",
      "187 1593.708267211914\n",
      "188 1474.4818706512451\n",
      "189 1508.6630821228027\n",
      "190 1379.161449432373\n",
      "191 1522.5216274261475\n",
      "192 1388.1619663238525\n",
      "193 1274.8647632598877\n",
      "194 1282.4470882415771\n",
      "195 1204.1006355285645\n",
      "196 1272.9767818450928\n",
      "197 1175.4284534454346\n",
      "198 1197.1949157714844\n",
      "199 1163.7524547576904\n"
     ]
    }
   ],
   "source": [
    "for epoca in range(200):\n",
    "    total_loss=0\n",
    "    for j,(x_train,y_train) in enumerate(dataloader):\n",
    "        optim_dec.zero_grad()\n",
    "        optim_enc.zero_grad()\n",
    "        #para cada batch de datos, actualizamos los pesos \n",
    "        loss=torch.tensor(0.0)\n",
    "        \n",
    "        hidden_state_encoder=encoder.init_hidden(batch_size)\n",
    "\n",
    "        #y esto tiene dos fases, la obtener los estados del encoder, y la de procesarlo por el decoder\n",
    "\n",
    "        #calculamoss los estados ocultos de cada instante\n",
    "        list_encoder_hidden_states,last_encoder_hidden=encoder(x_train,hidden_state_encoder)\n",
    "\n",
    "        prev_input=torch.ones(batch_size,1,input_dim)*-1.0\n",
    "\n",
    "        result=[]\n",
    "        for i in range(7):\n",
    "            out1,new_hidden_state, attn_weigths=decoder(prev_input,last_encoder_hidden,list_encoder_hidden_states)\n",
    "            last_encoder_hidden=new_hidden_state\n",
    "            prev_input=y_train[:,[i]]\n",
    "            result.append(out1)\n",
    "            loss+=loss_criterion(out1,y_train[:,[i]])\n",
    "        result2=torch.cat(result,dim=-2)\n",
    "        loss.backward()\n",
    "        optim_dec.step()\n",
    "        optim_enc.step()\n",
    "        total_loss+=loss.item()\n",
    "    print(epoca,total_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=DataFlip(7,1,32*2)\n",
    "dataloader=DataLoader(dataset,batch_size=32)\n",
    "x_train,y_train=next(iter(dataloader))\n",
    "\n",
    "#para cada batch de datos, actualizamos los pesos \n",
    "loss=torch.tensor(0.0)\n",
    "\n",
    "hidden_state_encoder=encoder.init_hidden(batch_size)\n",
    "\n",
    "#y esto tiene dos fases, la obtener los estados del encoder, y la de procesarlo por el decoder\n",
    "\n",
    "#calculamoss los estados ocultos de cada instante\n",
    "list_encoder_hidden_states,last_encoder_hidden=encoder(x_train,hidden_state_encoder)\n",
    "\n",
    "prev_input=torch.ones(batch_size,1,input_dim)*-1.0\n",
    "\n",
    "result=[]\n",
    "attn=[]\n",
    "for i in range(7):\n",
    "    out1,new_hidden_state, attn_weigths=decoder(prev_input,last_encoder_hidden,list_encoder_hidden_states)\n",
    "    last_encoder_hidden=new_hidden_state\n",
    "    prev_input=out1\n",
    "    result.append(out1)\n",
    "    attn.append(attn_weigths)\n",
    "    loss+=loss_criterion(out1,y_train[:,[i]])\n",
    "result2=torch.cat(result,dim=-2)\n",
    "atten=torch.cat(attn,dim=-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "       [0.3, 0.5, 0.1, 0. , 0. , 0. , 0. ],\n",
       "       [0.3, 0.5, 0.1, 0. , 0. , 0. , 0. ],\n",
       "       [0.2, 0.4, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "       [0.1, 0.2, 0. , 0.1, 0.1, 0.3, 0.2],\n",
       "       [0. , 0.1, 0. , 0.1, 0.2, 0.3, 0.3],\n",
       "       [0. , 0.1, 0. , 0.1, 0.2, 0.3, 0.3]], dtype=float32)"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atten[1].detach().numpy().round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fbab317ccd0>"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(atten[9].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[15.1777, 12.0000],\n",
      "        [36.3473, 47.0000],\n",
      "        [21.0382,  5.0000],\n",
      "        [20.1178, 39.0000],\n",
      "        [13.9750,  3.0000],\n",
      "        [21.8302, 30.0000],\n",
      "        [28.7812, 33.0000]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(torch.cat([result2[2],y_train[2]],dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 5, 3]) torch.Size([5, 7, 3]) tensor(True)\n"
     ]
    }
   ],
   "source": [
    "#vamos a probar como funciona la GRU y el output y tal\n",
    "RNN=nn.GRU(15,3,batch_first=True)\n",
    "x_test=torch.randint(0,100,(5,7,15)).float()\n",
    "hidden_state_encoder=torch.ones(1,5,3)\n",
    "list_encoder_hidden_states=torch.zeros(7,5,3)\n",
    "for j in range(7):\n",
    "    input_aux=x_test[:,[j],:]\n",
    "    #calculamoss los estados ocultos de cada instante\n",
    "    _,hidden_state_encoder=RNN(input_aux,hidden_state_encoder)\n",
    "    list_encoder_hidden_states[j]=hidden_state_encoder\n",
    "\n",
    "output_total,_=RNN(x_test,torch.ones(1,5,3))\n",
    "print(list_encoder_hidden_states.shape,output_total.shape,torch.all(list_encoder_hidden_states==output_total.transpose(0,1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
