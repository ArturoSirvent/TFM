{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo con SELF-attention\n",
    "\n",
    "Vamos a contruir un mecanismo de atención, y vamos a darle un dataser creado a posta para ver como le presta atención a cada zona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  as np \n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "from torch import nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "import math\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construción mecanismo de atención\n",
    "\n",
    "Vamos a construir un bloque de atencion con pytorch, que toma in input y procesa la self-atention y luego eso entra a una red densa y ya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#primero el scaled dot product\n",
    "def scaled_dot_prod(q,k,v):\n",
    "    #multiplicamos q por k y luego de escalarlo y la softmax por v\n",
    "    d_k=q.size()[-1]\n",
    "    qk_mult=torch.matmul(q,k.transpose(-2,-1))\n",
    "    qk_mult_scalet=qk_mult/math.sqrt(d_k)\n",
    "    attention = F.softmax(qk_mult_scalet, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " tensor([[-0.0944,  0.1860],\n",
      "        [ 0.3006, -1.5918],\n",
      "        [-0.9720, -2.1569]])\n",
      "K\n",
      " tensor([[ 0.0980, -0.4792],\n",
      "        [ 0.9607,  0.6717],\n",
      "        [ 1.0861,  1.4763]])\n",
      "V\n",
      " tensor([[-1.0670,  0.6806],\n",
      "        [-0.3315,  1.1840],\n",
      "        [ 0.0938,  0.2343]])\n",
      "Values\n",
      " tensor([[-0.3982,  0.6844],\n",
      "        [-0.7937,  0.7520],\n",
      "        [-0.9777,  0.7133]])\n",
      "Attention\n",
      " tensor([[0.3022, 0.3319, 0.3659],\n",
      "        [0.6824, 0.2244, 0.0932],\n",
      "        [0.8919, 0.0852, 0.0229]])\n"
     ]
    }
   ],
   "source": [
    "seq_len, d_k = 3, 2\n",
    "q = torch.randn(seq_len, d_k)\n",
    "k = torch.randn(seq_len, d_k)\n",
    "v = torch.randn(seq_len, d_k)\n",
    "values, attention = scaled_dot_prod(q, k, v)\n",
    "print(\"Q\\n\", q)\n",
    "print(\"K\\n\", k)\n",
    "print(\"V\\n\", v)\n",
    "print(\"Values\\n\", values)\n",
    "print(\"Attention\\n\", attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.],\n",
      "        [ 4.],\n",
      "        [ 2.],\n",
      "        [ 1.],\n",
      "        [ 5.],\n",
      "        [13.],\n",
      "        [ 3.]])\n",
      "Q\n",
      " tensor([[ -0.8995,  -1.8378,   0.7896,   1.3730,   0.4092],\n",
      "        [ -3.5979,  -7.3512,   3.1585,   5.4918,   1.6367],\n",
      "        [ -1.7989,  -3.6756,   1.5793,   2.7459,   0.8183],\n",
      "        [ -0.8995,  -1.8378,   0.7896,   1.3730,   0.4092],\n",
      "        [ -4.4973,  -9.1890,   3.9482,   6.8648,   2.0458],\n",
      "        [-11.6930, -23.8915,  10.2653,  17.8485,   5.3192],\n",
      "        [ -2.6984,  -5.5134,   2.3689,   4.1189,   1.2275]])\n",
      "K\n",
      " tensor([[  0.1965,  -1.0360,  -1.7784,   0.9759,  -1.6176],\n",
      "        [  0.7861,  -4.1440,  -7.1137,   3.9035,  -6.4706],\n",
      "        [  0.3930,  -2.0720,  -3.5568,   1.9517,  -3.2353],\n",
      "        [  0.1965,  -1.0360,  -1.7784,   0.9759,  -1.6176],\n",
      "        [  0.9826,  -5.1800,  -8.8921,   4.8794,  -8.0882],\n",
      "        [  2.5547, -13.4681, -23.1194,  12.6863, -21.0294],\n",
      "        [  0.5895,  -3.1080,  -5.3352,   2.9276,  -4.8529]])\n",
      "V\n",
      " tensor([[ -2.3431,   0.8445,   0.2970,  -2.1989,   0.0991],\n",
      "        [ -9.3722,   3.3782,   1.1879,  -8.7956,   0.3965],\n",
      "        [ -4.6861,   1.6891,   0.5940,  -4.3978,   0.1982],\n",
      "        [ -2.3431,   0.8445,   0.2970,  -2.1989,   0.0991],\n",
      "        [-11.7153,   4.2227,   1.4849, -10.9945,   0.4956],\n",
      "        [-30.4598,  10.9790,   3.8608, -28.5857,   1.2885],\n",
      "        [ -7.0292,   2.5336,   0.8910,  -6.5967,   0.2973]])\n"
     ]
    }
   ],
   "source": [
    "#los vectores Q, K y V son creados por combinacion lineal de un imput, por ejemplo\n",
    "#tenemos las matrics M_Q, M_K y M_V, que tendran dimension (d_input x d_k) y calculamos los vectores resultantes asi input x M_Q = Q\n",
    "d_input=1 # esta es la dimension de los tokens (puede ser una palabra enmbebida en un espacion de 512 dims, o puede ser una variable o un vector de 3 variables etc.), no la cantidad de tokens\n",
    "d_k=5 # espacio al que llevamos ese vector o valor del token inicial\n",
    "\n",
    "vector_input=torch.tensor([1,4,2,1,5,13,3],dtype=torch.float32).unsqueeze(1) #este es el vector de tokens embebidos o variables. La primera dimension recorre las posiciones se la sentencia input\n",
    "print(vector_input)\n",
    "M_Q=torch.randn(d_input,d_k)\n",
    "M_K=torch.randn(d_input,d_k)\n",
    "M_V=torch.randn(d_input,d_k)\n",
    "\n",
    "Q=torch.matmul(vector_input,M_Q)\n",
    "K=torch.matmul(vector_input,M_K)\n",
    "V=torch.matmul(vector_input,M_V)\n",
    "\n",
    "print(\"Q\\n\", Q)\n",
    "print(\"K\\n\", K)\n",
    "print(\"V\\n\", V)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNetwork(nn.Module):\n",
    "    def __init__(self, d_input, d_k, output_dim=1):\n",
    "        super(CustomNetwork, self).__init__()\n",
    "        \n",
    "        self.d_input = d_input\n",
    "        self.d_k = d_k\n",
    "        self.M_Q = nn.Parameter(torch.randn(d_input, d_k))\n",
    "        self.M_K = nn.Parameter(torch.randn(d_input, d_k))\n",
    "        self.M_V = nn.Parameter(torch.randn(d_input, d_k))\n",
    "        \n",
    "        self.fc = nn.Linear(d_k, output_dim)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        # Original Transformer initialization, see PyTorch documentation\n",
    "        nn.init.xavier_uniform_(self.M_Q)\n",
    "        nn.init.xavier_uniform_(self.M_K)        \n",
    "        nn.init.xavier_uniform_(self.M_V)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "\n",
    "    def scaled_dot_prod(self, q, k, v):\n",
    "        d_k = q.size(-1)\n",
    "        attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "        attn_logits /= math.sqrt(d_k)\n",
    "        attention = F.softmax(attn_logits, dim=-1)\n",
    "        values = torch.matmul(attention, v)\n",
    "        return values, attention\n",
    "\n",
    "    def forward(self, vector_input,return_attention=False):\n",
    "        Q = torch.matmul(vector_input, self.M_Q)\n",
    "        K = torch.matmul(vector_input, self.M_K)\n",
    "        V = torch.matmul(vector_input, self.M_V)\n",
    "        \n",
    "        values, attention = self.scaled_dot_prod(Q, K, V)\n",
    "        output = self.fc(values)\n",
    "        if return_attention:\n",
    "            return output, attention\n",
    "        else:\n",
    "            return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output batch shape: torch.Size([5, 30, 1])\n",
      "Attention batch shape: torch.Size([5, 30, 30])\n"
     ]
    }
   ],
   "source": [
    "# Parámetros\n",
    "d_input = 1\n",
    "len_seq=30\n",
    "d_k = 64\n",
    "batch_size=5\n",
    "\n",
    "# Crear datos de entrada en batch\n",
    "vector_input = torch.randn(batch_size,len_seq,1)\n",
    "\n",
    "# Crear una instancia del modelo e inicializarla\n",
    "model = CustomNetwork(d_input, d_k)\n",
    "\n",
    "# Pasar los datos en batch a través de la red\n",
    "output, attention = model(vector_input)\n",
    "\n",
    "# Verificar las dimensiones de la salida y la atención\n",
    "print(\"Output batch shape:\", output.shape)\n",
    "print(\"Attention batch shape:\", attention.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('M_K', Parameter containing:\n",
      "tensor([[ 0.0256,  0.2029, -0.1877, -0.1117, -0.2966,  0.2992,  0.1874,  0.2010,\n",
      "         -0.2460, -0.0186, -0.0015, -0.0862,  0.2996, -0.0610, -0.0093,  0.2151,\n",
      "         -0.2368,  0.2650,  0.0980, -0.2230,  0.0031, -0.2553,  0.2466,  0.2958,\n",
      "         -0.0949,  0.1714,  0.0502,  0.2359,  0.1466,  0.0974, -0.1122, -0.2019,\n",
      "         -0.0389,  0.1214,  0.0739, -0.2429, -0.1113,  0.1410, -0.0775, -0.1452,\n",
      "         -0.2049, -0.1707,  0.1257,  0.1181,  0.1403,  0.2980, -0.0987,  0.0578,\n",
      "         -0.0730, -0.1133,  0.2384,  0.1551,  0.1908,  0.2250,  0.1743, -0.0056,\n",
      "         -0.2848, -0.2282, -0.1506,  0.1071, -0.1419,  0.1079, -0.0602,  0.1342]],\n",
      "       requires_grad=True)), ('M_Q', Parameter containing:\n",
      "tensor([[-0.2574,  0.1931,  0.2285,  0.2258,  0.1950, -0.2175,  0.1261, -0.2163,\n",
      "         -0.2250, -0.1306, -0.0960, -0.2297,  0.1838,  0.0979, -0.1350, -0.2819,\n",
      "         -0.2117,  0.0514, -0.0699, -0.0192,  0.1733, -0.2037,  0.2745, -0.1583,\n",
      "          0.2157, -0.0802, -0.2437,  0.1521,  0.0690, -0.2807, -0.0820, -0.1439,\n",
      "          0.0174, -0.0918, -0.0160,  0.2818, -0.0164, -0.1504, -0.1934,  0.1253,\n",
      "         -0.2023, -0.0488, -0.2794, -0.0120,  0.0185,  0.1169, -0.1732, -0.0339,\n",
      "          0.2286,  0.1951,  0.1170, -0.0336, -0.0977, -0.2806,  0.0448,  0.2260,\n",
      "         -0.1440, -0.0313,  0.1965, -0.2051, -0.2297, -0.2796, -0.0909, -0.0735]],\n",
      "       requires_grad=True)), ('M_V', Parameter containing:\n",
      "tensor([[-0.1659,  0.1374, -0.0197, -0.1804, -0.0280, -0.0571, -0.1055, -0.1084,\n",
      "          0.0539,  0.2532, -0.1499, -0.0261, -0.2831, -0.2381,  0.2201, -0.1985,\n",
      "          0.1994,  0.1205, -0.2523,  0.0467, -0.2756, -0.1631, -0.1039, -0.1591,\n",
      "          0.2225, -0.0361,  0.0977, -0.1556, -0.2469, -0.2421, -0.1918,  0.0006,\n",
      "         -0.2071, -0.2333,  0.0823, -0.0935,  0.1366, -0.1873,  0.0635, -0.0325,\n",
      "         -0.0605, -0.1757, -0.2959,  0.1794,  0.2832, -0.0140,  0.2067,  0.1653,\n",
      "          0.2764, -0.0565, -0.2250, -0.2118,  0.1378, -0.0609,  0.0235,  0.1134,\n",
      "          0.1722, -0.2474, -0.0600, -0.0988,  0.0651, -0.0274,  0.2696,  0.2758]],\n",
      "       requires_grad=True)), ('T_destination', ~T_destination), ('__annotations__', {'dump_patches': <class 'bool'>, '_version': <class 'int'>, 'training': <class 'bool'>, '_is_full_backward_hook': typing.Optional[bool], 'forward': typing.Callable[..., typing.Any], '__call__': typing.Callable[..., typing.Any]}), ('__call__', <bound method Module._call_impl of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('__class__', <class '__main__.CustomNetwork'>), ('__delattr__', <bound method Module.__delattr__ of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('__dict__', {'training': True, '_parameters': OrderedDict([('M_Q', Parameter containing:\n",
      "tensor([[-0.2574,  0.1931,  0.2285,  0.2258,  0.1950, -0.2175,  0.1261, -0.2163,\n",
      "         -0.2250, -0.1306, -0.0960, -0.2297,  0.1838,  0.0979, -0.1350, -0.2819,\n",
      "         -0.2117,  0.0514, -0.0699, -0.0192,  0.1733, -0.2037,  0.2745, -0.1583,\n",
      "          0.2157, -0.0802, -0.2437,  0.1521,  0.0690, -0.2807, -0.0820, -0.1439,\n",
      "          0.0174, -0.0918, -0.0160,  0.2818, -0.0164, -0.1504, -0.1934,  0.1253,\n",
      "         -0.2023, -0.0488, -0.2794, -0.0120,  0.0185,  0.1169, -0.1732, -0.0339,\n",
      "          0.2286,  0.1951,  0.1170, -0.0336, -0.0977, -0.2806,  0.0448,  0.2260,\n",
      "         -0.1440, -0.0313,  0.1965, -0.2051, -0.2297, -0.2796, -0.0909, -0.0735]],\n",
      "       requires_grad=True)), ('M_K', Parameter containing:\n",
      "tensor([[ 0.0256,  0.2029, -0.1877, -0.1117, -0.2966,  0.2992,  0.1874,  0.2010,\n",
      "         -0.2460, -0.0186, -0.0015, -0.0862,  0.2996, -0.0610, -0.0093,  0.2151,\n",
      "         -0.2368,  0.2650,  0.0980, -0.2230,  0.0031, -0.2553,  0.2466,  0.2958,\n",
      "         -0.0949,  0.1714,  0.0502,  0.2359,  0.1466,  0.0974, -0.1122, -0.2019,\n",
      "         -0.0389,  0.1214,  0.0739, -0.2429, -0.1113,  0.1410, -0.0775, -0.1452,\n",
      "         -0.2049, -0.1707,  0.1257,  0.1181,  0.1403,  0.2980, -0.0987,  0.0578,\n",
      "         -0.0730, -0.1133,  0.2384,  0.1551,  0.1908,  0.2250,  0.1743, -0.0056,\n",
      "         -0.2848, -0.2282, -0.1506,  0.1071, -0.1419,  0.1079, -0.0602,  0.1342]],\n",
      "       requires_grad=True)), ('M_V', Parameter containing:\n",
      "tensor([[-0.1659,  0.1374, -0.0197, -0.1804, -0.0280, -0.0571, -0.1055, -0.1084,\n",
      "          0.0539,  0.2532, -0.1499, -0.0261, -0.2831, -0.2381,  0.2201, -0.1985,\n",
      "          0.1994,  0.1205, -0.2523,  0.0467, -0.2756, -0.1631, -0.1039, -0.1591,\n",
      "          0.2225, -0.0361,  0.0977, -0.1556, -0.2469, -0.2421, -0.1918,  0.0006,\n",
      "         -0.2071, -0.2333,  0.0823, -0.0935,  0.1366, -0.1873,  0.0635, -0.0325,\n",
      "         -0.0605, -0.1757, -0.2959,  0.1794,  0.2832, -0.0140,  0.2067,  0.1653,\n",
      "          0.2764, -0.0565, -0.2250, -0.2118,  0.1378, -0.0609,  0.0235,  0.1134,\n",
      "          0.1722, -0.2474, -0.0600, -0.0988,  0.0651, -0.0274,  0.2696,  0.2758]],\n",
      "       requires_grad=True))]), '_buffers': OrderedDict(), '_non_persistent_buffers_set': set(), '_backward_hooks': OrderedDict(), '_is_full_backward_hook': None, '_forward_hooks': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_load_state_dict_post_hooks': OrderedDict(), '_modules': OrderedDict([('fc', Linear(in_features=64, out_features=1, bias=True))]), 'd_input': 1, 'd_k': 64}), ('__dir__', <bound method Module.__dir__ of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('__doc__', None), ('__eq__', <method-wrapper '__eq__' of CustomNetwork object at 0x7ff1ccecb640>), ('__format__', <built-in method __format__ of CustomNetwork object at 0x7ff1ccecb640>), ('__ge__', <method-wrapper '__ge__' of CustomNetwork object at 0x7ff1ccecb640>), ('__getattr__', <bound method Module.__getattr__ of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('__getattribute__', <method-wrapper '__getattribute__' of CustomNetwork object at 0x7ff1ccecb640>), ('__gt__', <method-wrapper '__gt__' of CustomNetwork object at 0x7ff1ccecb640>), ('__hash__', <method-wrapper '__hash__' of CustomNetwork object at 0x7ff1ccecb640>), ('__init__', <bound method CustomNetwork.__init__ of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('__init_subclass__', <built-in method __init_subclass__ of type object at 0x524b110>), ('__le__', <method-wrapper '__le__' of CustomNetwork object at 0x7ff1ccecb640>), ('__lt__', <method-wrapper '__lt__' of CustomNetwork object at 0x7ff1ccecb640>), ('__module__', '__main__'), ('__ne__', <method-wrapper '__ne__' of CustomNetwork object at 0x7ff1ccecb640>), ('__new__', <built-in method __new__ of type object at 0x745800>), ('__reduce__', <built-in method __reduce__ of CustomNetwork object at 0x7ff1ccecb640>), ('__reduce_ex__', <built-in method __reduce_ex__ of CustomNetwork object at 0x7ff1ccecb640>), ('__repr__', <bound method Module.__repr__ of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('__setattr__', <bound method Module.__setattr__ of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('__setstate__', <bound method Module.__setstate__ of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('__sizeof__', <built-in method __sizeof__ of CustomNetwork object at 0x7ff1ccecb640>), ('__str__', <method-wrapper '__str__' of CustomNetwork object at 0x7ff1ccecb640>), ('__subclasshook__', <built-in method __subclasshook__ of type object at 0x524b110>), ('__weakref__', None), ('_apply', <bound method Module._apply of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('_backward_hooks', OrderedDict()), ('_buffers', OrderedDict()), ('_call_impl', <bound method Module._call_impl of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('_forward_hooks', OrderedDict()), ('_forward_pre_hooks', OrderedDict()), ('_get_backward_hooks', <bound method Module._get_backward_hooks of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('_get_name', <bound method Module._get_name of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('_is_full_backward_hook', None), ('_load_from_state_dict', <bound method Module._load_from_state_dict of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('_load_state_dict_post_hooks', OrderedDict()), ('_load_state_dict_pre_hooks', OrderedDict()), ('_maybe_warn_non_full_backward_hook', <bound method Module._maybe_warn_non_full_backward_hook of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('_modules', OrderedDict([('fc', Linear(in_features=64, out_features=1, bias=True))])), ('_named_members', <bound method Module._named_members of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('_non_persistent_buffers_set', set()), ('_parameters', OrderedDict([('M_Q', Parameter containing:\n",
      "tensor([[-0.2574,  0.1931,  0.2285,  0.2258,  0.1950, -0.2175,  0.1261, -0.2163,\n",
      "         -0.2250, -0.1306, -0.0960, -0.2297,  0.1838,  0.0979, -0.1350, -0.2819,\n",
      "         -0.2117,  0.0514, -0.0699, -0.0192,  0.1733, -0.2037,  0.2745, -0.1583,\n",
      "          0.2157, -0.0802, -0.2437,  0.1521,  0.0690, -0.2807, -0.0820, -0.1439,\n",
      "          0.0174, -0.0918, -0.0160,  0.2818, -0.0164, -0.1504, -0.1934,  0.1253,\n",
      "         -0.2023, -0.0488, -0.2794, -0.0120,  0.0185,  0.1169, -0.1732, -0.0339,\n",
      "          0.2286,  0.1951,  0.1170, -0.0336, -0.0977, -0.2806,  0.0448,  0.2260,\n",
      "         -0.1440, -0.0313,  0.1965, -0.2051, -0.2297, -0.2796, -0.0909, -0.0735]],\n",
      "       requires_grad=True)), ('M_K', Parameter containing:\n",
      "tensor([[ 0.0256,  0.2029, -0.1877, -0.1117, -0.2966,  0.2992,  0.1874,  0.2010,\n",
      "         -0.2460, -0.0186, -0.0015, -0.0862,  0.2996, -0.0610, -0.0093,  0.2151,\n",
      "         -0.2368,  0.2650,  0.0980, -0.2230,  0.0031, -0.2553,  0.2466,  0.2958,\n",
      "         -0.0949,  0.1714,  0.0502,  0.2359,  0.1466,  0.0974, -0.1122, -0.2019,\n",
      "         -0.0389,  0.1214,  0.0739, -0.2429, -0.1113,  0.1410, -0.0775, -0.1452,\n",
      "         -0.2049, -0.1707,  0.1257,  0.1181,  0.1403,  0.2980, -0.0987,  0.0578,\n",
      "         -0.0730, -0.1133,  0.2384,  0.1551,  0.1908,  0.2250,  0.1743, -0.0056,\n",
      "         -0.2848, -0.2282, -0.1506,  0.1071, -0.1419,  0.1079, -0.0602,  0.1342]],\n",
      "       requires_grad=True)), ('M_V', Parameter containing:\n",
      "tensor([[-0.1659,  0.1374, -0.0197, -0.1804, -0.0280, -0.0571, -0.1055, -0.1084,\n",
      "          0.0539,  0.2532, -0.1499, -0.0261, -0.2831, -0.2381,  0.2201, -0.1985,\n",
      "          0.1994,  0.1205, -0.2523,  0.0467, -0.2756, -0.1631, -0.1039, -0.1591,\n",
      "          0.2225, -0.0361,  0.0977, -0.1556, -0.2469, -0.2421, -0.1918,  0.0006,\n",
      "         -0.2071, -0.2333,  0.0823, -0.0935,  0.1366, -0.1873,  0.0635, -0.0325,\n",
      "         -0.0605, -0.1757, -0.2959,  0.1794,  0.2832, -0.0140,  0.2067,  0.1653,\n",
      "          0.2764, -0.0565, -0.2250, -0.2118,  0.1378, -0.0609,  0.0235,  0.1134,\n",
      "          0.1722, -0.2474, -0.0600, -0.0988,  0.0651, -0.0274,  0.2696,  0.2758]],\n",
      "       requires_grad=True))])), ('_register_load_state_dict_pre_hook', <bound method Module._register_load_state_dict_pre_hook of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('_register_state_dict_hook', <bound method Module._register_state_dict_hook of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('_replicate_for_data_parallel', <bound method Module._replicate_for_data_parallel of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('_reset_parameters', <bound method CustomNetwork._reset_parameters of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('_save_to_state_dict', <bound method Module._save_to_state_dict of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('_slow_forward', <bound method Module._slow_forward of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('_state_dict_hooks', OrderedDict()), ('_version', 1), ('add_module', <bound method Module.add_module of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('apply', <bound method Module.apply of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('bfloat16', <bound method Module.bfloat16 of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('buffers', <bound method Module.buffers of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('children', <bound method Module.children of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('cpu', <bound method Module.cpu of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('cuda', <bound method Module.cuda of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('d_input', 1), ('d_k', 64), ('double', <bound method Module.double of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('dump_patches', False), ('eval', <bound method Module.eval of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('extra_repr', <bound method Module.extra_repr of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('fc', Linear(in_features=64, out_features=1, bias=True)), ('float', <bound method Module.float of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('forward', <bound method CustomNetwork.forward of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('get_buffer', <bound method Module.get_buffer of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('get_extra_state', <bound method Module.get_extra_state of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('get_parameter', <bound method Module.get_parameter of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('get_submodule', <bound method Module.get_submodule of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('half', <bound method Module.half of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('ipu', <bound method Module.ipu of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('load_state_dict', <bound method Module.load_state_dict of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('modules', <bound method Module.modules of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('named_buffers', <bound method Module.named_buffers of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('named_children', <bound method Module.named_children of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('named_modules', <bound method Module.named_modules of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('named_parameters', <bound method Module.named_parameters of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('parameters', <bound method Module.parameters of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('register_backward_hook', <bound method Module.register_backward_hook of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('register_buffer', <bound method Module.register_buffer of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('register_forward_hook', <bound method Module.register_forward_hook of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('register_forward_pre_hook', <bound method Module.register_forward_pre_hook of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('register_full_backward_hook', <bound method Module.register_full_backward_hook of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('register_load_state_dict_post_hook', <bound method Module.register_load_state_dict_post_hook of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('register_module', <bound method Module.register_module of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('register_parameter', <bound method Module.register_parameter of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('requires_grad_', <bound method Module.requires_grad_ of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('scaled_dot_prod', <bound method CustomNetwork.scaled_dot_prod of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('set_extra_state', <bound method Module.set_extra_state of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('share_memory', <bound method Module.share_memory of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('state_dict', <bound method Module.state_dict of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('to', <bound method Module.to of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('to_empty', <bound method Module.to_empty of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('train', <bound method Module.train of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('training', True), ('type', <bound method Module.type of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('xpu', <bound method Module.xpu of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>), ('zero_grad', <bound method Module.zero_grad of CustomNetwork(\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")>)]\n",
      "(*input, **kwargs)\n",
      "None\n",
      "M_K: Parameter containing:\n",
      "tensor([[ 0.0256,  0.2029, -0.1877, -0.1117, -0.2966,  0.2992,  0.1874,  0.2010,\n",
      "         -0.2460, -0.0186, -0.0015, -0.0862,  0.2996, -0.0610, -0.0093,  0.2151,\n",
      "         -0.2368,  0.2650,  0.0980, -0.2230,  0.0031, -0.2553,  0.2466,  0.2958,\n",
      "         -0.0949,  0.1714,  0.0502,  0.2359,  0.1466,  0.0974, -0.1122, -0.2019,\n",
      "         -0.0389,  0.1214,  0.0739, -0.2429, -0.1113,  0.1410, -0.0775, -0.1452,\n",
      "         -0.2049, -0.1707,  0.1257,  0.1181,  0.1403,  0.2980, -0.0987,  0.0578,\n",
      "         -0.0730, -0.1133,  0.2384,  0.1551,  0.1908,  0.2250,  0.1743, -0.0056,\n",
      "         -0.2848, -0.2282, -0.1506,  0.1071, -0.1419,  0.1079, -0.0602,  0.1342]],\n",
      "       requires_grad=True)\n",
      "M_Q: Parameter containing:\n",
      "tensor([[-0.2574,  0.1931,  0.2285,  0.2258,  0.1950, -0.2175,  0.1261, -0.2163,\n",
      "         -0.2250, -0.1306, -0.0960, -0.2297,  0.1838,  0.0979, -0.1350, -0.2819,\n",
      "         -0.2117,  0.0514, -0.0699, -0.0192,  0.1733, -0.2037,  0.2745, -0.1583,\n",
      "          0.2157, -0.0802, -0.2437,  0.1521,  0.0690, -0.2807, -0.0820, -0.1439,\n",
      "          0.0174, -0.0918, -0.0160,  0.2818, -0.0164, -0.1504, -0.1934,  0.1253,\n",
      "         -0.2023, -0.0488, -0.2794, -0.0120,  0.0185,  0.1169, -0.1732, -0.0339,\n",
      "          0.2286,  0.1951,  0.1170, -0.0336, -0.0977, -0.2806,  0.0448,  0.2260,\n",
      "         -0.1440, -0.0313,  0.1965, -0.2051, -0.2297, -0.2796, -0.0909, -0.0735]],\n",
      "       requires_grad=True)\n",
      "M_V: Parameter containing:\n",
      "tensor([[-0.1659,  0.1374, -0.0197, -0.1804, -0.0280, -0.0571, -0.1055, -0.1084,\n",
      "          0.0539,  0.2532, -0.1499, -0.0261, -0.2831, -0.2381,  0.2201, -0.1985,\n",
      "          0.1994,  0.1205, -0.2523,  0.0467, -0.2756, -0.1631, -0.1039, -0.1591,\n",
      "          0.2225, -0.0361,  0.0977, -0.1556, -0.2469, -0.2421, -0.1918,  0.0006,\n",
      "         -0.2071, -0.2333,  0.0823, -0.0935,  0.1366, -0.1873,  0.0635, -0.0325,\n",
      "         -0.0605, -0.1757, -0.2959,  0.1794,  0.2832, -0.0140,  0.2067,  0.1653,\n",
      "          0.2764, -0.0565, -0.2250, -0.2118,  0.1378, -0.0609,  0.0235,  0.1134,\n",
      "          0.1722, -0.2474, -0.0600, -0.0988,  0.0651, -0.0274,  0.2696,  0.2758]],\n",
      "       requires_grad=True)\n",
      "T_destination: ~T_destination\n",
      "_backward_hooks: OrderedDict()\n",
      "_buffers: OrderedDict()\n",
      "_forward_hooks: OrderedDict()\n",
      "_forward_pre_hooks: OrderedDict()\n",
      "_is_full_backward_hook: None\n",
      "_load_state_dict_post_hooks: OrderedDict()\n",
      "_load_state_dict_pre_hooks: OrderedDict()\n",
      "_modules: OrderedDict([('fc', Linear(in_features=64, out_features=1, bias=True))])\n",
      "_non_persistent_buffers_set: set()\n",
      "_parameters: OrderedDict([('M_Q', Parameter containing:\n",
      "tensor([[-0.2574,  0.1931,  0.2285,  0.2258,  0.1950, -0.2175,  0.1261, -0.2163,\n",
      "         -0.2250, -0.1306, -0.0960, -0.2297,  0.1838,  0.0979, -0.1350, -0.2819,\n",
      "         -0.2117,  0.0514, -0.0699, -0.0192,  0.1733, -0.2037,  0.2745, -0.1583,\n",
      "          0.2157, -0.0802, -0.2437,  0.1521,  0.0690, -0.2807, -0.0820, -0.1439,\n",
      "          0.0174, -0.0918, -0.0160,  0.2818, -0.0164, -0.1504, -0.1934,  0.1253,\n",
      "         -0.2023, -0.0488, -0.2794, -0.0120,  0.0185,  0.1169, -0.1732, -0.0339,\n",
      "          0.2286,  0.1951,  0.1170, -0.0336, -0.0977, -0.2806,  0.0448,  0.2260,\n",
      "         -0.1440, -0.0313,  0.1965, -0.2051, -0.2297, -0.2796, -0.0909, -0.0735]],\n",
      "       requires_grad=True)), ('M_K', Parameter containing:\n",
      "tensor([[ 0.0256,  0.2029, -0.1877, -0.1117, -0.2966,  0.2992,  0.1874,  0.2010,\n",
      "         -0.2460, -0.0186, -0.0015, -0.0862,  0.2996, -0.0610, -0.0093,  0.2151,\n",
      "         -0.2368,  0.2650,  0.0980, -0.2230,  0.0031, -0.2553,  0.2466,  0.2958,\n",
      "         -0.0949,  0.1714,  0.0502,  0.2359,  0.1466,  0.0974, -0.1122, -0.2019,\n",
      "         -0.0389,  0.1214,  0.0739, -0.2429, -0.1113,  0.1410, -0.0775, -0.1452,\n",
      "         -0.2049, -0.1707,  0.1257,  0.1181,  0.1403,  0.2980, -0.0987,  0.0578,\n",
      "         -0.0730, -0.1133,  0.2384,  0.1551,  0.1908,  0.2250,  0.1743, -0.0056,\n",
      "         -0.2848, -0.2282, -0.1506,  0.1071, -0.1419,  0.1079, -0.0602,  0.1342]],\n",
      "       requires_grad=True)), ('M_V', Parameter containing:\n",
      "tensor([[-0.1659,  0.1374, -0.0197, -0.1804, -0.0280, -0.0571, -0.1055, -0.1084,\n",
      "          0.0539,  0.2532, -0.1499, -0.0261, -0.2831, -0.2381,  0.2201, -0.1985,\n",
      "          0.1994,  0.1205, -0.2523,  0.0467, -0.2756, -0.1631, -0.1039, -0.1591,\n",
      "          0.2225, -0.0361,  0.0977, -0.1556, -0.2469, -0.2421, -0.1918,  0.0006,\n",
      "         -0.2071, -0.2333,  0.0823, -0.0935,  0.1366, -0.1873,  0.0635, -0.0325,\n",
      "         -0.0605, -0.1757, -0.2959,  0.1794,  0.2832, -0.0140,  0.2067,  0.1653,\n",
      "          0.2764, -0.0565, -0.2250, -0.2118,  0.1378, -0.0609,  0.0235,  0.1134,\n",
      "          0.1722, -0.2474, -0.0600, -0.0988,  0.0651, -0.0274,  0.2696,  0.2758]],\n",
      "       requires_grad=True))])\n",
      "_state_dict_hooks: OrderedDict()\n",
      "_version: 1\n",
      "d_input: 1\n",
      "d_k: 64\n",
      "dump_patches: False\n",
      "training: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "import inspect\n",
    "\n",
    "print(inspect.getmembers(model))  # Obtén todos los miembros del objeto param\n",
    "print(inspect.signature(model))  # Muestra la firma de la función torch.matmul\n",
    "print(inspect.getdoc(model))  # Muestra la documentación del objeto param\n",
    "\"\"\"\n",
    "#ejemplo para ver todos los atributos\n",
    "for attr in dir(model):\n",
    "    if not callable(getattr(model, attr)) and not attr.startswith(\"__\"):\n",
    "        print(f\"{attr}: {getattr(model, attr)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "En esta sección construimos un dataset artificial que nos va a permitir ver si el modelo le está prestando atención a la zona que tiene la respuesta que busca.\n",
    "\n",
    "Esto lo vamos a hacer haciendo un data ser con correspondencias X->y, pero X tiene mucha \"basura\", tiene muchas zonas que no son las importantes.\n",
    "\n",
    "Para darle algo que pueda entender tenemos que saber **qué entiende**. El mecanismo que estamos implementado es de self-attention, el más sencillo podriamos decir. Entonces la \"pista\" que le indique al modelo DONDE mirar y porque, debe estar en la propia sentencia. Por ejemplo, tenemos una vector de vectores/tokens muy largo, queremos que cuando uno del los tokens sea =1 todo, sean las 3 palabras/token/vectores anteriores las que nos den los vectores que si los multiplicamos, nos da el label Y buscado.\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a definir un generador de datos, para asi crear nuestro dataset.  \n",
    "Este puede ser un generador infinito que devuelve X e Y.  \n",
    "\n",
    "La clave esta en crear una estructura en la secuencia, por ejemplo:\n",
    "* Cuando un elemento de la secuencia vale 4 o 9, los elementos anteriores, el primero y el tercero anteriores, multiplicados y + 0.3, son igual al label Y buscado.\n",
    "\n",
    "Para hacerlo hacemos: \n",
    "* Vamos a generar vector entre 0 y 100, y vamos a evitar que ninguno valga 4 o 9.\n",
    "* Metemos en una posicion aleatoria un 4 o un 9 y 1 y 3 posiciones antes, metemos los dos valores que multiplicados y +0.3 son igual a y.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_lista(d_list=100,d_randint=100,not_in=[4,9],bias=0.3):\n",
    "    lista = []\n",
    "    i = 0\n",
    "    while len(lista) < d_list:\n",
    "        i = np.random.randint(0, d_randint)\n",
    "        if i not in not_in:\n",
    "            lista.append(i)\n",
    "        i += 1\n",
    "\n",
    "    #ahora tenemos la lista, vamos a tomar dos valores aleatorios y calcular y \n",
    "    seguir=True\n",
    "    while seguir:\n",
    "        i = np.random.randint(0, d_randint)\n",
    "        if i not in not_in:\n",
    "            x1=i\n",
    "            seguir=False\n",
    "        i += 1\n",
    "    seguir=True\n",
    "    while seguir:\n",
    "        i = np.random.randint(0, d_randint)\n",
    "        if i not in not_in:\n",
    "            x2=i\n",
    "            seguir=False\n",
    "        i += 1\n",
    "    y=x1*x2+bias\n",
    "\n",
    "    #y ahora los colocamos \n",
    "    indx_rand=np.random.randint(4, d_list)\n",
    "    lista[indx_rand]=np.random.choice(not_in)\n",
    "    lista[indx_rand-1]=x1\n",
    "    lista[indx_rand-3]=x2\n",
    "    \n",
    "    return np.array(lista), y\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(size,d_list=100,d_randint=100,not_in=[4,9],bias=0.3):\n",
    "    X=[]\n",
    "    y=[]\n",
    "    for _ in range(size):\n",
    "        x_element, y_element =generar_lista(d_list=d_list,d_randint=d_randint,not_in=not_in,bias=bias)\n",
    "        X.append(x_element)\n",
    "        y.append(y_element)\n",
    "\n",
    "    return np.array(X),np.array(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista, y =generar_lista()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[]\n",
    "for i in range(56):\n",
    "    indx=np.argwhere((lista==4)|(lista==9)).squeeze()\n",
    "    a.append(lista[indx-1]*lista[indx-3]+0.3==y)\n",
    "\n",
    "\n",
    "np.all(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 100) (10,)\n"
     ]
    }
   ],
   "source": [
    "X,y=get_dataset(10)\n",
    "print(X.shape,y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de la red\n",
    "Ya tenemos la red y ahora vamos a entrenarla para pode ver su capa de atencion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "At least one stride in the given numpy array is negative, and tensors with negative strides are not currently supported. (You can probably work around this by making a copy of your array  with array.copy().) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[39m.\u001b[39;49mfrom_numpy(np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mrandint(\u001b[39m0\u001b[39;49m,\u001b[39m100\u001b[39;49m,(\u001b[39m8000\u001b[39;49m,len_seq))[:,::\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m])\n",
      "\u001b[0;31mValueError\u001b[0m: At least one stride in the given numpy array is negative, and tensors with negative strides are not currently supported. (You can probably work around this by making a copy of your array  with array.copy().) "
     ]
    }
   ],
   "source": [
    "torch.from_numpy(np.random.randint(0,100,(8000,len_seq))[:,::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 30) (8000, 30)\n",
      "Epoch 1/200, Train Loss: 14.6969\n",
      "Epoch 1/200, Validation Loss: 12.5171\n",
      "Epoch 2/200, Train Loss: 15.3850\n",
      "Epoch 2/200, Validation Loss: 12.5171\n",
      "Epoch 3/200, Train Loss: 14.2378\n",
      "Epoch 3/200, Validation Loss: 12.5171\n",
      "Epoch 4/200, Train Loss: 14.5632\n",
      "Epoch 4/200, Validation Loss: 12.5171\n",
      "Epoch 5/200, Train Loss: 14.8955\n",
      "Epoch 5/200, Validation Loss: 12.5171\n",
      "Epoch 6/200, Train Loss: 14.4116\n",
      "Epoch 6/200, Validation Loss: 12.5171\n",
      "Epoch 7/200, Train Loss: 15.0868\n",
      "Epoch 7/200, Validation Loss: 12.5171\n",
      "Epoch 8/200, Train Loss: 15.1393\n",
      "Epoch 8/200, Validation Loss: 12.5171\n",
      "Epoch 9/200, Train Loss: 15.1754\n",
      "Epoch 9/200, Validation Loss: 12.5171\n",
      "Epoch 10/200, Train Loss: 11.9414\n",
      "Epoch 10/200, Validation Loss: 12.5171\n",
      "Epoch 11/200, Train Loss: 13.8109\n",
      "Epoch 11/200, Validation Loss: 12.5171\n",
      "Epoch 12/200, Train Loss: 14.6038\n",
      "Epoch 12/200, Validation Loss: 12.5171\n",
      "Epoch 13/200, Train Loss: 14.9930\n",
      "Epoch 13/200, Validation Loss: 12.5171\n",
      "Epoch 14/200, Train Loss: 14.5729\n",
      "Epoch 14/200, Validation Loss: 12.5171\n",
      "Epoch 15/200, Train Loss: 14.3941\n",
      "Epoch 15/200, Validation Loss: 12.5171\n",
      "Epoch 16/200, Train Loss: 14.4328\n",
      "Epoch 16/200, Validation Loss: 12.5171\n",
      "Epoch 17/200, Train Loss: 14.6548\n",
      "Epoch 17/200, Validation Loss: 12.5171\n",
      "Epoch 18/200, Train Loss: 11.7014\n",
      "Epoch 18/200, Validation Loss: 12.5171\n",
      "Epoch 19/200, Train Loss: 16.0413\n",
      "Epoch 19/200, Validation Loss: 12.5171\n",
      "Epoch 20/200, Train Loss: 14.6652\n",
      "Epoch 20/200, Validation Loss: 12.5171\n",
      "Epoch 21/200, Train Loss: 14.0076\n",
      "Epoch 21/200, Validation Loss: 12.5171\n",
      "Epoch 22/200, Train Loss: 14.4160\n",
      "Epoch 22/200, Validation Loss: 12.5171\n",
      "Epoch 23/200, Train Loss: 14.6233\n",
      "Epoch 23/200, Validation Loss: 12.5171\n",
      "Epoch 24/200, Train Loss: 13.5472\n",
      "Epoch 24/200, Validation Loss: 12.5171\n",
      "Epoch 25/200, Train Loss: 13.7634\n",
      "Epoch 25/200, Validation Loss: 12.5171\n",
      "Epoch 26/200, Train Loss: 13.7012\n",
      "Epoch 26/200, Validation Loss: 12.5171\n",
      "Epoch 27/200, Train Loss: 16.3346\n",
      "Epoch 27/200, Validation Loss: 12.5171\n",
      "Epoch 28/200, Train Loss: 15.2045\n",
      "Epoch 28/200, Validation Loss: 12.5171\n",
      "Epoch 29/200, Train Loss: 15.6996\n",
      "Epoch 29/200, Validation Loss: 12.5171\n",
      "Epoch 30/200, Train Loss: 14.5887\n",
      "Epoch 30/200, Validation Loss: 12.5171\n",
      "Epoch 31/200, Train Loss: 14.1151\n",
      "Epoch 31/200, Validation Loss: 12.5171\n",
      "Epoch 32/200, Train Loss: 14.1197\n",
      "Epoch 32/200, Validation Loss: 12.5171\n",
      "Epoch 33/200, Train Loss: 14.3805\n",
      "Epoch 33/200, Validation Loss: 12.5171\n",
      "Epoch 34/200, Train Loss: 13.8338\n",
      "Epoch 34/200, Validation Loss: 12.5171\n",
      "Epoch 35/200, Train Loss: 14.1547\n",
      "Epoch 35/200, Validation Loss: 12.5171\n",
      "Epoch 36/200, Train Loss: 15.3097\n",
      "Epoch 36/200, Validation Loss: 12.5171\n",
      "Epoch 37/200, Train Loss: 14.1723\n",
      "Epoch 37/200, Validation Loss: 12.5171\n",
      "Epoch 38/200, Train Loss: 16.8112\n",
      "Epoch 38/200, Validation Loss: 12.5171\n",
      "Epoch 39/200, Train Loss: 14.0951\n",
      "Epoch 39/200, Validation Loss: 12.5171\n",
      "Epoch 40/200, Train Loss: 14.9411\n",
      "Epoch 40/200, Validation Loss: 12.5171\n",
      "Epoch 41/200, Train Loss: 14.8616\n",
      "Epoch 41/200, Validation Loss: 12.5171\n",
      "Epoch 42/200, Train Loss: 12.9925\n",
      "Epoch 42/200, Validation Loss: 12.5171\n",
      "Epoch 43/200, Train Loss: 14.4411\n",
      "Epoch 43/200, Validation Loss: 12.5171\n",
      "Epoch 44/200, Train Loss: 11.8071\n",
      "Epoch 44/200, Validation Loss: 12.5171\n",
      "Epoch 45/200, Train Loss: 16.3106\n",
      "Epoch 45/200, Validation Loss: 12.5171\n",
      "Epoch 46/200, Train Loss: 16.0052\n",
      "Epoch 46/200, Validation Loss: 12.5171\n",
      "Epoch 47/200, Train Loss: 12.9227\n",
      "Epoch 47/200, Validation Loss: 12.5171\n",
      "Epoch 48/200, Train Loss: 17.2722\n",
      "Epoch 48/200, Validation Loss: 12.5171\n",
      "Epoch 49/200, Train Loss: 14.3917\n",
      "Epoch 49/200, Validation Loss: 12.5171\n",
      "Epoch 50/200, Train Loss: 16.0062\n",
      "Epoch 50/200, Validation Loss: 12.5171\n",
      "Epoch 51/200, Train Loss: 16.2848\n",
      "Epoch 51/200, Validation Loss: 12.5171\n",
      "Epoch 52/200, Train Loss: 15.4383\n",
      "Epoch 52/200, Validation Loss: 12.5171\n",
      "Epoch 53/200, Train Loss: 15.0638\n",
      "Epoch 53/200, Validation Loss: 12.5171\n",
      "Epoch 54/200, Train Loss: 14.4192\n",
      "Epoch 54/200, Validation Loss: 12.5171\n",
      "Epoch 55/200, Train Loss: 15.2710\n",
      "Epoch 55/200, Validation Loss: 12.5171\n",
      "Epoch 56/200, Train Loss: 16.8170\n",
      "Epoch 56/200, Validation Loss: 12.5171\n",
      "Epoch 57/200, Train Loss: 15.1536\n",
      "Epoch 57/200, Validation Loss: 12.5171\n",
      "Epoch 58/200, Train Loss: 13.8608\n",
      "Epoch 58/200, Validation Loss: 12.5171\n",
      "Epoch 59/200, Train Loss: 12.0558\n",
      "Epoch 59/200, Validation Loss: 12.5171\n",
      "Epoch 60/200, Train Loss: 14.1771\n",
      "Epoch 60/200, Validation Loss: 12.5171\n",
      "Epoch 61/200, Train Loss: 14.5825\n",
      "Epoch 61/200, Validation Loss: 12.5171\n",
      "Epoch 62/200, Train Loss: 17.8091\n",
      "Epoch 62/200, Validation Loss: 12.5171\n",
      "Epoch 63/200, Train Loss: 14.9245\n",
      "Epoch 63/200, Validation Loss: 12.5171\n",
      "Epoch 64/200, Train Loss: 14.1298\n",
      "Epoch 64/200, Validation Loss: 12.5171\n",
      "Epoch 65/200, Train Loss: 11.8147\n",
      "Epoch 65/200, Validation Loss: 12.5171\n",
      "Epoch 66/200, Train Loss: 13.1126\n",
      "Epoch 66/200, Validation Loss: 12.5171\n",
      "Epoch 67/200, Train Loss: 15.8514\n",
      "Epoch 67/200, Validation Loss: 12.5171\n",
      "Epoch 68/200, Train Loss: 12.9497\n",
      "Epoch 68/200, Validation Loss: 12.5171\n",
      "Epoch 69/200, Train Loss: 14.0050\n",
      "Epoch 69/200, Validation Loss: 12.5171\n",
      "Epoch 70/200, Train Loss: 15.0248\n",
      "Epoch 70/200, Validation Loss: 12.5171\n",
      "Epoch 71/200, Train Loss: 12.2424\n",
      "Epoch 71/200, Validation Loss: 12.5171\n",
      "Epoch 72/200, Train Loss: 13.0244\n",
      "Epoch 72/200, Validation Loss: 12.5171\n",
      "Epoch 73/200, Train Loss: 14.7384\n",
      "Epoch 73/200, Validation Loss: 12.5171\n",
      "Epoch 74/200, Train Loss: 11.5359\n",
      "Epoch 74/200, Validation Loss: 12.5171\n",
      "Epoch 75/200, Train Loss: 13.5189\n",
      "Epoch 75/200, Validation Loss: 12.5171\n",
      "Epoch 76/200, Train Loss: 15.6825\n",
      "Epoch 76/200, Validation Loss: 12.5171\n",
      "Epoch 77/200, Train Loss: 11.9432\n",
      "Epoch 77/200, Validation Loss: 12.5171\n",
      "Epoch 78/200, Train Loss: 16.7293\n",
      "Epoch 78/200, Validation Loss: 12.5171\n",
      "Epoch 79/200, Train Loss: 13.6372\n",
      "Epoch 79/200, Validation Loss: 12.5171\n",
      "Epoch 80/200, Train Loss: 14.2927\n",
      "Epoch 80/200, Validation Loss: 12.5171\n",
      "Epoch 81/200, Train Loss: 17.1368\n",
      "Epoch 81/200, Validation Loss: 12.5171\n",
      "Epoch 82/200, Train Loss: 12.9982\n",
      "Epoch 82/200, Validation Loss: 12.5171\n",
      "Epoch 83/200, Train Loss: 13.9757\n",
      "Epoch 83/200, Validation Loss: 12.5171\n",
      "Epoch 84/200, Train Loss: 11.9545\n",
      "Epoch 84/200, Validation Loss: 12.5171\n",
      "Epoch 85/200, Train Loss: 16.2873\n",
      "Epoch 85/200, Validation Loss: 12.5171\n",
      "Epoch 86/200, Train Loss: 12.8540\n",
      "Epoch 86/200, Validation Loss: 12.5171\n",
      "Epoch 87/200, Train Loss: 14.8120\n",
      "Epoch 87/200, Validation Loss: 12.5171\n",
      "Epoch 88/200, Train Loss: 13.9573\n",
      "Epoch 88/200, Validation Loss: 12.5171\n",
      "Epoch 89/200, Train Loss: 14.8341\n",
      "Epoch 89/200, Validation Loss: 12.5171\n",
      "Epoch 90/200, Train Loss: 14.7546\n",
      "Epoch 90/200, Validation Loss: 12.5171\n",
      "Epoch 91/200, Train Loss: 13.5975\n",
      "Epoch 91/200, Validation Loss: 12.5171\n",
      "Epoch 92/200, Train Loss: 14.2676\n",
      "Epoch 92/200, Validation Loss: 12.5171\n",
      "Epoch 93/200, Train Loss: 13.8063\n",
      "Epoch 93/200, Validation Loss: 12.5171\n",
      "Epoch 94/200, Train Loss: 14.3510\n",
      "Epoch 94/200, Validation Loss: 12.5171\n",
      "Epoch 95/200, Train Loss: 13.4485\n",
      "Epoch 95/200, Validation Loss: 12.5171\n",
      "Epoch 96/200, Train Loss: 14.9241\n",
      "Epoch 96/200, Validation Loss: 12.5171\n",
      "Epoch 97/200, Train Loss: 11.6985\n",
      "Epoch 97/200, Validation Loss: 12.5171\n",
      "Epoch 98/200, Train Loss: 15.7859\n",
      "Epoch 98/200, Validation Loss: 12.5171\n",
      "Epoch 99/200, Train Loss: 14.6806\n",
      "Epoch 99/200, Validation Loss: 12.5171\n",
      "Epoch 100/200, Train Loss: 17.1146\n",
      "Epoch 100/200, Validation Loss: 12.5171\n",
      "Epoch 101/200, Train Loss: 17.4120\n",
      "Epoch 101/200, Validation Loss: 12.5171\n",
      "Epoch 102/200, Train Loss: 16.5502\n",
      "Epoch 102/200, Validation Loss: 12.5171\n",
      "Epoch 103/200, Train Loss: 15.1398\n",
      "Epoch 103/200, Validation Loss: 12.5171\n",
      "Epoch 104/200, Train Loss: 12.9052\n",
      "Epoch 104/200, Validation Loss: 12.5171\n",
      "Epoch 105/200, Train Loss: 15.3348\n",
      "Epoch 105/200, Validation Loss: 12.5171\n",
      "Epoch 106/200, Train Loss: 13.8257\n",
      "Epoch 106/200, Validation Loss: 12.5171\n",
      "Epoch 107/200, Train Loss: 12.7995\n",
      "Epoch 107/200, Validation Loss: 12.5171\n",
      "Epoch 108/200, Train Loss: 15.9332\n",
      "Epoch 108/200, Validation Loss: 12.5171\n",
      "Epoch 109/200, Train Loss: 15.4561\n",
      "Epoch 109/200, Validation Loss: 12.5171\n",
      "Epoch 110/200, Train Loss: 15.0770\n",
      "Epoch 110/200, Validation Loss: 12.5171\n",
      "Epoch 111/200, Train Loss: 14.1056\n",
      "Epoch 111/200, Validation Loss: 12.5171\n",
      "Epoch 112/200, Train Loss: 12.6378\n",
      "Epoch 112/200, Validation Loss: 12.5171\n",
      "Epoch 113/200, Train Loss: 13.5328\n",
      "Epoch 113/200, Validation Loss: 12.5171\n",
      "Epoch 114/200, Train Loss: 12.5707\n",
      "Epoch 114/200, Validation Loss: 12.5171\n",
      "Epoch 115/200, Train Loss: 16.8853\n",
      "Epoch 115/200, Validation Loss: 12.5171\n",
      "Epoch 116/200, Train Loss: 15.4194\n",
      "Epoch 116/200, Validation Loss: 12.5171\n",
      "Epoch 117/200, Train Loss: 15.4894\n",
      "Epoch 117/200, Validation Loss: 12.5171\n",
      "Epoch 118/200, Train Loss: 14.4977\n",
      "Epoch 118/200, Validation Loss: 12.5171\n",
      "Epoch 119/200, Train Loss: 13.8994\n",
      "Epoch 119/200, Validation Loss: 12.5171\n",
      "Epoch 120/200, Train Loss: 14.8483\n",
      "Epoch 120/200, Validation Loss: 12.5171\n",
      "Epoch 121/200, Train Loss: 13.7017\n",
      "Epoch 121/200, Validation Loss: 12.5171\n",
      "Epoch 122/200, Train Loss: 16.1574\n",
      "Epoch 122/200, Validation Loss: 12.5171\n",
      "Epoch 123/200, Train Loss: 14.3879\n",
      "Epoch 123/200, Validation Loss: 12.5171\n",
      "Epoch 124/200, Train Loss: 13.0536\n",
      "Epoch 124/200, Validation Loss: 12.5171\n",
      "Epoch 125/200, Train Loss: 16.4418\n",
      "Epoch 125/200, Validation Loss: 12.5171\n",
      "Epoch 126/200, Train Loss: 14.1437\n",
      "Epoch 126/200, Validation Loss: 12.5171\n",
      "Epoch 127/200, Train Loss: 12.9531\n",
      "Epoch 127/200, Validation Loss: 12.5171\n",
      "Epoch 128/200, Train Loss: 15.7197\n",
      "Epoch 128/200, Validation Loss: 12.5171\n",
      "Epoch 129/200, Train Loss: 16.8012\n",
      "Epoch 129/200, Validation Loss: 12.5171\n",
      "Epoch 130/200, Train Loss: 13.0344\n",
      "Epoch 130/200, Validation Loss: 12.5171\n",
      "Epoch 131/200, Train Loss: 14.0683\n",
      "Epoch 131/200, Validation Loss: 12.5171\n",
      "Epoch 132/200, Train Loss: 11.9400\n",
      "Epoch 132/200, Validation Loss: 12.5171\n",
      "Epoch 133/200, Train Loss: 15.8598\n",
      "Epoch 133/200, Validation Loss: 12.5171\n",
      "Epoch 134/200, Train Loss: 16.3094\n",
      "Epoch 134/200, Validation Loss: 12.5171\n",
      "Epoch 135/200, Train Loss: 12.5762\n",
      "Epoch 135/200, Validation Loss: 12.5171\n",
      "Epoch 136/200, Train Loss: 15.5378\n",
      "Epoch 136/200, Validation Loss: 12.5171\n",
      "Epoch 137/200, Train Loss: 13.8067\n",
      "Epoch 137/200, Validation Loss: 12.5171\n",
      "Epoch 138/200, Train Loss: 17.0106\n",
      "Epoch 138/200, Validation Loss: 12.5171\n",
      "Epoch 139/200, Train Loss: 16.1880\n",
      "Epoch 139/200, Validation Loss: 12.5171\n",
      "Epoch 140/200, Train Loss: 14.8200\n",
      "Epoch 140/200, Validation Loss: 12.5171\n",
      "Epoch 141/200, Train Loss: 14.6195\n",
      "Epoch 141/200, Validation Loss: 12.5171\n",
      "Epoch 142/200, Train Loss: 16.4843\n",
      "Epoch 142/200, Validation Loss: 12.5171\n",
      "Epoch 143/200, Train Loss: 15.4144\n",
      "Epoch 143/200, Validation Loss: 12.5171\n",
      "Epoch 144/200, Train Loss: 11.8737\n",
      "Epoch 144/200, Validation Loss: 12.5171\n",
      "Epoch 145/200, Train Loss: 16.4002\n",
      "Epoch 145/200, Validation Loss: 12.5171\n",
      "Epoch 146/200, Train Loss: 13.7641\n",
      "Epoch 146/200, Validation Loss: 12.5171\n",
      "Epoch 147/200, Train Loss: 15.7526\n",
      "Epoch 147/200, Validation Loss: 12.5171\n",
      "Epoch 148/200, Train Loss: 15.0170\n",
      "Epoch 148/200, Validation Loss: 12.5171\n",
      "Epoch 149/200, Train Loss: 13.9710\n",
      "Epoch 149/200, Validation Loss: 12.5171\n",
      "Epoch 150/200, Train Loss: 14.3416\n",
      "Epoch 150/200, Validation Loss: 12.5171\n",
      "Epoch 151/200, Train Loss: 14.4407\n",
      "Epoch 151/200, Validation Loss: 12.5171\n",
      "Epoch 152/200, Train Loss: 15.3204\n",
      "Epoch 152/200, Validation Loss: 12.5171\n",
      "Epoch 153/200, Train Loss: 16.3016\n",
      "Epoch 153/200, Validation Loss: 12.5171\n",
      "Epoch 154/200, Train Loss: 16.0790\n",
      "Epoch 154/200, Validation Loss: 12.5171\n",
      "Epoch 155/200, Train Loss: 16.9894\n",
      "Epoch 155/200, Validation Loss: 12.5171\n",
      "Epoch 156/200, Train Loss: 15.3064\n",
      "Epoch 156/200, Validation Loss: 12.5171\n",
      "Epoch 157/200, Train Loss: 14.6373\n",
      "Epoch 157/200, Validation Loss: 12.5171\n",
      "Epoch 158/200, Train Loss: 14.4376\n",
      "Epoch 158/200, Validation Loss: 12.5171\n",
      "Epoch 159/200, Train Loss: 14.0230\n",
      "Epoch 159/200, Validation Loss: 12.5171\n",
      "Epoch 160/200, Train Loss: 18.0619\n",
      "Epoch 160/200, Validation Loss: 12.5171\n",
      "Epoch 161/200, Train Loss: 16.8640\n",
      "Epoch 161/200, Validation Loss: 12.5171\n",
      "Epoch 162/200, Train Loss: 17.6981\n",
      "Epoch 162/200, Validation Loss: 12.5171\n",
      "Epoch 163/200, Train Loss: 14.4998\n",
      "Epoch 163/200, Validation Loss: 12.5171\n",
      "Epoch 164/200, Train Loss: 14.3275\n",
      "Epoch 164/200, Validation Loss: 12.5171\n",
      "Epoch 165/200, Train Loss: 16.1328\n",
      "Epoch 165/200, Validation Loss: 12.5171\n",
      "Epoch 166/200, Train Loss: 13.7212\n",
      "Epoch 166/200, Validation Loss: 12.5171\n",
      "Epoch 167/200, Train Loss: 14.5014\n",
      "Epoch 167/200, Validation Loss: 12.5171\n",
      "Epoch 168/200, Train Loss: 13.6908\n",
      "Epoch 168/200, Validation Loss: 12.5171\n",
      "Epoch 169/200, Train Loss: 15.0852\n",
      "Epoch 169/200, Validation Loss: 12.5171\n",
      "Epoch 170/200, Train Loss: 15.6522\n",
      "Epoch 170/200, Validation Loss: 12.5171\n",
      "Epoch 171/200, Train Loss: 14.2430\n",
      "Epoch 171/200, Validation Loss: 12.5171\n",
      "Epoch 172/200, Train Loss: 15.7101\n",
      "Epoch 172/200, Validation Loss: 12.5171\n",
      "Epoch 173/200, Train Loss: 13.8120\n",
      "Epoch 173/200, Validation Loss: 12.5171\n",
      "Epoch 174/200, Train Loss: 14.6938\n",
      "Epoch 174/200, Validation Loss: 12.5171\n",
      "Epoch 175/200, Train Loss: 14.0119\n",
      "Epoch 175/200, Validation Loss: 12.5171\n",
      "Epoch 176/200, Train Loss: 13.6945\n",
      "Epoch 176/200, Validation Loss: 12.5171\n",
      "Epoch 177/200, Train Loss: 12.6310\n",
      "Epoch 177/200, Validation Loss: 12.5171\n",
      "Epoch 178/200, Train Loss: 14.8970\n",
      "Epoch 178/200, Validation Loss: 12.5171\n",
      "Epoch 179/200, Train Loss: 14.1864\n",
      "Epoch 179/200, Validation Loss: 12.5171\n",
      "Epoch 180/200, Train Loss: 13.9337\n",
      "Epoch 180/200, Validation Loss: 12.5171\n",
      "Epoch 181/200, Train Loss: 18.5339\n",
      "Epoch 181/200, Validation Loss: 12.5171\n",
      "Epoch 182/200, Train Loss: 12.7807\n",
      "Epoch 182/200, Validation Loss: 12.5171\n",
      "Epoch 183/200, Train Loss: 12.8064\n",
      "Epoch 183/200, Validation Loss: 12.5171\n",
      "Epoch 184/200, Train Loss: 15.8420\n",
      "Epoch 184/200, Validation Loss: 12.5171\n",
      "Epoch 185/200, Train Loss: 15.5006\n",
      "Epoch 185/200, Validation Loss: 12.5171\n",
      "Epoch 186/200, Train Loss: 13.8465\n",
      "Epoch 186/200, Validation Loss: 12.5171\n",
      "Epoch 187/200, Train Loss: 14.2436\n",
      "Epoch 187/200, Validation Loss: 12.5171\n",
      "Epoch 188/200, Train Loss: 15.3467\n",
      "Epoch 188/200, Validation Loss: 12.5171\n",
      "Epoch 189/200, Train Loss: 13.2556\n",
      "Epoch 189/200, Validation Loss: 12.5171\n",
      "Epoch 190/200, Train Loss: 15.7141\n",
      "Epoch 190/200, Validation Loss: 12.5171\n",
      "Epoch 191/200, Train Loss: 14.8217\n",
      "Epoch 191/200, Validation Loss: 12.5171\n",
      "Epoch 192/200, Train Loss: 14.8826\n",
      "Epoch 192/200, Validation Loss: 12.5171\n",
      "Epoch 193/200, Train Loss: 16.4977\n",
      "Epoch 193/200, Validation Loss: 12.5171\n",
      "Epoch 194/200, Train Loss: 15.2963\n",
      "Epoch 194/200, Validation Loss: 12.5171\n",
      "Epoch 195/200, Train Loss: 15.7192\n",
      "Epoch 195/200, Validation Loss: 12.5171\n",
      "Epoch 196/200, Train Loss: 14.2860\n",
      "Epoch 196/200, Validation Loss: 12.5171\n",
      "Epoch 197/200, Train Loss: 15.2787\n",
      "Epoch 197/200, Validation Loss: 12.5171\n",
      "Epoch 198/200, Train Loss: 14.1352\n",
      "Epoch 198/200, Validation Loss: 12.5171\n",
      "Epoch 199/200, Train Loss: 13.6344\n",
      "Epoch 199/200, Validation Loss: 12.5171\n",
      "Epoch 200/200, Train Loss: 15.9534\n",
      "Epoch 200/200, Validation Loss: 12.5171\n"
     ]
    }
   ],
   "source": [
    "#cargamos los datos1\n",
    "#num_samples=8000\n",
    "#x_train,y_train=get_dataset(num_samples)\n",
    "#x_train=np.expand_dims(x_train,-1)\n",
    "\n",
    "#probamos con los datos2\n",
    "num_samples=8000\n",
    "len_seq=30\n",
    "x_train=np.random.randint(0,100,(8000,len_seq))\n",
    "y_train=x_train[:,::-1].copy()\n",
    "print(x_train.shape,y_train.shape)\n",
    "\n",
    "x_train,y_train=torch.from_numpy(x_train).float() ,torch.from_numpy(y_train).float()\n",
    "\n",
    "#instanciamos el modelo\n",
    "d_k=40\n",
    "#model = CustomNetwork(len_seq, d_k,len_seq)\n",
    "\n",
    "#creamos la funcion perdida y el optimizador\n",
    "loss_criterion=nn.MSELoss()\n",
    "optimizer=optim.Adam(model.parameters(),lr=0.0001)\n",
    "scheduler = ExponentialLR(optimizer, 0.95)\n",
    "\n",
    "\n",
    "#creamos los datasets y los dataloaders\n",
    "train_size = int(0.8 * num_samples)\n",
    "val_size = num_samples - train_size\n",
    "train_dataset, val_dataset = random_split(TensorDataset(x_train,y_train), [train_size, val_size])\n",
    "\n",
    "# Crear DataLoaders para entrenamiento y validación\n",
    "batch_size=32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "#hacemos un bucle para entrenamiento\n",
    "epocas=200\n",
    "for epoca in range(epocas):\n",
    "    #establecemos el modelo en modo entrenamiento\n",
    "    model.train()\n",
    "\n",
    "    #establecemos en 0 el contador de la funcion de perdida\n",
    "    train_loss=0\n",
    "\n",
    "    #bucle para recorrer todos los batches\n",
    "    for batch_x_train,batch_y_train in train_dataloader:\n",
    "        #ponemos a zero los gradientes para el optimizador\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #predecimos\n",
    "\n",
    "        output = model(batch_x_train)\n",
    "\n",
    "        #calculamos la perdida\n",
    "        loss = loss_criterion(output,batch_y_train)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        #retropropagamos/calculamso lso gradientes\n",
    "        loss.backward()\n",
    "        #y modificamos los pesos\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "    print(f\"Epoch {epoca + 1}/{epocas}, Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "\n",
    "    # modo validacion, \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x_val,batch_y_val in val_dataloader:\n",
    "            outputs = model(batch_x_val)\n",
    "            loss = loss_criterion(outputs, batch_y_val)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_dataloader)\n",
    "    print(f\"Epoch {epoca + 1}/{epocas}, Validation Loss: {val_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_attention_heatmap(attention_matrix, labels):\n",
    "    fig, ax = plt.subplots(figsize=(16,16))\n",
    "    cax = ax.matshow(attention_matrix, cmap=plt.cm.Blues)\n",
    "\n",
    "    # Configurar etiquetas de los ejes\n",
    "    ax.set_xticklabels(labels, rotation=90)\n",
    "    ax.set_yticklabels(labels)\n",
    "\n",
    "    # Alinear las etiquetas de los ejes a las marcas de graduación\n",
    "    ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "\n",
    "    # Agregar una barra de colores\n",
    "    cbar = fig.colorbar(cax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 30]) torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "x_test=np.random.randint(0,100,(1,30))\n",
    "y_test=x_test[:,::-1].copy()\n",
    "\n",
    "x_test,y_test=torch.from_numpy(x_test).float() ,torch.from_numpy(y_test).float()\n",
    "\n",
    "output,attention=model(x_test,return_attention=True)\n",
    "print(output.shape,attention.detach().squeeze().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid shape () for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[114], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plot_attention_heatmap(attention\u001b[39m.\u001b[39;49mdetach()\u001b[39m.\u001b[39;49msqueeze()\u001b[39m.\u001b[39;49mnumpy(),x_test\u001b[39m.\u001b[39;49mdetach()\u001b[39m.\u001b[39;49msqueeze()\u001b[39m.\u001b[39;49mnumpy())\n",
      "Cell \u001b[0;32mIn[112], line 3\u001b[0m, in \u001b[0;36mplot_attention_heatmap\u001b[0;34m(attention_matrix, labels)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot_attention_heatmap\u001b[39m(attention_matrix, labels):\n\u001b[1;32m      2\u001b[0m     fig, ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(figsize\u001b[39m=\u001b[39m(\u001b[39m16\u001b[39m,\u001b[39m16\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m     cax \u001b[39m=\u001b[39m ax\u001b[39m.\u001b[39;49mmatshow(attention_matrix, cmap\u001b[39m=\u001b[39;49mplt\u001b[39m.\u001b[39;49mcm\u001b[39m.\u001b[39;49mBlues)\n\u001b[1;32m      5\u001b[0m     \u001b[39m# Configurar etiquetas de los ejes\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     ax\u001b[39m.\u001b[39mset_xticklabels(labels, rotation\u001b[39m=\u001b[39m\u001b[39m90\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.10/site-packages/matplotlib/axes/_axes.py:7928\u001b[0m, in \u001b[0;36mAxes.matshow\u001b[0;34m(self, Z, **kwargs)\u001b[0m\n\u001b[1;32m   7923\u001b[0m Z \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masanyarray(Z)\n\u001b[1;32m   7924\u001b[0m kw \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39morigin\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mupper\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   7925\u001b[0m       \u001b[39m'\u001b[39m\u001b[39minterpolation\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mnearest\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   7926\u001b[0m       \u001b[39m'\u001b[39m\u001b[39maspect\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mequal\u001b[39m\u001b[39m'\u001b[39m,          \u001b[39m# (already the imshow default)\u001b[39;00m\n\u001b[1;32m   7927\u001b[0m       \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs}\n\u001b[0;32m-> 7928\u001b[0m im \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimshow(Z, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m   7929\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtitle\u001b[39m.\u001b[39mset_y(\u001b[39m1.05\u001b[39m)\n\u001b[1;32m   7930\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mxaxis\u001b[39m.\u001b[39mtick_top()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.10/site-packages/matplotlib/_api/deprecation.py:454\u001b[0m, in \u001b[0;36mmake_keyword_only.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m name_idx:\n\u001b[1;32m    449\u001b[0m     warn_deprecated(\n\u001b[1;32m    450\u001b[0m         since, message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPassing the \u001b[39m\u001b[39m%(name)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%(obj_type)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    451\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpositionally is deprecated since Matplotlib \u001b[39m\u001b[39m%(since)s\u001b[39;00m\u001b[39m; the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mparameter will become keyword-only \u001b[39m\u001b[39m%(removal)s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    453\u001b[0m         name\u001b[39m=\u001b[39mname, obj_type\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 454\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.10/site-packages/matplotlib/__init__.py:1423\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1420\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m   1421\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(ax, \u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1422\u001b[0m     \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1423\u001b[0m         \u001b[39mreturn\u001b[39;00m func(ax, \u001b[39m*\u001b[39;49m\u001b[39mmap\u001b[39;49m(sanitize_sequence, args), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1425\u001b[0m     bound \u001b[39m=\u001b[39m new_sig\u001b[39m.\u001b[39mbind(ax, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1426\u001b[0m     auto_label \u001b[39m=\u001b[39m (bound\u001b[39m.\u001b[39marguments\u001b[39m.\u001b[39mget(label_namer)\n\u001b[1;32m   1427\u001b[0m                   \u001b[39mor\u001b[39;00m bound\u001b[39m.\u001b[39mkwargs\u001b[39m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.10/site-packages/matplotlib/axes/_axes.py:5604\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5596\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_aspect(aspect)\n\u001b[1;32m   5597\u001b[0m im \u001b[39m=\u001b[39m mimage\u001b[39m.\u001b[39mAxesImage(\u001b[39mself\u001b[39m, cmap\u001b[39m=\u001b[39mcmap, norm\u001b[39m=\u001b[39mnorm,\n\u001b[1;32m   5598\u001b[0m                       interpolation\u001b[39m=\u001b[39minterpolation, origin\u001b[39m=\u001b[39morigin,\n\u001b[1;32m   5599\u001b[0m                       extent\u001b[39m=\u001b[39mextent, filternorm\u001b[39m=\u001b[39mfilternorm,\n\u001b[1;32m   5600\u001b[0m                       filterrad\u001b[39m=\u001b[39mfilterrad, resample\u001b[39m=\u001b[39mresample,\n\u001b[1;32m   5601\u001b[0m                       interpolation_stage\u001b[39m=\u001b[39minterpolation_stage,\n\u001b[1;32m   5602\u001b[0m                       \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 5604\u001b[0m im\u001b[39m.\u001b[39;49mset_data(X)\n\u001b[1;32m   5605\u001b[0m im\u001b[39m.\u001b[39mset_alpha(alpha)\n\u001b[1;32m   5606\u001b[0m \u001b[39mif\u001b[39;00m im\u001b[39m.\u001b[39mget_clip_path() \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   5607\u001b[0m     \u001b[39m# image does not already have clipping set, clip to axes patch\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.10/site-packages/matplotlib/image.py:710\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    706\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A[:, :, \u001b[39m0\u001b[39m]\n\u001b[1;32m    708\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m    709\u001b[0m         \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39min\u001b[39;00m [\u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m]):\n\u001b[0;32m--> 710\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid shape \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m for image data\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    711\u001b[0m                     \u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mshape))\n\u001b[1;32m    713\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[1;32m    714\u001b[0m     \u001b[39m# If the input data has values outside the valid range (after\u001b[39;00m\n\u001b[1;32m    715\u001b[0m     \u001b[39m# normalisation), we issue a warning and then clip X to the bounds\u001b[39;00m\n\u001b[1;32m    716\u001b[0m     \u001b[39m# - otherwise casting wraps extreme values, hiding outliers and\u001b[39;00m\n\u001b[1;32m    717\u001b[0m     \u001b[39m# making reliable interpretation impossible.\u001b[39;00m\n\u001b[1;32m    718\u001b[0m     high \u001b[39m=\u001b[39m \u001b[39m255\u001b[39m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39missubdtype(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype, np\u001b[39m.\u001b[39minteger) \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid shape () for image data"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x1600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_attention_heatmap(attention.detach().squeeze().numpy(),x_test.detach().squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
