{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iHKOrwwy3UT"
      },
      "source": [
        "# Modelo con SELF-attention\n",
        "\n",
        "Vamos a contruir un mecanismo de atención, y vamos a darle un dataser creado a posta para ver como le presta atención a cada zona."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "LqBrO0aoy3UY"
      },
      "outputs": [],
      "source": [
        "import numpy  as np \n",
        "import matplotlib.pyplot as plt \n",
        "import torch\n",
        "from torch import nn \n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset,Dataset, DataLoader, random_split, IterableDataset\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "import gc\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQ5bpCuAy3Ua"
      },
      "source": [
        "### Construción mecanismo de atención\n",
        "\n",
        "Vamos a construir un bloque de atencion con pytorch, que toma in input y procesa la self-atention y luego eso entra a una red densa y ya."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AM-6hM0Fy3Ua"
      },
      "outputs": [],
      "source": [
        "#primero el scaled dot product\n",
        "def scaled_dot_prod(q,k,v):\n",
        "    #multiplicamos q por k y luego de escalarlo y la softmax por v\n",
        "    d_k=q.size()[-1]\n",
        "    qk_mult=torch.matmul(q,k.transpose(-2,-1))\n",
        "    qk_mult_scalet=qk_mult/math.sqrt(d_k)\n",
        "    attention = F.softmax(qk_mult_scalet, dim=-1)\n",
        "    attention=torch.eye(*attention.size(),out=attention).flip(-1)\n",
        "    values = torch.matmul(attention, v)\n",
        "    return values, attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12tMcR0Uy3Ub",
        "outputId": "f080de79-ca54-44a2-a93c-e1dd1e8a8bac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q\n",
            " tensor([[-0.0476],\n",
            "        [ 0.2206],\n",
            "        [ 0.3571],\n",
            "        [ 0.5960],\n",
            "        [ 1.7237],\n",
            "        [-0.6713],\n",
            "        [-0.6513]])\n",
            "K\n",
            " tensor([[-1.3747],\n",
            "        [ 1.3016],\n",
            "        [-0.0784],\n",
            "        [ 1.0663],\n",
            "        [-0.4982],\n",
            "        [-0.8816],\n",
            "        [-0.3101]])\n",
            "V\n",
            " tensor([[ 0.4233],\n",
            "        [-0.5659],\n",
            "        [-0.0817],\n",
            "        [ 1.2227],\n",
            "        [ 0.8173],\n",
            "        [ 1.6023],\n",
            "        [ 2.0337]])\n",
            "Values\n",
            " tensor([[0.7911],\n",
            "        [0.7137],\n",
            "        [0.6678],\n",
            "        [0.5817],\n",
            "        [0.2349],\n",
            "        [0.8844],\n",
            "        [0.8833]])\n",
            "Attention\n",
            " tensor([[0.1516, 0.1335, 0.1425, 0.1350, 0.1454, 0.1481, 0.1441],\n",
            "        [0.1059, 0.1911, 0.1410, 0.1815, 0.1285, 0.1181, 0.1339],\n",
            "        [0.0862, 0.2241, 0.1369, 0.2061, 0.1179, 0.1028, 0.1261],\n",
            "        [0.0578, 0.2850, 0.1252, 0.2477, 0.0975, 0.0776, 0.1091],\n",
            "        [0.0052, 0.5265, 0.0488, 0.3509, 0.0237, 0.0122, 0.0327],\n",
            "        [0.2824, 0.0468, 0.1183, 0.0548, 0.1568, 0.2028, 0.1382],\n",
            "        [0.2779, 0.0486, 0.1194, 0.0567, 0.1570, 0.2015, 0.1389]])\n"
          ]
        }
      ],
      "source": [
        "seq_len, d_k = 7, 1\n",
        "q = torch.randn(seq_len, d_k)\n",
        "k = torch.randn(seq_len, d_k)\n",
        "v = torch.randn(seq_len, d_k)\n",
        "values, attention = scaled_dot_prod(q, k, v)\n",
        "print(\"Q\\n\", q)\n",
        "print(\"K\\n\", k)\n",
        "print(\"V\\n\", v)\n",
        "print(\"Values\\n\", values)\n",
        "print(\"Attention\\n\", attention)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TeSQ41iy3Uc",
        "outputId": "9c65db99-f4f0-4c0a-abf0-e55a7720647e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 1.],\n",
            "        [ 4.],\n",
            "        [ 2.],\n",
            "        [ 1.],\n",
            "        [ 5.],\n",
            "        [13.],\n",
            "        [ 3.]])\n",
            "Q\n",
            " tensor([[ 0.1982,  1.2992],\n",
            "        [ 0.7930,  5.1967],\n",
            "        [ 0.3965,  2.5984],\n",
            "        [ 0.1982,  1.2992],\n",
            "        [ 0.9912,  6.4959],\n",
            "        [ 2.5772, 16.8893],\n",
            "        [ 0.5947,  3.8975]])\n",
            "K\n",
            " tensor([[ 1.4544,  0.7871],\n",
            "        [ 5.8177,  3.1486],\n",
            "        [ 2.9088,  1.5743],\n",
            "        [ 1.4544,  0.7871],\n",
            "        [ 7.2721,  3.9357],\n",
            "        [18.9074, 10.2328],\n",
            "        [ 4.3632,  2.3614]])\n",
            "V\n",
            " tensor([[ 0.8920,  0.9615],\n",
            "        [ 3.5680,  3.8459],\n",
            "        [ 1.7840,  1.9230],\n",
            "        [ 0.8920,  0.9615],\n",
            "        [ 4.4600,  4.8074],\n",
            "        [11.5960, 12.4992],\n",
            "        [ 2.6760,  2.8844]])\n"
          ]
        }
      ],
      "source": [
        "#los vectores Q, K y V son creados por combinacion lineal de un imput, por ejemplo\n",
        "#tenemos las matrics M_Q, M_K y M_V, que tendran dimension (d_input x d_k) y calculamos los vectores resultantes asi input x M_Q = Q\n",
        "d_input=1 # esta es la dimension de los tokens (puede ser una palabra enmbebida en un espacion de 512 dims, o puede ser una variable o un vector de 3 variables etc.), no la cantidad de tokens\n",
        "d_k=2 # espacio al que llevamos ese vector o valor del token inicial\n",
        "\n",
        "vector_input=torch.tensor([1,4,2,1,5,13,3],dtype=torch.float32).unsqueeze(1) #este es el vector de tokens embebidos o variables. La primera dimension recorre las posiciones se la sentencia input\n",
        "print(vector_input)\n",
        "M_Q=torch.randn(d_input,d_k)\n",
        "M_K=torch.randn(d_input,d_k)\n",
        "M_V=torch.randn(d_input,d_k)\n",
        "\n",
        "Q=torch.matmul(vector_input,M_Q)\n",
        "K=torch.matmul(vector_input,M_K)\n",
        "V=torch.matmul(vector_input,M_V)\n",
        "\n",
        "print(\"Q\\n\", Q)\n",
        "print(\"K\\n\", K)\n",
        "print(\"V\\n\", V)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tj_T0-SWy3Uc"
      },
      "outputs": [],
      "source": [
        "class CustomNetwork(nn.Module):\n",
        "    #esta implementación es como una MUltiHeadAttention usando self-attentino de All you need is Attention, pero una sola HEAD\n",
        "    def __init__(self, d_input, d_k, output_dim=1):\n",
        "        super(CustomNetwork, self).__init__()\n",
        "        \n",
        "        self.d_input = d_input\n",
        "        self.d_k = d_k\n",
        "        self.M_Q = nn.Parameter(torch.randn(d_input, d_k))\n",
        "        self.M_K = nn.Parameter(torch.randn(d_input, d_k))\n",
        "        self.M_V = torch.ones(d_input, d_k) # nn.Parameter(torch.randn(d_input, d_k)) # \n",
        "        self.fc = nn.Linear(d_k, output_dim)\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        # Original Transformer initialization, see PyTorch documentation\n",
        "        nn.init.xavier_uniform_(self.M_Q)\n",
        "        nn.init.xavier_uniform_(self.M_K)        \n",
        "        #nn.init.xavier_uniform_(self.M_V)\n",
        "        self.fc.bias.data.fill_(0)\n",
        "        nn.init.xavier_uniform_(self.fc.weight)\n",
        "    \n",
        "    def scaled_dot_prod(self, q, k, v):\n",
        "        d_k = q.size(-1)\n",
        "        attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
        "        attn_logits /= math.sqrt(d_k)\n",
        "        attention = F.softmax(attn_logits, dim=-1)\n",
        "        #batch_dim,dim1,dim2=attention.size()\n",
        "        #attention_aux=torch.eye(dim1,dim2).flip(-1)\n",
        "        #attention = attention_aux.unsqueeze(0).expand(batch_dim, -1, -1)\n",
        "        values = torch.matmul(attention, v)\n",
        "        return values, attention\n",
        "\n",
        "    def forward(self, vector_input,return_attention=False):\n",
        "        Q = torch.matmul(vector_input, self.M_Q)\n",
        "        K = torch.matmul(vector_input, self.M_K)\n",
        "        V = torch.matmul(vector_input, self.M_V)\n",
        "        \n",
        "        values, attention = self.scaled_dot_prod(Q, K, V)\n",
        "        output = self.fc(values)\n",
        "        if return_attention:\n",
        "            return output, attention\n",
        "        else:\n",
        "            return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1R46grJQy3Ud"
      },
      "outputs": [],
      "source": [
        "class CustomNetwork2(nn.Module):\n",
        "    #ahora intentemos implementarlo con la propia capa de Pytorch\n",
        "    def __init__(self, d_input, d_k, output_dim=1,num_heads=1):\n",
        "        super(CustomNetwork2, self).__init__()\n",
        "        \n",
        "        self.d_input = d_input\n",
        "        self.d_k = d_k\n",
        "        self.M_Q = nn.Parameter(torch.randn(d_input, d_k))\n",
        "        self.M_K = nn.Parameter(torch.randn(d_input, d_k))\n",
        "        self.M_V = nn.Parameter(torch.randn(d_input, d_k)) \n",
        "        self.attention = nn.MultiheadAttention(d_k, num_heads=num_heads,batch_first=True)\n",
        "\n",
        "        self.fc = nn.Linear(d_k, output_dim)\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        # Original Transformer initialization, see PyTorch documentation\n",
        "        nn.init.xavier_uniform_(self.M_Q)\n",
        "        nn.init.xavier_uniform_(self.M_K)        \n",
        "        nn.init.xavier_uniform_(self.M_V)\n",
        "        self.fc.bias.data.fill_(0)\n",
        "        nn.init.xavier_uniform_(self.fc.weight)\n",
        "    \n",
        "\n",
        "\n",
        "    def forward(self, vector_input,return_attention=False,average_attn_weights=False):\n",
        "        Q = torch.matmul(vector_input, self.M_Q)\n",
        "        K = torch.matmul(vector_input, self.M_K)\n",
        "        V = torch.matmul(vector_input, self.M_V)\n",
        "\n",
        "        values, attention = self.attention(Q, K, V,average_attn_weights=average_attn_weights)\n",
        "        #values = values.squeeze(0)\n",
        "\n",
        "        output = self.fc(values)\n",
        "        if return_attention:\n",
        "            return output, attention\n",
        "        else:\n",
        "            return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xgc-aqv9y3Ue"
      },
      "outputs": [],
      "source": [
        "class CustomNetwork3(nn.Module):\n",
        "    #ahora intentemos implementarlo con la propia capa de Pytorch\n",
        "    def __init__(self, d_input, d_k, output_dim=1,num_heads=1):\n",
        "        super(CustomNetwork3, self).__init__()\n",
        "        \n",
        "        self.d_input = d_input\n",
        "        self.d_k = d_k\n",
        "        self.dense=nn.Linear(d_input,d_k)\n",
        "        self.attention = nn.MultiheadAttention(d_k, num_heads=num_heads,batch_first=True)\n",
        "\n",
        "        self.fc = nn.Linear(d_k, output_dim)\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        # Original Transformer initialization, see PyTorch documentation\n",
        "        nn.init.xavier_uniform_(self.dense.weight)\n",
        "               \n",
        "        #self.fc.bias.data.fill_(0)\n",
        "        nn.init.xavier_uniform_(self.fc.weight)\n",
        "    \n",
        "\n",
        "\n",
        "    def forward(self, vector_input,return_attention=False,average_attn_weights=False):\n",
        "\n",
        "        x=self.dense(vector_input)\n",
        "\n",
        "        values, attention = self.attention(x,x,x,average_attn_weights=average_attn_weights)\n",
        "\n",
        "        output = self.fc(values)\n",
        "        if return_attention:\n",
        "            return output, attention\n",
        "        else:\n",
        "            return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1XyDiaPy3Uf"
      },
      "outputs": [],
      "source": [
        "class CustomNetwork4(nn.Module):\n",
        "    #ahora intentemos implementarlo con la propia capa de Pytorch\n",
        "    #ademas añadimos conexiones residuales y normalizacion\n",
        "    def __init__(self, d_input, d_k, output_dim=1,num_heads=1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.d_input = d_input\n",
        "        self.d_k = d_k\n",
        "        self.dense=nn.Linear(d_input,d_k)\n",
        "        self.attention = nn.MultiheadAttention(d_k, num_heads=num_heads,batch_first=True)\n",
        "        self.norm1 = nn.LayerNorm(d_k)\n",
        "        self.norm2 = nn.LayerNorm(d_k)\n",
        "\n",
        "        self.fc = nn.Linear(d_k, d_k)\n",
        "        self.last_dense = nn.Linear(d_k, output_dim)\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        # Original Transformer initialization, see PyTorch documentation\n",
        "        nn.init.xavier_uniform_(self.dense.weight)\n",
        "               \n",
        "        #self.fc.bias.data.fill_(0)\n",
        "        nn.init.xavier_uniform_(self.fc.weight)\n",
        "    \n",
        "\n",
        "\n",
        "    def forward(self, vector_input,return_attention=False,average_attn_weights=False):\n",
        "\n",
        "        x=self.dense(vector_input)\n",
        "\n",
        "        values, attention = self.attention(x,x,x,average_attn_weights=average_attn_weights)\n",
        "        values=self.norm1(x+values)\n",
        "        output = self.last_dense(values)\n",
        "        #output = self.fc(values)\n",
        "        #output=self.norm2(x+output)\n",
        "        #final_output=self.last_dense(output)\n",
        "        if return_attention:\n",
        "            return output, attention\n",
        "        else:\n",
        "            return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xn2s9XLy3Uf"
      },
      "outputs": [],
      "source": [
        "\n",
        "class PostionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    compute sinusoid encoding.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, max_len, device):\n",
        "        \n",
        "        \"\"\"\n",
        "        constructor of sinusoid encoding class\n",
        "\n",
        "        :param d_model: dimension of model\n",
        "        :param max_len: max sequence length\n",
        "        :param device: hardware device setting\n",
        "        \"\"\"\n",
        "\n",
        "        super(PostionalEncoding, self).__init__()\n",
        "\n",
        "        # same size with input matrix (for adding with isnput matrix)\n",
        "        self.encoding = torch.zeros(max_len, d_model, device=device)\n",
        "        self.encoding.requires_grad = False  # we don't need to compute gradient\n",
        "\n",
        "        pos = torch.arange(0, max_len, device=device)\n",
        "        pos = pos.float().unsqueeze(dim=1)\n",
        "        # 1D => 2D unsqueeze to represent word's position\n",
        "\n",
        "        _2i = torch.arange(0, d_model, step=2, device=device).float()\n",
        "        # 'i' means index of d_model (e.g. embedding size = 50, 'i' = [0,50])\n",
        "        # \"step=2\" means 'i' multiplied with two (same with 2 * i)\n",
        "\n",
        "        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n",
        "        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n",
        "        # compute positional encoding to consider positional information of words\n",
        "\n",
        "    def forward(self, x):\n",
        "        # self.encoding\n",
        "        # [max_len = 512, d_model = 512]\n",
        "\n",
        "        batch_size, seq_len = x.size()\n",
        "        # [batch_size = 128, seq_len = 30]\n",
        "\n",
        "        return self.encoding[:seq_len, :]\n",
        "        # [seq_len = 30, d_model = 512]\n",
        "        # it will add with tok_emb : [128, 30, 512]\n",
        "\n",
        "class CustomNetwork4_withPosEncoding(nn.Module):\n",
        "    #ahora intentemos implementarlo con la propia capa de Pytorch\n",
        "    #ademas añadimos conexiones residuales y normalizacion\n",
        "    def __init__(self, d_input, d_k, output_dim=1,num_heads=1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.d_input = d_input\n",
        "        self.d_k = d_k\n",
        "        self.dense=nn.Linear(d_input,d_k)\n",
        "        self.attention = nn.MultiheadAttention(d_k, num_heads=num_heads,batch_first=True)\n",
        "        self.norm1 = nn.LayerNorm(d_k)\n",
        "        self.norm2 = nn.LayerNorm(d_k)\n",
        "        self.pos_encode=PostionalEncoding(d_input,20,None)\n",
        "\n",
        "        self.fc = nn.Linear(d_k, d_k)\n",
        "        self.last_dense = nn.Linear(d_k, output_dim)\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        # Original Transformer initialization, see PyTorch documentation\n",
        "        nn.init.xavier_uniform_(self.dense.weight)\n",
        "               \n",
        "        #self.fc.bias.data.fill_(0)\n",
        "        nn.init.xavier_uniform_(self.fc.weight)\n",
        "    \n",
        "\n",
        "\n",
        "    def forward(self, vector_input,return_attention=False,average_attn_weights=False):\n",
        "        x=self.pos_encode(vector_input)\n",
        "        #x=self.dense(x)\n",
        "\n",
        "        values, attention = self.attention(x,x,x,average_attn_weights=average_attn_weights)\n",
        "        values=self.norm1(x+values)\n",
        "        output = self.last_dense(values)\n",
        "        #output = self.fc(values)\n",
        "        #output=self.norm2(x+output)\n",
        "        #final_output=self.last_dense(output)\n",
        "        if return_attention:\n",
        "            return output, attention\n",
        "        else:\n",
        "            return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPkZB648y3Ug"
      },
      "outputs": [],
      "source": [
        "class CustomNetwork5(nn.Module):\n",
        "    #ahora intentemos implementarlo con la propia capa de Pytorch\n",
        "    #ademas añadimos conexiones residuales y normalizacion\n",
        "    def __init__(self, d_input, d_k, output_dim=1,num_heads=1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.d_input = d_input\n",
        "        self.d_k = d_k\n",
        "        self.dense=nn.Linear(d_input,d_k)\n",
        "        self.attention = nn.MultiheadAttention(d_k, num_heads=num_heads,batch_first=True)\n",
        "        self.norm1 = nn.LayerNorm(d_k)\n",
        "        self.norm2 = nn.LayerNorm(d_k)\n",
        "\n",
        "        self.fc = nn.Linear(d_k, d_k)\n",
        "        self.last_dense = nn.Linear(d_k, output_dim)\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        # Original Transformer initialization, see PyTorch documentation\n",
        "        nn.init.xavier_uniform_(self.dense.weight)\n",
        "               \n",
        "        #self.fc.bias.data.fill_(0)\n",
        "        nn.init.xavier_uniform_(self.fc.weight)\n",
        "    \n",
        "\n",
        "\n",
        "    def forward(self, vector_input,return_attention=False,average_attn_weights=False):\n",
        "\n",
        "        x=self.dense(vector_input)\n",
        "\n",
        "        values, attention = self.attention(x,x,x,average_attn_weights=average_attn_weights)\n",
        "        values=self.norm1(x+values)\n",
        "        output = self.fc(values)\n",
        "        output=self.norm2(x+output)\n",
        "        final_output=self.last_dense(output)\n",
        "        if return_attention:\n",
        "            return final_output, attention\n",
        "        else:\n",
        "            return final_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bqUpkiXy3Ug",
        "outputId": "bf9051ec-65a2-48c5-b0fe-71cd51dc2ead"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output batch shape: torch.Size([5, 30, 1])\n",
            "Attention batch shape: torch.Size([5, 30, 30])\n"
          ]
        }
      ],
      "source": [
        "# Parámetros\n",
        "d_input = 1\n",
        "len_seq=30\n",
        "d_k = 2\n",
        "batch_size=5\n",
        "\n",
        "# Crear datos de entrada en batch\n",
        "vector_input = torch.randn(batch_size,len_seq,1)\n",
        "\n",
        "# Crear una instancia del modelo e inicializarla\n",
        "model = CustomNetwork(d_input, d_k)\n",
        "\n",
        "# Pasar los datos en batch a través de la red\n",
        "output, attention = model(vector_input,return_attention=True)\n",
        "\n",
        "# Verificar las dimensiones de la salida y la atención\n",
        "print(\"Output batch shape:\", output.shape)\n",
        "print(\"Attention batch shape:\", attention.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQS2E281y3Uh",
        "outputId": "a07961bd-00f6-4b06-c7ad-b1d0d7449b52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "M_K: Parameter containing:\n",
            "tensor([[-0.1664,  1.1715]], requires_grad=True)\n",
            "M_Q: Parameter containing:\n",
            "tensor([[-1.1030,  0.0070]], requires_grad=True)\n",
            "M_V: Parameter containing:\n",
            "tensor([[ 1.3521, -0.8854]], requires_grad=True)\n",
            "T_destination: ~T_destination\n",
            "_backward_hooks: OrderedDict()\n",
            "_buffers: OrderedDict()\n",
            "_forward_hooks: OrderedDict()\n",
            "_forward_pre_hooks: OrderedDict()\n",
            "_is_full_backward_hook: None\n",
            "_load_state_dict_post_hooks: OrderedDict()\n",
            "_load_state_dict_pre_hooks: OrderedDict()\n",
            "_modules: OrderedDict([('fc', Linear(in_features=2, out_features=3, bias=True))])\n",
            "_non_persistent_buffers_set: set()\n",
            "_parameters: OrderedDict([('M_Q', Parameter containing:\n",
            "tensor([[-1.1030,  0.0070]], requires_grad=True)), ('M_K', Parameter containing:\n",
            "tensor([[-0.1664,  1.1715]], requires_grad=True)), ('M_V', Parameter containing:\n",
            "tensor([[ 1.3521, -0.8854]], requires_grad=True))])\n",
            "_state_dict_hooks: OrderedDict()\n",
            "_version: 1\n",
            "d_input: 1\n",
            "d_k: 2\n",
            "dump_patches: False\n",
            "training: False\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "import inspect\n",
        "\n",
        "print(inspect.getmembers(model))  # Obtén todos los miembros del objeto param\n",
        "print(inspect.signature(model))  # Muestra la firma de la función torch.matmul\n",
        "print(inspect.getdoc(model))  # Muestra la documentación del objeto param\n",
        "\"\"\"\n",
        "#ejemplo para ver todos los atributos\n",
        "for attr in dir(model):\n",
        "    if not callable(getattr(model, attr)) and not attr.startswith(\"__\"):\n",
        "        print(f\"{attr}: {getattr(model, attr)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4v1b4UcGy3Ui"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "En esta sección construimos un dataset artificial que nos va a permitir ver si el modelo le está prestando atención a la zona que tiene la respuesta que busca.\n",
        "\n",
        "Esto lo vamos a hacer haciendo un data ser con correspondencias X->y, pero X tiene mucha \"basura\", tiene muchas zonas que no son las importantes.\n",
        "\n",
        "Para darle algo que pueda entender tenemos que saber **qué entiende**. El mecanismo que estamos implementado es de self-attention, el más sencillo podriamos decir. Entonces la \"pista\" que le indique al modelo DONDE mirar y porque, debe estar en la propia sentencia. Por ejemplo, tenemos una vector de vectores/tokens muy largo, queremos que cuando uno del los tokens sea =1 todo, sean las 3 palabras/token/vectores anteriores las que nos den los vectores que si los multiplicamos, nos da el label Y buscado.\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EC6ay5HBy3Uj"
      },
      "source": [
        "Vamos a definir un generador de datos, para asi crear nuestro dataset.  \n",
        "Este puede ser un generador infinito que devuelve X e Y.  \n",
        "\n",
        "La clave esta en crear una estructura en la secuencia, por ejemplo:\n",
        "* Cuando un elemento de la secuencia vale 4 o 9, los elementos anteriores, el primero y el tercero anteriores, multiplicados y + 0.3, son igual al label Y buscado.\n",
        "\n",
        "Para hacerlo hacemos: \n",
        "* Vamos a generar vector entre 0 y 100, y vamos a evitar que ninguno valga 4 o 9.\n",
        "* Metemos en una posicion aleatoria un 4 o un 9 y 1 y 3 posiciones antes, metemos los dos valores que multiplicados y +0.3 son igual a y.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moqr4Gowy3Uj"
      },
      "outputs": [],
      "source": [
        "def generar_lista(d_list=100,d_randint=100,not_in=[4,9],bias=0.3):\n",
        "    lista = []\n",
        "    i = 0\n",
        "    while len(lista) < d_list:\n",
        "        i = np.random.randint(0, d_randint)\n",
        "        if i not in not_in:\n",
        "            lista.append(i)\n",
        "        i += 1\n",
        "\n",
        "    #ahora tenemos la lista, vamos a tomar dos valores aleatorios y calcular y \n",
        "    seguir=True\n",
        "    while seguir:\n",
        "        i = np.random.randint(0, d_randint)\n",
        "        if i not in not_in:\n",
        "            x1=i\n",
        "            seguir=False\n",
        "        i += 1\n",
        "    seguir=True\n",
        "    while seguir:\n",
        "        i = np.random.randint(0, d_randint)\n",
        "        if i not in not_in:\n",
        "            x2=i\n",
        "            seguir=False\n",
        "        i += 1\n",
        "    y=x1*x2+bias\n",
        "\n",
        "    #y ahora los colocamos \n",
        "    indx_rand=np.random.randint(4, d_list)\n",
        "    lista[indx_rand]=np.random.choice(not_in)\n",
        "    lista[indx_rand-1]=x1\n",
        "    lista[indx_rand-3]=x2\n",
        "    \n",
        "    return np.array(lista), y\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcfAAuYHy3Uj"
      },
      "outputs": [],
      "source": [
        "def get_dataset(size,d_list=100,d_randint=100,not_in=[4,9],bias=0.3):\n",
        "    X=[]\n",
        "    y=[]\n",
        "    for _ in range(size):\n",
        "        x_element, y_element =generar_lista(d_list=d_list,d_randint=d_randint,not_in=not_in,bias=bias)\n",
        "        X.append(x_element)\n",
        "        y.append(y_element)\n",
        "\n",
        "    return np.array(X),np.array(y)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BkFiIfTy3Uk"
      },
      "outputs": [],
      "source": [
        "lista, y =generar_lista()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSAlAtNly3Uk",
        "outputId": "25abbca5-8639-4d56-b0b2-9c8c4b15fe9d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a=[]\n",
        "for i in range(56):\n",
        "    indx=np.argwhere((lista==4)|(lista==9)).squeeze()\n",
        "    a.append(lista[indx-1]*lista[indx-3]+0.3==y)\n",
        "\n",
        "\n",
        "np.all(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IytHV_PBy3Uk",
        "outputId": "48142306-f129-44e1-d69f-1298b52f42d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10, 100) (10,)\n"
          ]
        }
      ],
      "source": [
        "X,y=get_dataset(10)\n",
        "print(X.shape,y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zP6TzQXy3Ul"
      },
      "source": [
        "## Entrenamiento de la red\n",
        "Ya tenemos la red y ahora vamos a entrenarla para pode ver su capa de atencion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRdXm_U0y3Ul"
      },
      "source": [
        "### Entrenar CustomNetwork2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n__3MnY3y3Ul",
        "outputId": "9e1210f5-56f5-4655-d2ca-8911b28c60cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200, Train Loss: 10571.5425\n",
            "Epoch 1/200, Validation Loss: 5625.1355\n",
            "Epoch 2/200, Train Loss: 2933.9381\n",
            "Epoch 2/200, Validation Loss: 1385.7329\n",
            "Epoch 3/200, Train Loss: 1147.1386\n",
            "Epoch 3/200, Validation Loss: 1163.1114\n",
            "Epoch 4/200, Train Loss: 1163.1960\n",
            "Epoch 4/200, Validation Loss: 1194.2163\n",
            "Epoch 5/200, Train Loss: 1112.3172\n",
            "Epoch 5/200, Validation Loss: 1107.8771\n",
            "Epoch 6/200, Train Loss: 1086.8275\n",
            "Epoch 6/200, Validation Loss: 1096.6878\n",
            "Epoch 7/200, Train Loss: 1085.3559\n",
            "Epoch 7/200, Validation Loss: 1094.7703\n",
            "Epoch 8/200, Train Loss: 1338.9930\n",
            "Epoch 8/200, Validation Loss: 1134.1387\n",
            "Epoch 9/200, Train Loss: 1366.8407\n",
            "Epoch 9/200, Validation Loss: 697.9342\n",
            "Epoch 10/200, Train Loss: 911.2641\n",
            "Epoch 10/200, Validation Loss: 701.3481\n",
            "Epoch 11/200, Train Loss: 731.4867\n",
            "Epoch 11/200, Validation Loss: 716.8067\n",
            "Epoch 12/200, Train Loss: 685.7419\n",
            "Epoch 12/200, Validation Loss: 702.0722\n",
            "Epoch 13/200, Train Loss: 668.8643\n",
            "Epoch 13/200, Validation Loss: 700.5630\n",
            "Epoch 14/200, Train Loss: 663.1038\n",
            "Epoch 14/200, Validation Loss: 692.7400\n",
            "Epoch 15/200, Train Loss: 663.6111\n",
            "Epoch 15/200, Validation Loss: 692.4357\n",
            "Epoch 16/200, Train Loss: 660.5551\n",
            "Epoch 16/200, Validation Loss: 698.5688\n",
            "Epoch 17/200, Train Loss: 663.6424\n",
            "Epoch 17/200, Validation Loss: 692.4798\n",
            "Epoch 18/200, Train Loss: 664.6190\n",
            "Epoch 18/200, Validation Loss: 694.1516\n",
            "Epoch 19/200, Train Loss: 667.3062\n",
            "Epoch 19/200, Validation Loss: 692.5215\n",
            "Epoch 20/200, Train Loss: 666.6003\n",
            "Epoch 20/200, Validation Loss: 694.9944\n",
            "Epoch 21/200, Train Loss: 668.0767\n",
            "Epoch 21/200, Validation Loss: 695.0651\n",
            "Epoch 22/200, Train Loss: 659.9873\n",
            "Epoch 22/200, Validation Loss: 693.1924\n",
            "Epoch 23/200, Train Loss: 664.0024\n",
            "Epoch 23/200, Validation Loss: 694.3325\n",
            "Epoch 24/200, Train Loss: 668.6396\n",
            "Epoch 24/200, Validation Loss: 693.0906\n",
            "Epoch 25/200, Train Loss: 662.9392\n",
            "Epoch 25/200, Validation Loss: 693.6201\n",
            "Epoch 26/200, Train Loss: 664.6074\n",
            "Epoch 26/200, Validation Loss: 699.1392\n",
            "Epoch 27/200, Train Loss: 661.2082\n",
            "Epoch 27/200, Validation Loss: 692.8924\n",
            "Epoch 28/200, Train Loss: 665.3419\n",
            "Epoch 28/200, Validation Loss: 693.2957\n",
            "Epoch 29/200, Train Loss: 663.3291\n",
            "Epoch 29/200, Validation Loss: 692.6958\n",
            "Epoch 30/200, Train Loss: 662.2946\n",
            "Epoch 30/200, Validation Loss: 693.3709\n",
            "Epoch 31/200, Train Loss: 662.8366\n",
            "Epoch 31/200, Validation Loss: 695.3812\n",
            "Epoch 32/200, Train Loss: 663.6793\n",
            "Epoch 32/200, Validation Loss: 692.9107\n",
            "Epoch 33/200, Train Loss: 671.0713\n",
            "Epoch 33/200, Validation Loss: 692.5528\n",
            "Epoch 34/200, Train Loss: 665.5692\n",
            "Epoch 34/200, Validation Loss: 695.2596\n",
            "Epoch 35/200, Train Loss: 665.4867\n",
            "Epoch 35/200, Validation Loss: 692.4362\n",
            "Epoch 36/200, Train Loss: 664.7084\n",
            "Epoch 36/200, Validation Loss: 697.1414\n",
            "Epoch 37/200, Train Loss: 664.1873\n",
            "Epoch 37/200, Validation Loss: 693.0534\n",
            "Epoch 38/200, Train Loss: 660.1617\n",
            "Epoch 38/200, Validation Loss: 692.6435\n",
            "Epoch 39/200, Train Loss: 659.9605\n",
            "Epoch 39/200, Validation Loss: 693.6547\n",
            "Epoch 40/200, Train Loss: 665.7486\n",
            "Epoch 40/200, Validation Loss: 694.7460\n",
            "Epoch 41/200, Train Loss: 660.7858\n",
            "Epoch 41/200, Validation Loss: 692.5265\n",
            "Epoch 42/200, Train Loss: 664.0895\n",
            "Epoch 42/200, Validation Loss: 692.4419\n",
            "Epoch 43/200, Train Loss: 665.5107\n",
            "Epoch 43/200, Validation Loss: 694.9803\n",
            "Epoch 44/200, Train Loss: 666.1130\n",
            "Epoch 44/200, Validation Loss: 692.5955\n",
            "Epoch 45/200, Train Loss: 660.3618\n",
            "Epoch 45/200, Validation Loss: 692.6752\n",
            "Epoch 46/200, Train Loss: 662.6414\n",
            "Epoch 46/200, Validation Loss: 695.1014\n",
            "Epoch 47/200, Train Loss: 666.3628\n",
            "Epoch 47/200, Validation Loss: 692.4980\n",
            "Epoch 48/200, Train Loss: 671.1142\n",
            "Epoch 48/200, Validation Loss: 693.2597\n",
            "Epoch 49/200, Train Loss: 660.7785\n",
            "Epoch 49/200, Validation Loss: 692.4466\n",
            "Epoch 50/200, Train Loss: 663.5168\n",
            "Epoch 50/200, Validation Loss: 692.5393\n",
            "Epoch 51/200, Train Loss: 667.4957\n",
            "Epoch 51/200, Validation Loss: 692.5476\n",
            "Epoch 52/200, Train Loss: 669.5547\n",
            "Epoch 52/200, Validation Loss: 695.6474\n",
            "Epoch 53/200, Train Loss: 660.1912\n",
            "Epoch 53/200, Validation Loss: 692.5028\n",
            "Epoch 54/200, Train Loss: 668.4431\n",
            "Epoch 54/200, Validation Loss: 693.1412\n",
            "Epoch 55/200, Train Loss: 660.2043\n",
            "Epoch 55/200, Validation Loss: 692.5396\n",
            "Epoch 56/200, Train Loss: 659.8184\n",
            "Epoch 56/200, Validation Loss: 694.2254\n",
            "Epoch 57/200, Train Loss: 664.1330\n",
            "Epoch 57/200, Validation Loss: 693.8494\n",
            "Epoch 58/200, Train Loss: 670.0049\n",
            "Epoch 58/200, Validation Loss: 694.1750\n",
            "Epoch 59/200, Train Loss: 659.9460\n",
            "Epoch 59/200, Validation Loss: 693.4887\n",
            "Epoch 60/200, Train Loss: 669.5009\n",
            "Epoch 60/200, Validation Loss: 693.3476\n",
            "Epoch 61/200, Train Loss: 667.2155\n",
            "Epoch 61/200, Validation Loss: 697.4934\n",
            "Epoch 62/200, Train Loss: 657.3265\n",
            "Epoch 62/200, Validation Loss: 694.8404\n",
            "Epoch 63/200, Train Loss: 669.7081\n",
            "Epoch 63/200, Validation Loss: 692.9448\n",
            "Epoch 64/200, Train Loss: 663.8634\n",
            "Epoch 64/200, Validation Loss: 692.4399\n",
            "Epoch 65/200, Train Loss: 660.0157\n",
            "Epoch 65/200, Validation Loss: 692.5686\n",
            "Epoch 66/200, Train Loss: 670.4191\n",
            "Epoch 66/200, Validation Loss: 692.7445\n",
            "Epoch 67/200, Train Loss: 662.8676\n",
            "Epoch 67/200, Validation Loss: 692.4750\n",
            "Epoch 68/200, Train Loss: 660.4373\n",
            "Epoch 68/200, Validation Loss: 693.3564\n",
            "Epoch 69/200, Train Loss: 659.6828\n",
            "Epoch 69/200, Validation Loss: 692.5522\n",
            "Epoch 70/200, Train Loss: 662.9970\n",
            "Epoch 70/200, Validation Loss: 695.7834\n",
            "Epoch 71/200, Train Loss: 670.4180\n",
            "Epoch 71/200, Validation Loss: 692.9277\n",
            "Epoch 72/200, Train Loss: 671.3450\n",
            "Epoch 72/200, Validation Loss: 692.8154\n",
            "Epoch 73/200, Train Loss: 664.3620\n",
            "Epoch 73/200, Validation Loss: 694.5453\n",
            "Epoch 74/200, Train Loss: 663.6434\n",
            "Epoch 74/200, Validation Loss: 692.5249\n",
            "Epoch 75/200, Train Loss: 672.5179\n",
            "Epoch 75/200, Validation Loss: 692.5241\n",
            "Epoch 76/200, Train Loss: 668.2793\n",
            "Epoch 76/200, Validation Loss: 693.2273\n",
            "Epoch 77/200, Train Loss: 660.3696\n",
            "Epoch 77/200, Validation Loss: 692.4950\n",
            "Epoch 78/200, Train Loss: 665.9164\n",
            "Epoch 78/200, Validation Loss: 694.3628\n",
            "Epoch 79/200, Train Loss: 660.5489\n",
            "Epoch 79/200, Validation Loss: 695.8917\n",
            "Epoch 80/200, Train Loss: 663.2351\n",
            "Epoch 80/200, Validation Loss: 694.9723\n",
            "Epoch 81/200, Train Loss: 668.1996\n",
            "Epoch 81/200, Validation Loss: 696.1106\n",
            "Epoch 82/200, Train Loss: 669.5405\n",
            "Epoch 82/200, Validation Loss: 694.9414\n",
            "Epoch 83/200, Train Loss: 662.5164\n",
            "Epoch 83/200, Validation Loss: 692.7533\n",
            "Epoch 84/200, Train Loss: 665.6058\n",
            "Epoch 84/200, Validation Loss: 694.5838\n",
            "Epoch 85/200, Train Loss: 666.0236\n",
            "Epoch 85/200, Validation Loss: 693.0770\n",
            "Epoch 86/200, Train Loss: 662.9255\n",
            "Epoch 86/200, Validation Loss: 692.4414\n",
            "Epoch 87/200, Train Loss: 663.0475\n",
            "Epoch 87/200, Validation Loss: 692.5276\n",
            "Epoch 88/200, Train Loss: 667.3507\n",
            "Epoch 88/200, Validation Loss: 692.7312\n",
            "Epoch 89/200, Train Loss: 665.5459\n",
            "Epoch 89/200, Validation Loss: 693.6054\n",
            "Epoch 90/200, Train Loss: 664.4731\n",
            "Epoch 90/200, Validation Loss: 696.7471\n",
            "Epoch 91/200, Train Loss: 667.7713\n",
            "Epoch 91/200, Validation Loss: 693.6389\n",
            "Epoch 92/200, Train Loss: 666.3518\n",
            "Epoch 92/200, Validation Loss: 692.4379\n",
            "Epoch 93/200, Train Loss: 667.9695\n",
            "Epoch 93/200, Validation Loss: 692.4684\n",
            "Epoch 94/200, Train Loss: 666.1957\n",
            "Epoch 94/200, Validation Loss: 696.4761\n",
            "Epoch 95/200, Train Loss: 671.5429\n",
            "Epoch 95/200, Validation Loss: 698.2030\n",
            "Epoch 96/200, Train Loss: 669.6274\n",
            "Epoch 96/200, Validation Loss: 695.5181\n",
            "Epoch 97/200, Train Loss: 662.5011\n",
            "Epoch 97/200, Validation Loss: 692.6774\n",
            "Epoch 98/200, Train Loss: 669.4285\n",
            "Epoch 98/200, Validation Loss: 692.8489\n",
            "Epoch 99/200, Train Loss: 663.3271\n",
            "Epoch 99/200, Validation Loss: 694.0972\n",
            "Epoch 100/200, Train Loss: 665.3113\n",
            "Epoch 100/200, Validation Loss: 692.4817\n",
            "Epoch 101/200, Train Loss: 666.4612\n",
            "Epoch 101/200, Validation Loss: 692.4415\n",
            "Epoch 102/200, Train Loss: 666.2291\n",
            "Epoch 102/200, Validation Loss: 692.5994\n",
            "Epoch 103/200, Train Loss: 662.1102\n",
            "Epoch 103/200, Validation Loss: 692.6082\n",
            "Epoch 104/200, Train Loss: 661.1858\n",
            "Epoch 104/200, Validation Loss: 699.7611\n",
            "Epoch 105/200, Train Loss: 662.7974\n",
            "Epoch 105/200, Validation Loss: 692.4390\n",
            "Epoch 106/200, Train Loss: 661.3008\n",
            "Epoch 106/200, Validation Loss: 692.6032\n",
            "Epoch 107/200, Train Loss: 666.5287\n",
            "Epoch 107/200, Validation Loss: 696.4769\n",
            "Epoch 108/200, Train Loss: 664.2263\n",
            "Epoch 108/200, Validation Loss: 694.1484\n",
            "Epoch 109/200, Train Loss: 666.9287\n",
            "Epoch 109/200, Validation Loss: 693.0402\n",
            "Epoch 110/200, Train Loss: 666.3289\n",
            "Epoch 110/200, Validation Loss: 693.5496\n",
            "Epoch 111/200, Train Loss: 656.7624\n",
            "Epoch 111/200, Validation Loss: 694.6683\n",
            "Epoch 112/200, Train Loss: 680.5271\n",
            "Epoch 112/200, Validation Loss: 708.8997\n",
            "Epoch 113/200, Train Loss: 665.8979\n",
            "Epoch 113/200, Validation Loss: 694.8839\n",
            "Epoch 114/200, Train Loss: 662.9960\n",
            "Epoch 114/200, Validation Loss: 692.6648\n",
            "Epoch 115/200, Train Loss: 667.9869\n",
            "Epoch 115/200, Validation Loss: 692.7225\n",
            "Epoch 116/200, Train Loss: 669.9566\n",
            "Epoch 116/200, Validation Loss: 692.4465\n",
            "Epoch 117/200, Train Loss: 662.6406\n",
            "Epoch 117/200, Validation Loss: 699.1337\n",
            "Epoch 118/200, Train Loss: 665.2582\n",
            "Epoch 118/200, Validation Loss: 692.4626\n",
            "Epoch 119/200, Train Loss: 667.2625\n",
            "Epoch 119/200, Validation Loss: 692.5840\n",
            "Epoch 120/200, Train Loss: 667.3928\n",
            "Epoch 120/200, Validation Loss: 698.3877\n",
            "Epoch 121/200, Train Loss: 667.8921\n",
            "Epoch 121/200, Validation Loss: 692.4974\n",
            "Epoch 122/200, Train Loss: 669.7126\n",
            "Epoch 122/200, Validation Loss: 692.8731\n",
            "Epoch 123/200, Train Loss: 675.4207\n",
            "Epoch 123/200, Validation Loss: 704.4931\n",
            "Epoch 124/200, Train Loss: 669.9037\n",
            "Epoch 124/200, Validation Loss: 723.6996\n",
            "Epoch 125/200, Train Loss: 670.9048\n",
            "Epoch 125/200, Validation Loss: 695.6087\n",
            "Epoch 126/200, Train Loss: 662.8967\n",
            "Epoch 126/200, Validation Loss: 714.3639\n",
            "Epoch 127/200, Train Loss: 667.1947\n",
            "Epoch 127/200, Validation Loss: 694.6127\n",
            "Epoch 128/200, Train Loss: 663.4409\n",
            "Epoch 128/200, Validation Loss: 695.5413\n",
            "Epoch 129/200, Train Loss: 663.6835\n",
            "Epoch 129/200, Validation Loss: 694.2764\n",
            "Epoch 130/200, Train Loss: 666.1227\n",
            "Epoch 130/200, Validation Loss: 692.4519\n",
            "Epoch 131/200, Train Loss: 664.1553\n",
            "Epoch 131/200, Validation Loss: 693.9760\n",
            "Epoch 132/200, Train Loss: 661.4265\n",
            "Epoch 132/200, Validation Loss: 696.8699\n",
            "Epoch 133/200, Train Loss: 665.0079\n",
            "Epoch 133/200, Validation Loss: 692.7570\n",
            "Epoch 134/200, Train Loss: 665.1827\n",
            "Epoch 134/200, Validation Loss: 703.0627\n",
            "Epoch 135/200, Train Loss: 664.2190\n",
            "Epoch 135/200, Validation Loss: 692.7641\n",
            "Epoch 136/200, Train Loss: 663.7654\n",
            "Epoch 136/200, Validation Loss: 709.8429\n",
            "Epoch 137/200, Train Loss: 675.0273\n",
            "Epoch 137/200, Validation Loss: 693.5065\n",
            "Epoch 138/200, Train Loss: 672.8833\n",
            "Epoch 138/200, Validation Loss: 699.7216\n",
            "Epoch 139/200, Train Loss: 672.2283\n",
            "Epoch 139/200, Validation Loss: 692.6364\n",
            "Epoch 140/200, Train Loss: 660.0906\n",
            "Epoch 140/200, Validation Loss: 699.3106\n",
            "Epoch 141/200, Train Loss: 662.9136\n",
            "Epoch 141/200, Validation Loss: 692.7882\n",
            "Epoch 142/200, Train Loss: 665.7340\n",
            "Epoch 142/200, Validation Loss: 705.4670\n",
            "Epoch 143/200, Train Loss: 673.8950\n",
            "Epoch 143/200, Validation Loss: 692.6782\n",
            "Epoch 144/200, Train Loss: 665.9670\n",
            "Epoch 144/200, Validation Loss: 703.7011\n",
            "Epoch 145/200, Train Loss: 676.9640\n",
            "Epoch 145/200, Validation Loss: 692.5623\n",
            "Epoch 146/200, Train Loss: 671.6365\n",
            "Epoch 146/200, Validation Loss: 696.6889\n",
            "Epoch 147/200, Train Loss: 665.6395\n",
            "Epoch 147/200, Validation Loss: 692.5708\n",
            "Epoch 148/200, Train Loss: 665.3553\n",
            "Epoch 148/200, Validation Loss: 698.5056\n",
            "Epoch 149/200, Train Loss: 666.3661\n",
            "Epoch 149/200, Validation Loss: 704.0264\n",
            "Epoch 150/200, Train Loss: 673.2419\n",
            "Epoch 150/200, Validation Loss: 702.0081\n",
            "Epoch 151/200, Train Loss: 668.0434\n",
            "Epoch 151/200, Validation Loss: 692.4713\n",
            "Epoch 152/200, Train Loss: 662.9035\n",
            "Epoch 152/200, Validation Loss: 700.7201\n",
            "Epoch 153/200, Train Loss: 671.1234\n",
            "Epoch 153/200, Validation Loss: 693.1477\n",
            "Epoch 154/200, Train Loss: 673.3557\n",
            "Epoch 154/200, Validation Loss: 701.5327\n",
            "Epoch 155/200, Train Loss: 663.7589\n",
            "Epoch 155/200, Validation Loss: 693.1300\n",
            "Epoch 156/200, Train Loss: 665.4782\n",
            "Epoch 156/200, Validation Loss: 692.5648\n",
            "Epoch 157/200, Train Loss: 669.4491\n",
            "Epoch 157/200, Validation Loss: 700.0029\n",
            "Epoch 158/200, Train Loss: 676.6138\n",
            "Epoch 158/200, Validation Loss: 706.8799\n",
            "Epoch 159/200, Train Loss: 666.7822\n",
            "Epoch 159/200, Validation Loss: 695.5808\n",
            "Epoch 160/200, Train Loss: 662.0081\n",
            "Epoch 160/200, Validation Loss: 695.6963\n",
            "Epoch 161/200, Train Loss: 662.0033\n",
            "Epoch 161/200, Validation Loss: 700.0311\n",
            "Epoch 162/200, Train Loss: 668.8653\n",
            "Epoch 162/200, Validation Loss: 697.5155\n",
            "Epoch 163/200, Train Loss: 671.2492\n",
            "Epoch 163/200, Validation Loss: 692.8520\n",
            "Epoch 164/200, Train Loss: 660.4454\n",
            "Epoch 164/200, Validation Loss: 693.2088\n",
            "Epoch 165/200, Train Loss: 661.8122\n",
            "Epoch 165/200, Validation Loss: 692.5309\n",
            "Epoch 166/200, Train Loss: 664.7018\n",
            "Epoch 166/200, Validation Loss: 693.0257\n",
            "Epoch 167/200, Train Loss: 671.4327\n",
            "Epoch 167/200, Validation Loss: 712.9754\n",
            "Epoch 168/200, Train Loss: 672.2349\n",
            "Epoch 168/200, Validation Loss: 692.6532\n",
            "Epoch 169/200, Train Loss: 660.8600\n",
            "Epoch 169/200, Validation Loss: 694.0592\n",
            "Epoch 170/200, Train Loss: 664.2651\n",
            "Epoch 170/200, Validation Loss: 692.7761\n",
            "Epoch 171/200, Train Loss: 664.9447\n",
            "Epoch 171/200, Validation Loss: 699.9853\n",
            "Epoch 172/200, Train Loss: 667.2478\n",
            "Epoch 172/200, Validation Loss: 693.5740\n",
            "Epoch 173/200, Train Loss: 666.6798\n",
            "Epoch 173/200, Validation Loss: 693.5632\n",
            "Epoch 174/200, Train Loss: 666.6630\n",
            "Epoch 174/200, Validation Loss: 692.8329\n",
            "Epoch 175/200, Train Loss: 666.1570\n",
            "Epoch 175/200, Validation Loss: 692.8019\n",
            "Epoch 176/200, Train Loss: 674.3735\n",
            "Epoch 176/200, Validation Loss: 732.7154\n",
            "Epoch 177/200, Train Loss: 679.9004\n",
            "Epoch 177/200, Validation Loss: 702.9610\n",
            "Epoch 178/200, Train Loss: 673.1321\n",
            "Epoch 178/200, Validation Loss: 693.9737\n",
            "Epoch 179/200, Train Loss: 664.9672\n",
            "Epoch 179/200, Validation Loss: 692.6622\n",
            "Epoch 180/200, Train Loss: 665.9361\n",
            "Epoch 180/200, Validation Loss: 692.5895\n",
            "Epoch 181/200, Train Loss: 666.4836\n",
            "Epoch 181/200, Validation Loss: 700.0930\n",
            "Epoch 182/200, Train Loss: 667.6755\n",
            "Epoch 182/200, Validation Loss: 692.5711\n",
            "Epoch 183/200, Train Loss: 665.2516\n",
            "Epoch 183/200, Validation Loss: 694.7542\n",
            "Epoch 184/200, Train Loss: 665.0366\n",
            "Epoch 184/200, Validation Loss: 704.4968\n",
            "Epoch 185/200, Train Loss: 666.9779\n",
            "Epoch 185/200, Validation Loss: 693.0740\n",
            "Epoch 186/200, Train Loss: 665.1747\n",
            "Epoch 186/200, Validation Loss: 694.7935\n",
            "Epoch 187/200, Train Loss: 659.4504\n",
            "Epoch 187/200, Validation Loss: 692.4625\n",
            "Epoch 188/200, Train Loss: 670.8457\n",
            "Epoch 188/200, Validation Loss: 696.3275\n",
            "Epoch 189/200, Train Loss: 665.3236\n",
            "Epoch 189/200, Validation Loss: 699.3462\n",
            "Epoch 190/200, Train Loss: 662.4635\n",
            "Epoch 190/200, Validation Loss: 694.2218\n",
            "Epoch 191/200, Train Loss: 665.7679\n",
            "Epoch 191/200, Validation Loss: 694.7776\n",
            "Epoch 192/200, Train Loss: 665.3205\n",
            "Epoch 192/200, Validation Loss: 693.7515\n",
            "Epoch 193/200, Train Loss: 665.5069\n",
            "Epoch 193/200, Validation Loss: 692.4487\n",
            "Epoch 194/200, Train Loss: 660.4279\n",
            "Epoch 194/200, Validation Loss: 692.4468\n",
            "Epoch 195/200, Train Loss: 664.8722\n",
            "Epoch 195/200, Validation Loss: 697.4194\n",
            "Epoch 196/200, Train Loss: 670.4954\n",
            "Epoch 196/200, Validation Loss: 694.5729\n",
            "Epoch 197/200, Train Loss: 664.7340\n",
            "Epoch 197/200, Validation Loss: 695.8045\n",
            "Epoch 198/200, Train Loss: 663.4322\n",
            "Epoch 198/200, Validation Loss: 693.9847\n",
            "Epoch 199/200, Train Loss: 665.4051\n",
            "Epoch 199/200, Validation Loss: 697.2252\n",
            "Epoch 200/200, Train Loss: 669.7793\n",
            "Epoch 200/200, Validation Loss: 698.5119\n"
          ]
        }
      ],
      "source": [
        "#cargamos los datos1\n",
        "#num_samples=8000\n",
        "#x_train,y_train=get_dataset(num_samples)\n",
        "#x_train=np.expand_dims(x_train,-1)\n",
        "\n",
        "#probamos con los datos2\n",
        "\n",
        "num_samples=1000\n",
        "input_dim=1\n",
        "len_seq=3\n",
        "x_train=np.random.randint(0,100,(num_samples,len_seq))\n",
        "y_train=x_train[:,::-1].copy()\n",
        "\n",
        "x_train,y_train=np.expand_dims(x_train,-1),np.expand_dims(y_train,-1)\n",
        "\n",
        "x_train,y_train=torch.from_numpy(x_train).float() ,torch.from_numpy(y_train).float()\n",
        "\n",
        "\n",
        "#instanciamos el modelo\n",
        "d_k=15\n",
        "gc.collect()\n",
        "try:\n",
        "    del model\n",
        "except:\n",
        "    pass\n",
        "model = CustomNetwork(input_dim, d_k,input_dim)\n",
        "\n",
        "#creamos la funcion perdida y el optimizador\n",
        "loss_criterion=nn.MSELoss()\n",
        "optimizer=optim.Adam(model.parameters(),lr=0.01)\n",
        "#scheduler = ExponentialLR(optimizer, 0.95)\n",
        "\n",
        "\n",
        "#creamos los datasets y los dataloaders\n",
        "train_size = int(0.8 * num_samples)\n",
        "val_size = num_samples - train_size\n",
        "train_dataset, val_dataset = random_split(TensorDataset(x_train,y_train), [train_size, val_size])\n",
        "\n",
        "# Crear DataLoaders para entrenamiento y validación\n",
        "batch_size=64\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "epocas=200\n",
        "\n",
        "\n",
        "#hacemos un bucle para entrenamiento\n",
        "for epoca in range(epocas):\n",
        "    #establecemos el modelo en modo entrenamiento\n",
        "    model.train()\n",
        "\n",
        "    #establecemos en 0 el contador de la funcion de perdida\n",
        "    train_loss=0\n",
        "\n",
        "    #bucle para recorrer todos los batches\n",
        "    for batch_x_train,batch_y_train in train_dataloader:\n",
        "        #ponemos a zero los gradientes para el optimizador\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #predecimos\n",
        "\n",
        "        output = model(batch_x_train)\n",
        "\n",
        "        #calculamos la perdida\n",
        "        loss = loss_criterion(output,batch_y_train)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        #retropropagamos/calculamso lso gradientes\n",
        "        loss.backward()\n",
        "        #y modificamos los pesos\n",
        "        optimizer.step()\n",
        "\n",
        "    train_loss /= len(train_dataloader)\n",
        "    print(f\"Epoch {epoca + 1}/{epocas}, Train Loss: {train_loss:.4f}\")\n",
        "\n",
        "\n",
        "    # modo validacion, \n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_x_val,batch_y_val in val_dataloader:\n",
        "            outputs = model(batch_x_val)\n",
        "            loss = loss_criterion(outputs, batch_y_val)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_loss /= len(val_dataloader)\n",
        "    print(f\"Epoch {epoca + 1}/{epocas}, Validation Loss: {val_loss:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqwoFPpPy3Um"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_attention_heatmap(attention_matrix, labels):\n",
        "    fig, ax = plt.subplots(figsize=(16,16))\n",
        "    cax = ax.matshow(attention_matrix, cmap=plt.cm.Blues)\n",
        "\n",
        "    # Configurar etiquetas de los ejes\n",
        "    ax.set_xticklabels(labels, rotation=90)\n",
        "    ax.set_yticklabels(labels)\n",
        "\n",
        "    # Alinear las etiquetas de los ejes a las marcas de graduación\n",
        "    ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(plt.MultipleLocator(1))\n",
        "\n",
        "    # Agregar una barra de colores\n",
        "    cbar = fig.colorbar(cax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjCfDZ3Ty3Um",
        "outputId": "8e61de8a-a8be-42a6-eedb-4cac25f4f6b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[12.],\n",
            "         [58.],\n",
            "         [46.]]])\n",
            "tensor([[[38.9905],\n",
            "         [38.9905],\n",
            "         [38.9905]]], grad_fn=<ViewBackward0>)\n",
            "tensor([[[0.0000e+00, 1.0000e+00, 1.9153e-22],\n",
            "         [0.0000e+00, 1.0000e+00, 0.0000e+00],\n",
            "         [0.0000e+00, 1.0000e+00, 0.0000e+00]]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "x=torch.randint(0,100,(1,len_seq,1)).float()\n",
        "print(x)\n",
        "y,att=model(x,return_attention=True)\n",
        "print(y)\n",
        "print(att)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yefi-SiWy3Um",
        "outputId": "0d522b98-88d2-40bd-aae9-df42e7f58c72"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_10244/1681730846.py:6: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
            "  ax.set_xticklabels(labels, rotation=90)\n",
            "/tmp/ipykernel_10244/1681730846.py:7: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
            "  ax.set_yticklabels(labels)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1600x1600 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_attention_heatmap(att.detach().squeeze().numpy(),x.detach().squeeze().numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWBC-n5ay3Um"
      },
      "source": [
        "### Entrenar CustomNetwork2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_tvORBPy3Un",
        "outputId": "6e7d4358-46a5-40b5-a8dd-dcbe770287af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200, Train Loss: 1017.4464\n",
            "Epoch 1/200, Validation Loss: 588.5538\n",
            "Epoch 2/200, Train Loss: 654.2844\n",
            "Epoch 2/200, Validation Loss: 964.6838\n",
            "Epoch 3/200, Train Loss: 691.4332\n",
            "Epoch 3/200, Validation Loss: 546.1404\n",
            "Epoch 4/200, Train Loss: 597.3898\n",
            "Epoch 4/200, Validation Loss: 557.1630\n",
            "Epoch 5/200, Train Loss: 582.3827\n",
            "Epoch 5/200, Validation Loss: 535.5259\n",
            "Epoch 6/200, Train Loss: 582.1234\n",
            "Epoch 6/200, Validation Loss: 532.4084\n",
            "Epoch 7/200, Train Loss: 578.7372\n",
            "Epoch 7/200, Validation Loss: 535.4134\n",
            "Epoch 8/200, Train Loss: 581.9578\n",
            "Epoch 8/200, Validation Loss: 536.2614\n",
            "Epoch 9/200, Train Loss: 580.8549\n",
            "Epoch 9/200, Validation Loss: 537.4352\n",
            "Epoch 10/200, Train Loss: 577.8993\n",
            "Epoch 10/200, Validation Loss: 536.6995\n",
            "Epoch 11/200, Train Loss: 584.0578\n",
            "Epoch 11/200, Validation Loss: 536.1711\n",
            "Epoch 12/200, Train Loss: 579.9230\n",
            "Epoch 12/200, Validation Loss: 534.3603\n",
            "Epoch 13/200, Train Loss: 577.5793\n",
            "Epoch 13/200, Validation Loss: 542.4921\n",
            "Epoch 14/200, Train Loss: 576.1330\n",
            "Epoch 14/200, Validation Loss: 534.3523\n",
            "Epoch 15/200, Train Loss: 579.9505\n",
            "Epoch 15/200, Validation Loss: 534.5635\n",
            "Epoch 16/200, Train Loss: 578.5280\n",
            "Epoch 16/200, Validation Loss: 533.7356\n",
            "Epoch 17/200, Train Loss: 585.4407\n",
            "Epoch 17/200, Validation Loss: 535.1621\n",
            "Epoch 18/200, Train Loss: 582.8341\n",
            "Epoch 18/200, Validation Loss: 535.9707\n",
            "Epoch 19/200, Train Loss: 581.8529\n",
            "Epoch 19/200, Validation Loss: 534.4932\n",
            "Epoch 20/200, Train Loss: 577.8383\n",
            "Epoch 20/200, Validation Loss: 534.4221\n",
            "Epoch 21/200, Train Loss: 579.7402\n",
            "Epoch 21/200, Validation Loss: 538.7634\n",
            "Epoch 22/200, Train Loss: 579.4192\n",
            "Epoch 22/200, Validation Loss: 536.5684\n",
            "Epoch 23/200, Train Loss: 578.6882\n",
            "Epoch 23/200, Validation Loss: 533.8120\n",
            "Epoch 24/200, Train Loss: 583.9414\n",
            "Epoch 24/200, Validation Loss: 537.8440\n",
            "Epoch 25/200, Train Loss: 581.7989\n",
            "Epoch 25/200, Validation Loss: 535.2033\n",
            "Epoch 26/200, Train Loss: 587.2057\n",
            "Epoch 26/200, Validation Loss: 538.9364\n",
            "Epoch 27/200, Train Loss: 583.0280\n",
            "Epoch 27/200, Validation Loss: 533.7771\n",
            "Epoch 28/200, Train Loss: 584.9965\n",
            "Epoch 28/200, Validation Loss: 536.3285\n",
            "Epoch 29/200, Train Loss: 578.5016\n",
            "Epoch 29/200, Validation Loss: 534.0509\n",
            "Epoch 30/200, Train Loss: 578.0417\n",
            "Epoch 30/200, Validation Loss: 536.1219\n",
            "Epoch 31/200, Train Loss: 580.4890\n",
            "Epoch 31/200, Validation Loss: 538.7752\n",
            "Epoch 32/200, Train Loss: 576.4482\n",
            "Epoch 32/200, Validation Loss: 534.4994\n",
            "Epoch 33/200, Train Loss: 581.8856\n",
            "Epoch 33/200, Validation Loss: 534.3363\n",
            "Epoch 34/200, Train Loss: 576.7107\n",
            "Epoch 34/200, Validation Loss: 533.9851\n",
            "Epoch 35/200, Train Loss: 580.8005\n",
            "Epoch 35/200, Validation Loss: 539.7098\n",
            "Epoch 36/200, Train Loss: 579.1803\n",
            "Epoch 36/200, Validation Loss: 534.8250\n",
            "Epoch 37/200, Train Loss: 586.7410\n",
            "Epoch 37/200, Validation Loss: 537.0788\n",
            "Epoch 38/200, Train Loss: 581.6845\n",
            "Epoch 38/200, Validation Loss: 534.0894\n",
            "Epoch 39/200, Train Loss: 575.0537\n",
            "Epoch 39/200, Validation Loss: 539.8858\n",
            "Epoch 40/200, Train Loss: 587.3690\n",
            "Epoch 40/200, Validation Loss: 534.5405\n",
            "Epoch 41/200, Train Loss: 576.8654\n",
            "Epoch 41/200, Validation Loss: 538.8904\n",
            "Epoch 42/200, Train Loss: 582.8223\n",
            "Epoch 42/200, Validation Loss: 546.0257\n",
            "Epoch 43/200, Train Loss: 582.0439\n",
            "Epoch 43/200, Validation Loss: 544.5306\n",
            "Epoch 44/200, Train Loss: 586.5060\n",
            "Epoch 44/200, Validation Loss: 544.4920\n",
            "Epoch 45/200, Train Loss: 584.8237\n",
            "Epoch 45/200, Validation Loss: 536.8750\n",
            "Epoch 46/200, Train Loss: 584.8550\n",
            "Epoch 46/200, Validation Loss: 533.5847\n",
            "Epoch 47/200, Train Loss: 579.8113\n",
            "Epoch 47/200, Validation Loss: 533.3609\n",
            "Epoch 48/200, Train Loss: 587.6328\n",
            "Epoch 48/200, Validation Loss: 536.7087\n",
            "Epoch 49/200, Train Loss: 582.0711\n",
            "Epoch 49/200, Validation Loss: 533.5794\n",
            "Epoch 50/200, Train Loss: 585.8367\n",
            "Epoch 50/200, Validation Loss: 536.4467\n",
            "Epoch 51/200, Train Loss: 581.5116\n",
            "Epoch 51/200, Validation Loss: 535.2532\n",
            "Epoch 52/200, Train Loss: 584.5197\n",
            "Epoch 52/200, Validation Loss: 537.3064\n",
            "Epoch 53/200, Train Loss: 580.2625\n",
            "Epoch 53/200, Validation Loss: 533.9822\n",
            "Epoch 54/200, Train Loss: 575.9531\n",
            "Epoch 54/200, Validation Loss: 533.7530\n",
            "Epoch 55/200, Train Loss: 575.9474\n",
            "Epoch 55/200, Validation Loss: 533.7941\n",
            "Epoch 56/200, Train Loss: 585.8078\n",
            "Epoch 56/200, Validation Loss: 533.1579\n",
            "Epoch 57/200, Train Loss: 585.2662\n",
            "Epoch 57/200, Validation Loss: 544.6115\n",
            "Epoch 58/200, Train Loss: 582.8283\n",
            "Epoch 58/200, Validation Loss: 548.5954\n",
            "Epoch 59/200, Train Loss: 585.3954\n",
            "Epoch 59/200, Validation Loss: 536.3119\n",
            "Epoch 60/200, Train Loss: 582.4873\n",
            "Epoch 60/200, Validation Loss: 536.9396\n",
            "Epoch 61/200, Train Loss: 591.9799\n",
            "Epoch 61/200, Validation Loss: 548.0053\n",
            "Epoch 62/200, Train Loss: 589.9178\n",
            "Epoch 62/200, Validation Loss: 543.1170\n",
            "Epoch 63/200, Train Loss: 578.0594\n",
            "Epoch 63/200, Validation Loss: 534.8020\n",
            "Epoch 64/200, Train Loss: 581.1295\n",
            "Epoch 64/200, Validation Loss: 534.4683\n",
            "Epoch 65/200, Train Loss: 581.4062\n",
            "Epoch 65/200, Validation Loss: 534.0513\n",
            "Epoch 66/200, Train Loss: 586.7971\n",
            "Epoch 66/200, Validation Loss: 536.6424\n",
            "Epoch 67/200, Train Loss: 589.4873\n",
            "Epoch 67/200, Validation Loss: 533.9802\n",
            "Epoch 68/200, Train Loss: 583.5540\n",
            "Epoch 68/200, Validation Loss: 551.2660\n",
            "Epoch 69/200, Train Loss: 584.1807\n",
            "Epoch 69/200, Validation Loss: 543.8331\n",
            "Epoch 70/200, Train Loss: 589.2642\n",
            "Epoch 70/200, Validation Loss: 538.7329\n",
            "Epoch 71/200, Train Loss: 579.9696\n",
            "Epoch 71/200, Validation Loss: 537.8177\n",
            "Epoch 72/200, Train Loss: 585.9118\n",
            "Epoch 72/200, Validation Loss: 536.2834\n",
            "Epoch 73/200, Train Loss: 581.7463\n",
            "Epoch 73/200, Validation Loss: 533.5886\n",
            "Epoch 74/200, Train Loss: 579.1722\n",
            "Epoch 74/200, Validation Loss: 537.0007\n",
            "Epoch 75/200, Train Loss: 578.3212\n",
            "Epoch 75/200, Validation Loss: 538.6151\n",
            "Epoch 76/200, Train Loss: 576.2173\n",
            "Epoch 76/200, Validation Loss: 537.1259\n",
            "Epoch 77/200, Train Loss: 584.7417\n",
            "Epoch 77/200, Validation Loss: 536.5089\n",
            "Epoch 78/200, Train Loss: 581.8868\n",
            "Epoch 78/200, Validation Loss: 533.1923\n",
            "Epoch 79/200, Train Loss: 575.8080\n",
            "Epoch 79/200, Validation Loss: 536.7367\n",
            "Epoch 80/200, Train Loss: 580.8053\n",
            "Epoch 80/200, Validation Loss: 535.6057\n",
            "Epoch 81/200, Train Loss: 582.8149\n",
            "Epoch 81/200, Validation Loss: 537.9529\n",
            "Epoch 82/200, Train Loss: 588.0375\n",
            "Epoch 82/200, Validation Loss: 535.9252\n",
            "Epoch 83/200, Train Loss: 584.4940\n",
            "Epoch 83/200, Validation Loss: 532.6287\n",
            "Epoch 84/200, Train Loss: 578.1265\n",
            "Epoch 84/200, Validation Loss: 535.0519\n",
            "Epoch 85/200, Train Loss: 589.1552\n",
            "Epoch 85/200, Validation Loss: 544.2208\n",
            "Epoch 86/200, Train Loss: 581.6972\n",
            "Epoch 86/200, Validation Loss: 550.9377\n",
            "Epoch 87/200, Train Loss: 586.7228\n",
            "Epoch 87/200, Validation Loss: 551.1236\n",
            "Epoch 88/200, Train Loss: 581.5156\n",
            "Epoch 88/200, Validation Loss: 535.4642\n",
            "Epoch 89/200, Train Loss: 577.7786\n",
            "Epoch 89/200, Validation Loss: 534.8620\n",
            "Epoch 90/200, Train Loss: 586.4114\n",
            "Epoch 90/200, Validation Loss: 537.8815\n",
            "Epoch 91/200, Train Loss: 585.7321\n",
            "Epoch 91/200, Validation Loss: 538.4209\n",
            "Epoch 92/200, Train Loss: 580.2006\n",
            "Epoch 92/200, Validation Loss: 544.9876\n",
            "Epoch 93/200, Train Loss: 580.4826\n",
            "Epoch 93/200, Validation Loss: 544.9144\n",
            "Epoch 94/200, Train Loss: 583.3348\n",
            "Epoch 94/200, Validation Loss: 540.0376\n",
            "Epoch 95/200, Train Loss: 587.8692\n",
            "Epoch 95/200, Validation Loss: 533.7971\n",
            "Epoch 96/200, Train Loss: 577.0536\n",
            "Epoch 96/200, Validation Loss: 533.6744\n",
            "Epoch 97/200, Train Loss: 584.8670\n",
            "Epoch 97/200, Validation Loss: 536.8449\n",
            "Epoch 98/200, Train Loss: 578.8254\n",
            "Epoch 98/200, Validation Loss: 536.4216\n",
            "Epoch 99/200, Train Loss: 582.5997\n",
            "Epoch 99/200, Validation Loss: 548.3068\n",
            "Epoch 100/200, Train Loss: 590.1414\n",
            "Epoch 100/200, Validation Loss: 536.6214\n",
            "Epoch 101/200, Train Loss: 578.2316\n",
            "Epoch 101/200, Validation Loss: 534.0420\n",
            "Epoch 102/200, Train Loss: 581.0253\n",
            "Epoch 102/200, Validation Loss: 534.1373\n",
            "Epoch 103/200, Train Loss: 580.6977\n",
            "Epoch 103/200, Validation Loss: 533.4852\n",
            "Epoch 104/200, Train Loss: 574.2503\n",
            "Epoch 104/200, Validation Loss: 533.1571\n",
            "Epoch 105/200, Train Loss: 581.6389\n",
            "Epoch 105/200, Validation Loss: 535.9356\n",
            "Epoch 106/200, Train Loss: 581.9929\n",
            "Epoch 106/200, Validation Loss: 535.2520\n",
            "Epoch 107/200, Train Loss: 584.2288\n",
            "Epoch 107/200, Validation Loss: 537.6346\n",
            "Epoch 108/200, Train Loss: 582.3893\n",
            "Epoch 108/200, Validation Loss: 536.0533\n",
            "Epoch 109/200, Train Loss: 581.2547\n",
            "Epoch 109/200, Validation Loss: 533.4249\n",
            "Epoch 110/200, Train Loss: 583.9096\n",
            "Epoch 110/200, Validation Loss: 536.2991\n",
            "Epoch 111/200, Train Loss: 585.8605\n",
            "Epoch 111/200, Validation Loss: 532.5789\n",
            "Epoch 112/200, Train Loss: 582.6130\n",
            "Epoch 112/200, Validation Loss: 538.1953\n",
            "Epoch 113/200, Train Loss: 582.3384\n",
            "Epoch 113/200, Validation Loss: 534.2859\n",
            "Epoch 114/200, Train Loss: 576.7595\n",
            "Epoch 114/200, Validation Loss: 533.9665\n",
            "Epoch 115/200, Train Loss: 584.9693\n",
            "Epoch 115/200, Validation Loss: 534.4112\n",
            "Epoch 116/200, Train Loss: 585.0887\n",
            "Epoch 116/200, Validation Loss: 535.9902\n",
            "Epoch 117/200, Train Loss: 584.5813\n",
            "Epoch 117/200, Validation Loss: 535.5848\n",
            "Epoch 118/200, Train Loss: 585.8775\n",
            "Epoch 118/200, Validation Loss: 543.2981\n",
            "Epoch 119/200, Train Loss: 580.8331\n",
            "Epoch 119/200, Validation Loss: 533.2158\n",
            "Epoch 120/200, Train Loss: 579.2859\n",
            "Epoch 120/200, Validation Loss: 536.8758\n",
            "Epoch 121/200, Train Loss: 584.7615\n",
            "Epoch 121/200, Validation Loss: 540.2025\n",
            "Epoch 122/200, Train Loss: 578.9879\n",
            "Epoch 122/200, Validation Loss: 539.6428\n",
            "Epoch 123/200, Train Loss: 579.2979\n",
            "Epoch 123/200, Validation Loss: 538.8115\n",
            "Epoch 124/200, Train Loss: 584.6590\n",
            "Epoch 124/200, Validation Loss: 541.7141\n",
            "Epoch 125/200, Train Loss: 579.6621\n",
            "Epoch 125/200, Validation Loss: 556.2240\n",
            "Epoch 126/200, Train Loss: 590.2945\n",
            "Epoch 126/200, Validation Loss: 539.8787\n",
            "Epoch 127/200, Train Loss: 580.1689\n",
            "Epoch 127/200, Validation Loss: 540.5441\n",
            "Epoch 128/200, Train Loss: 586.8236\n",
            "Epoch 128/200, Validation Loss: 557.5633\n",
            "Epoch 129/200, Train Loss: 584.9931\n",
            "Epoch 129/200, Validation Loss: 553.2848\n",
            "Epoch 130/200, Train Loss: 582.4071\n",
            "Epoch 130/200, Validation Loss: 541.6893\n",
            "Epoch 131/200, Train Loss: 580.6297\n",
            "Epoch 131/200, Validation Loss: 540.6715\n",
            "Epoch 132/200, Train Loss: 582.1957\n",
            "Epoch 132/200, Validation Loss: 534.3281\n",
            "Epoch 133/200, Train Loss: 588.2672\n",
            "Epoch 133/200, Validation Loss: 542.8365\n",
            "Epoch 134/200, Train Loss: 590.4265\n",
            "Epoch 134/200, Validation Loss: 538.1715\n",
            "Epoch 135/200, Train Loss: 590.2735\n",
            "Epoch 135/200, Validation Loss: 539.4600\n",
            "Epoch 136/200, Train Loss: 591.6893\n",
            "Epoch 136/200, Validation Loss: 533.1071\n",
            "Epoch 137/200, Train Loss: 667.7522\n",
            "Epoch 137/200, Validation Loss: 558.7179\n",
            "Epoch 138/200, Train Loss: 588.0768\n",
            "Epoch 138/200, Validation Loss: 539.3531\n",
            "Epoch 139/200, Train Loss: 581.9952\n",
            "Epoch 139/200, Validation Loss: 536.4959\n",
            "Epoch 140/200, Train Loss: 583.4550\n",
            "Epoch 140/200, Validation Loss: 534.7732\n",
            "Epoch 141/200, Train Loss: 998.9878\n",
            "Epoch 141/200, Validation Loss: 991.3827\n",
            "Epoch 142/200, Train Loss: 865.9298\n",
            "Epoch 142/200, Validation Loss: 551.0063\n",
            "Epoch 143/200, Train Loss: 624.5482\n",
            "Epoch 143/200, Validation Loss: 549.1638\n",
            "Epoch 144/200, Train Loss: 590.9477\n",
            "Epoch 144/200, Validation Loss: 539.2654\n",
            "Epoch 145/200, Train Loss: 590.2484\n",
            "Epoch 145/200, Validation Loss: 538.8207\n",
            "Epoch 146/200, Train Loss: 581.4385\n",
            "Epoch 146/200, Validation Loss: 547.1217\n",
            "Epoch 147/200, Train Loss: 597.1996\n",
            "Epoch 147/200, Validation Loss: 538.9635\n",
            "Epoch 148/200, Train Loss: 586.1055\n",
            "Epoch 148/200, Validation Loss: 545.9149\n",
            "Epoch 149/200, Train Loss: 583.5998\n",
            "Epoch 149/200, Validation Loss: 553.9623\n",
            "Epoch 150/200, Train Loss: 587.8046\n",
            "Epoch 150/200, Validation Loss: 547.4088\n",
            "Epoch 151/200, Train Loss: 584.6895\n",
            "Epoch 151/200, Validation Loss: 547.0833\n",
            "Epoch 152/200, Train Loss: 584.2601\n",
            "Epoch 152/200, Validation Loss: 541.3401\n",
            "Epoch 153/200, Train Loss: 579.8456\n",
            "Epoch 153/200, Validation Loss: 540.3122\n",
            "Epoch 154/200, Train Loss: 581.8489\n",
            "Epoch 154/200, Validation Loss: 546.2624\n",
            "Epoch 155/200, Train Loss: 579.7612\n",
            "Epoch 155/200, Validation Loss: 539.7582\n",
            "Epoch 156/200, Train Loss: 584.6001\n",
            "Epoch 156/200, Validation Loss: 538.2089\n",
            "Epoch 157/200, Train Loss: 582.8040\n",
            "Epoch 157/200, Validation Loss: 540.1833\n",
            "Epoch 158/200, Train Loss: 581.3470\n",
            "Epoch 158/200, Validation Loss: 538.1719\n",
            "Epoch 159/200, Train Loss: 579.9406\n",
            "Epoch 159/200, Validation Loss: 539.5757\n",
            "Epoch 160/200, Train Loss: 582.8413\n",
            "Epoch 160/200, Validation Loss: 542.6559\n",
            "Epoch 161/200, Train Loss: 582.5791\n",
            "Epoch 161/200, Validation Loss: 538.1795\n",
            "Epoch 162/200, Train Loss: 578.2912\n",
            "Epoch 162/200, Validation Loss: 538.8716\n",
            "Epoch 163/200, Train Loss: 580.5278\n",
            "Epoch 163/200, Validation Loss: 538.9396\n",
            "Epoch 164/200, Train Loss: 579.4699\n",
            "Epoch 164/200, Validation Loss: 541.9069\n",
            "Epoch 165/200, Train Loss: 581.0726\n",
            "Epoch 165/200, Validation Loss: 537.0800\n",
            "Epoch 166/200, Train Loss: 587.5401\n",
            "Epoch 166/200, Validation Loss: 536.9124\n",
            "Epoch 167/200, Train Loss: 585.2003\n",
            "Epoch 167/200, Validation Loss: 537.2911\n",
            "Epoch 168/200, Train Loss: 579.0230\n",
            "Epoch 168/200, Validation Loss: 537.3464\n",
            "Epoch 169/200, Train Loss: 579.2378\n",
            "Epoch 169/200, Validation Loss: 540.3349\n",
            "Epoch 170/200, Train Loss: 584.3917\n",
            "Epoch 170/200, Validation Loss: 536.6646\n",
            "Epoch 171/200, Train Loss: 582.9897\n",
            "Epoch 171/200, Validation Loss: 538.2572\n",
            "Epoch 172/200, Train Loss: 578.8111\n",
            "Epoch 172/200, Validation Loss: 537.5918\n",
            "Epoch 173/200, Train Loss: 587.8735\n",
            "Epoch 173/200, Validation Loss: 536.1080\n",
            "Epoch 174/200, Train Loss: 581.4694\n",
            "Epoch 174/200, Validation Loss: 537.5289\n",
            "Epoch 175/200, Train Loss: 584.7874\n",
            "Epoch 175/200, Validation Loss: 536.5885\n",
            "Epoch 176/200, Train Loss: 578.4442\n",
            "Epoch 176/200, Validation Loss: 536.4649\n",
            "Epoch 177/200, Train Loss: 578.9897\n",
            "Epoch 177/200, Validation Loss: 537.7175\n",
            "Epoch 178/200, Train Loss: 583.9488\n",
            "Epoch 178/200, Validation Loss: 535.9878\n",
            "Epoch 179/200, Train Loss: 577.9231\n",
            "Epoch 179/200, Validation Loss: 539.2422\n",
            "Epoch 180/200, Train Loss: 585.9876\n",
            "Epoch 180/200, Validation Loss: 536.4206\n",
            "Epoch 181/200, Train Loss: 588.6975\n",
            "Epoch 181/200, Validation Loss: 535.9256\n",
            "Epoch 182/200, Train Loss: 585.8845\n",
            "Epoch 182/200, Validation Loss: 537.4062\n",
            "Epoch 183/200, Train Loss: 587.4775\n",
            "Epoch 183/200, Validation Loss: 539.8412\n",
            "Epoch 184/200, Train Loss: 581.7462\n",
            "Epoch 184/200, Validation Loss: 538.3103\n",
            "Epoch 185/200, Train Loss: 583.3026\n",
            "Epoch 185/200, Validation Loss: 537.4847\n",
            "Epoch 186/200, Train Loss: 582.0508\n",
            "Epoch 186/200, Validation Loss: 535.9063\n",
            "Epoch 187/200, Train Loss: 583.4160\n",
            "Epoch 187/200, Validation Loss: 537.2197\n",
            "Epoch 188/200, Train Loss: 579.1957\n",
            "Epoch 188/200, Validation Loss: 537.0141\n",
            "Epoch 189/200, Train Loss: 574.5008\n",
            "Epoch 189/200, Validation Loss: 536.0471\n",
            "Epoch 190/200, Train Loss: 587.5274\n",
            "Epoch 190/200, Validation Loss: 539.0422\n",
            "Epoch 191/200, Train Loss: 590.8684\n",
            "Epoch 191/200, Validation Loss: 543.4534\n",
            "Epoch 192/200, Train Loss: 586.8255\n",
            "Epoch 192/200, Validation Loss: 538.1929\n",
            "Epoch 193/200, Train Loss: 581.9873\n",
            "Epoch 193/200, Validation Loss: 536.2260\n",
            "Epoch 194/200, Train Loss: 581.7057\n",
            "Epoch 194/200, Validation Loss: 549.3864\n",
            "Epoch 195/200, Train Loss: 576.9948\n",
            "Epoch 195/200, Validation Loss: 540.1711\n",
            "Epoch 196/200, Train Loss: 577.6340\n",
            "Epoch 196/200, Validation Loss: 549.0989\n",
            "Epoch 197/200, Train Loss: 585.4781\n",
            "Epoch 197/200, Validation Loss: 536.4447\n",
            "Epoch 198/200, Train Loss: 584.6392\n",
            "Epoch 198/200, Validation Loss: 536.6619\n",
            "Epoch 199/200, Train Loss: 582.3260\n",
            "Epoch 199/200, Validation Loss: 542.2816\n",
            "Epoch 200/200, Train Loss: 577.2256\n",
            "Epoch 200/200, Validation Loss: 537.6567\n"
          ]
        }
      ],
      "source": [
        "#entrenamos la red con la implementacion del pytorch\n",
        "#cargamos los datos1\n",
        "#num_samples=8000\n",
        "#x_train,y_train=get_dataset(num_samples)\n",
        "#x_train=np.expand_dims(x_train,-1)\n",
        "\n",
        "#probamos con los datos2\n",
        "\n",
        "num_samples=1000\n",
        "input_dim=1\n",
        "len_seq=3\n",
        "x_train=np.random.randint(0,100,(num_samples,len_seq))\n",
        "y_train=x_train[:,::-1].copy()\n",
        "\n",
        "x_train,y_train=np.expand_dims(x_train,-1),np.expand_dims(y_train,-1)\n",
        "\n",
        "x_train,y_train=torch.from_numpy(x_train).float() ,torch.from_numpy(y_train).float()\n",
        "\n",
        "\n",
        "#instanciamos el modelo\n",
        "d_k=64\n",
        "gc.collect()\n",
        "try:\n",
        "    del model\n",
        "except:\n",
        "    pass\n",
        "model = CustomNetwork2(input_dim, d_k,input_dim,num_heads=4)\n",
        "\n",
        "#creamos la funcion perdida y el optimizador\n",
        "loss_criterion=nn.MSELoss()\n",
        "optimizer=optim.Adam(model.parameters(),lr=0.01)\n",
        "#scheduler = ExponentialLR(optimizer, 0.95)\n",
        "\n",
        "\n",
        "#creamos los datasets y los dataloaders\n",
        "train_size = int(0.8 * num_samples)\n",
        "val_size = num_samples - train_size\n",
        "train_dataset, val_dataset = random_split(TensorDataset(x_train,y_train), [train_size, val_size])\n",
        "\n",
        "# Crear DataLoaders para entrenamiento y validación\n",
        "batch_size=64\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "epocas=200\n",
        "\n",
        "\n",
        "#hacemos un bucle para entrenamiento\n",
        "for epoca in range(epocas):\n",
        "    #establecemos el modelo en modo entrenamiento\n",
        "    model.train()\n",
        "\n",
        "    #establecemos en 0 el contador de la funcion de perdida\n",
        "    train_loss=0\n",
        "\n",
        "    #bucle para recorrer todos los batches\n",
        "    for batch_x_train,batch_y_train in train_dataloader:\n",
        "        #ponemos a zero los gradientes para el optimizador\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #predecimos\n",
        "\n",
        "        output = model(batch_x_train)\n",
        "\n",
        "        #calculamos la perdida\n",
        "        loss = loss_criterion(output,batch_y_train)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        #retropropagamos/calculamso lso gradientes\n",
        "        loss.backward()\n",
        "        #y modificamos los pesos\n",
        "        optimizer.step()\n",
        "\n",
        "    train_loss /= len(train_dataloader)\n",
        "    print(f\"Epoch {epoca + 1}/{epocas}, Train Loss: {train_loss:.4f}\")\n",
        "\n",
        "\n",
        "    # modo validacion, \n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_x_val,batch_y_val in val_dataloader:\n",
        "            outputs = model(batch_x_val)\n",
        "            loss = loss_criterion(outputs, batch_y_val)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_loss /= len(val_dataloader)\n",
        "    print(f\"Epoch {epoca + 1}/{epocas}, Validation Loss: {val_loss:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FX2Kap6fy3Un"
      },
      "source": [
        "### Pruebas con MultiHeadAttention layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6EeVaKjy3Un",
        "outputId": "7946138e-21a2-4279-e237-fdfe57ba89f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[83.],\n",
            "         [76.],\n",
            "         [91.]]])\n",
            "tensor([[81.9744],\n",
            "        [81.9744],\n",
            "        [81.9744]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[[[0., 1., 0.],\n",
            "          [0., 1., 0.],\n",
            "          [0., 1., 0.]],\n",
            "\n",
            "         [[0., 0., 1.],\n",
            "          [0., 0., 1.],\n",
            "          [0., 0., 1.]],\n",
            "\n",
            "         [[0., 1., 0.],\n",
            "          [0., 1., 0.],\n",
            "          [0., 1., 0.]],\n",
            "\n",
            "         [[0., 1., 0.],\n",
            "          [0., 1., 0.],\n",
            "          [0., 1., 0.]]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "x=torch.randint(0,100,(1,len_seq,1)).float()\n",
        "print(x)\n",
        "y,att=model(x,return_attention=True,average_attn_weights=False)\n",
        "print(y)\n",
        "print(att)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvKtzSxxy3Un"
      },
      "outputs": [],
      "source": [
        "len_seq=10\n",
        "d_k=20\n",
        "input_dim=1\n",
        "batch_dim=32\n",
        "x=torch.randn(batch_dim,len_seq,d_k)\n",
        "Q=torch.randn(batch_dim,len_seq,d_k)\n",
        "K=torch.randn(batch_dim,len_seq,d_k)\n",
        "V=torch.randn(batch_dim,len_seq,d_k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1m4lkbnYy3Uo"
      },
      "outputs": [],
      "source": [
        "attention = nn.MultiheadAttention(d_k, num_heads=1,batch_first=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kmdgvkay3Uo",
        "outputId": "371a551e-6ffe-4b6d-889e-6477d2d2fedd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 10, 20]) torch.Size([32, 10, 10])\n"
          ]
        }
      ],
      "source": [
        "value,attn=attention(x,x,x)\n",
        "print(value.shape,attn.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBOtIRpEy3Uo"
      },
      "source": [
        "### Entrenar CustomNetwork3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bw7Z-wBy3Uo",
        "outputId": "566b061a-4226-425b-adaa-5091a9c59865"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200, Train Loss: 1269.6548\n",
            "Epoch 1/200, Validation Loss: 827.7826\n",
            "Epoch 2/200, Train Loss: 690.1705\n",
            "Epoch 2/200, Validation Loss: 591.2638\n",
            "Epoch 3/200, Train Loss: 603.2746\n",
            "Epoch 3/200, Validation Loss: 551.4520\n",
            "Epoch 4/200, Train Loss: 583.5020\n",
            "Epoch 4/200, Validation Loss: 544.1393\n",
            "Epoch 5/200, Train Loss: 578.4459\n",
            "Epoch 5/200, Validation Loss: 543.6025\n",
            "Epoch 6/200, Train Loss: 583.2575\n",
            "Epoch 6/200, Validation Loss: 544.2557\n",
            "Epoch 7/200, Train Loss: 576.2405\n",
            "Epoch 7/200, Validation Loss: 543.6211\n",
            "Epoch 8/200, Train Loss: 579.1571\n",
            "Epoch 8/200, Validation Loss: 543.1811\n",
            "Epoch 9/200, Train Loss: 583.4442\n",
            "Epoch 9/200, Validation Loss: 543.7884\n",
            "Epoch 10/200, Train Loss: 578.9010\n",
            "Epoch 10/200, Validation Loss: 542.9674\n",
            "Epoch 11/200, Train Loss: 584.4840\n",
            "Epoch 11/200, Validation Loss: 543.6751\n",
            "Epoch 12/200, Train Loss: 576.3864\n",
            "Epoch 12/200, Validation Loss: 543.5571\n",
            "Epoch 13/200, Train Loss: 580.1746\n",
            "Epoch 13/200, Validation Loss: 543.4741\n",
            "Epoch 14/200, Train Loss: 579.7872\n",
            "Epoch 14/200, Validation Loss: 542.8926\n",
            "Epoch 15/200, Train Loss: 578.8411\n",
            "Epoch 15/200, Validation Loss: 544.1615\n",
            "Epoch 16/200, Train Loss: 580.5633\n",
            "Epoch 16/200, Validation Loss: 543.3157\n",
            "Epoch 17/200, Train Loss: 578.6910\n",
            "Epoch 17/200, Validation Loss: 551.5906\n",
            "Epoch 18/200, Train Loss: 584.1883\n",
            "Epoch 18/200, Validation Loss: 545.3572\n",
            "Epoch 19/200, Train Loss: 583.4569\n",
            "Epoch 19/200, Validation Loss: 547.6174\n",
            "Epoch 20/200, Train Loss: 577.5780\n",
            "Epoch 20/200, Validation Loss: 543.5724\n",
            "Epoch 21/200, Train Loss: 578.2115\n",
            "Epoch 21/200, Validation Loss: 543.7724\n",
            "Epoch 22/200, Train Loss: 582.2906\n",
            "Epoch 22/200, Validation Loss: 544.1279\n",
            "Epoch 23/200, Train Loss: 582.2319\n",
            "Epoch 23/200, Validation Loss: 543.0229\n",
            "Epoch 24/200, Train Loss: 575.8104\n",
            "Epoch 24/200, Validation Loss: 546.5043\n",
            "Epoch 25/200, Train Loss: 580.2192\n",
            "Epoch 25/200, Validation Loss: 542.8818\n",
            "Epoch 26/200, Train Loss: 583.1838\n",
            "Epoch 26/200, Validation Loss: 542.7952\n",
            "Epoch 27/200, Train Loss: 585.4987\n",
            "Epoch 27/200, Validation Loss: 548.7592\n",
            "Epoch 28/200, Train Loss: 581.9327\n",
            "Epoch 28/200, Validation Loss: 543.5545\n",
            "Epoch 29/200, Train Loss: 582.6871\n",
            "Epoch 29/200, Validation Loss: 542.8980\n",
            "Epoch 30/200, Train Loss: 579.8279\n",
            "Epoch 30/200, Validation Loss: 549.4287\n",
            "Epoch 31/200, Train Loss: 579.7085\n",
            "Epoch 31/200, Validation Loss: 542.8983\n",
            "Epoch 32/200, Train Loss: 584.4316\n",
            "Epoch 32/200, Validation Loss: 542.8171\n",
            "Epoch 33/200, Train Loss: 588.5725\n",
            "Epoch 33/200, Validation Loss: 551.5660\n",
            "Epoch 34/200, Train Loss: 587.2513\n",
            "Epoch 34/200, Validation Loss: 542.8467\n",
            "Epoch 35/200, Train Loss: 575.7948\n",
            "Epoch 35/200, Validation Loss: 543.4634\n",
            "Epoch 36/200, Train Loss: 581.0138\n",
            "Epoch 36/200, Validation Loss: 550.9015\n",
            "Epoch 37/200, Train Loss: 583.0113\n",
            "Epoch 37/200, Validation Loss: 546.6368\n",
            "Epoch 38/200, Train Loss: 583.0421\n",
            "Epoch 38/200, Validation Loss: 543.1717\n",
            "Epoch 39/200, Train Loss: 578.4526\n",
            "Epoch 39/200, Validation Loss: 543.3886\n",
            "Epoch 40/200, Train Loss: 578.3658\n",
            "Epoch 40/200, Validation Loss: 543.9696\n",
            "Epoch 41/200, Train Loss: 581.1905\n",
            "Epoch 41/200, Validation Loss: 545.4789\n",
            "Epoch 42/200, Train Loss: 575.7482\n",
            "Epoch 42/200, Validation Loss: 545.7023\n",
            "Epoch 43/200, Train Loss: 585.4013\n",
            "Epoch 43/200, Validation Loss: 542.7956\n",
            "Epoch 44/200, Train Loss: 581.9851\n",
            "Epoch 44/200, Validation Loss: 544.8129\n",
            "Epoch 45/200, Train Loss: 584.1323\n",
            "Epoch 45/200, Validation Loss: 547.9293\n",
            "Epoch 46/200, Train Loss: 581.1521\n",
            "Epoch 46/200, Validation Loss: 542.8709\n",
            "Epoch 47/200, Train Loss: 576.1644\n",
            "Epoch 47/200, Validation Loss: 544.6155\n",
            "Epoch 48/200, Train Loss: 573.6262\n",
            "Epoch 48/200, Validation Loss: 542.7734\n",
            "Epoch 49/200, Train Loss: 584.0684\n",
            "Epoch 49/200, Validation Loss: 545.0820\n",
            "Epoch 50/200, Train Loss: 575.6539\n",
            "Epoch 50/200, Validation Loss: 547.3451\n",
            "Epoch 51/200, Train Loss: 584.1863\n",
            "Epoch 51/200, Validation Loss: 545.7528\n",
            "Epoch 52/200, Train Loss: 579.9071\n",
            "Epoch 52/200, Validation Loss: 543.1145\n",
            "Epoch 53/200, Train Loss: 584.4692\n",
            "Epoch 53/200, Validation Loss: 542.8752\n",
            "Epoch 54/200, Train Loss: 578.4827\n",
            "Epoch 54/200, Validation Loss: 547.5409\n",
            "Epoch 55/200, Train Loss: 576.9634\n",
            "Epoch 55/200, Validation Loss: 542.8146\n",
            "Epoch 56/200, Train Loss: 577.7558\n",
            "Epoch 56/200, Validation Loss: 542.9949\n",
            "Epoch 57/200, Train Loss: 576.0383\n",
            "Epoch 57/200, Validation Loss: 544.9554\n",
            "Epoch 58/200, Train Loss: 583.8191\n",
            "Epoch 58/200, Validation Loss: 543.3774\n",
            "Epoch 59/200, Train Loss: 581.0669\n",
            "Epoch 59/200, Validation Loss: 543.0810\n",
            "Epoch 60/200, Train Loss: 578.9286\n",
            "Epoch 60/200, Validation Loss: 543.4136\n",
            "Epoch 61/200, Train Loss: 577.7537\n",
            "Epoch 61/200, Validation Loss: 550.7969\n",
            "Epoch 62/200, Train Loss: 578.8256\n",
            "Epoch 62/200, Validation Loss: 542.9711\n",
            "Epoch 63/200, Train Loss: 577.5918\n",
            "Epoch 63/200, Validation Loss: 548.0682\n",
            "Epoch 64/200, Train Loss: 579.8245\n",
            "Epoch 64/200, Validation Loss: 543.4527\n",
            "Epoch 65/200, Train Loss: 577.8527\n",
            "Epoch 65/200, Validation Loss: 542.8661\n",
            "Epoch 66/200, Train Loss: 574.3478\n",
            "Epoch 66/200, Validation Loss: 542.7921\n",
            "Epoch 67/200, Train Loss: 582.5625\n",
            "Epoch 67/200, Validation Loss: 542.8162\n",
            "Epoch 68/200, Train Loss: 585.3384\n",
            "Epoch 68/200, Validation Loss: 558.7833\n",
            "Epoch 69/200, Train Loss: 593.4626\n",
            "Epoch 69/200, Validation Loss: 543.2170\n",
            "Epoch 70/200, Train Loss: 578.6983\n",
            "Epoch 70/200, Validation Loss: 543.2184\n",
            "Epoch 71/200, Train Loss: 578.3830\n",
            "Epoch 71/200, Validation Loss: 544.4512\n",
            "Epoch 72/200, Train Loss: 579.6529\n",
            "Epoch 72/200, Validation Loss: 546.4311\n",
            "Epoch 73/200, Train Loss: 577.8104\n",
            "Epoch 73/200, Validation Loss: 545.0667\n",
            "Epoch 74/200, Train Loss: 581.8997\n",
            "Epoch 74/200, Validation Loss: 543.7843\n",
            "Epoch 75/200, Train Loss: 584.7404\n",
            "Epoch 75/200, Validation Loss: 546.0040\n",
            "Epoch 76/200, Train Loss: 581.6643\n",
            "Epoch 76/200, Validation Loss: 544.0206\n",
            "Epoch 77/200, Train Loss: 582.9134\n",
            "Epoch 77/200, Validation Loss: 544.1314\n",
            "Epoch 78/200, Train Loss: 580.4267\n",
            "Epoch 78/200, Validation Loss: 542.9445\n",
            "Epoch 79/200, Train Loss: 579.6157\n",
            "Epoch 79/200, Validation Loss: 545.1900\n",
            "Epoch 80/200, Train Loss: 580.9663\n",
            "Epoch 80/200, Validation Loss: 547.1248\n",
            "Epoch 81/200, Train Loss: 585.2104\n",
            "Epoch 81/200, Validation Loss: 543.1420\n",
            "Epoch 82/200, Train Loss: 577.6990\n",
            "Epoch 82/200, Validation Loss: 547.1072\n",
            "Epoch 83/200, Train Loss: 576.3911\n",
            "Epoch 83/200, Validation Loss: 546.6592\n",
            "Epoch 84/200, Train Loss: 572.5326\n",
            "Epoch 84/200, Validation Loss: 543.5167\n",
            "Epoch 85/200, Train Loss: 582.3926\n",
            "Epoch 85/200, Validation Loss: 543.4440\n",
            "Epoch 86/200, Train Loss: 574.3113\n",
            "Epoch 86/200, Validation Loss: 544.4668\n",
            "Epoch 87/200, Train Loss: 585.5372\n",
            "Epoch 87/200, Validation Loss: 546.8290\n",
            "Epoch 88/200, Train Loss: 576.8444\n",
            "Epoch 88/200, Validation Loss: 546.9509\n",
            "Epoch 89/200, Train Loss: 584.0692\n",
            "Epoch 89/200, Validation Loss: 542.7866\n",
            "Epoch 90/200, Train Loss: 579.0737\n",
            "Epoch 90/200, Validation Loss: 545.1828\n",
            "Epoch 91/200, Train Loss: 579.1609\n",
            "Epoch 91/200, Validation Loss: 542.8893\n",
            "Epoch 92/200, Train Loss: 577.0633\n",
            "Epoch 92/200, Validation Loss: 543.0034\n",
            "Epoch 93/200, Train Loss: 581.7042\n",
            "Epoch 93/200, Validation Loss: 543.2418\n",
            "Epoch 94/200, Train Loss: 578.9540\n",
            "Epoch 94/200, Validation Loss: 543.0906\n",
            "Epoch 95/200, Train Loss: 583.5742\n",
            "Epoch 95/200, Validation Loss: 543.1818\n",
            "Epoch 96/200, Train Loss: 577.8376\n",
            "Epoch 96/200, Validation Loss: 557.0134\n",
            "Epoch 97/200, Train Loss: 580.9068\n",
            "Epoch 97/200, Validation Loss: 547.2009\n",
            "Epoch 98/200, Train Loss: 584.0606\n",
            "Epoch 98/200, Validation Loss: 545.2334\n",
            "Epoch 99/200, Train Loss: 586.8819\n",
            "Epoch 99/200, Validation Loss: 545.4940\n",
            "Epoch 100/200, Train Loss: 584.9911\n",
            "Epoch 100/200, Validation Loss: 543.4484\n",
            "Epoch 101/200, Train Loss: 582.5747\n",
            "Epoch 101/200, Validation Loss: 543.3916\n",
            "Epoch 102/200, Train Loss: 580.9248\n",
            "Epoch 102/200, Validation Loss: 546.0315\n",
            "Epoch 103/200, Train Loss: 581.9674\n",
            "Epoch 103/200, Validation Loss: 544.4579\n",
            "Epoch 104/200, Train Loss: 580.9114\n",
            "Epoch 104/200, Validation Loss: 542.9943\n",
            "Epoch 105/200, Train Loss: 575.0777\n",
            "Epoch 105/200, Validation Loss: 542.9191\n",
            "Epoch 106/200, Train Loss: 581.8116\n",
            "Epoch 106/200, Validation Loss: 544.3394\n",
            "Epoch 107/200, Train Loss: 581.4236\n",
            "Epoch 107/200, Validation Loss: 545.2121\n",
            "Epoch 108/200, Train Loss: 577.9426\n",
            "Epoch 108/200, Validation Loss: 543.2860\n",
            "Epoch 109/200, Train Loss: 574.2456\n",
            "Epoch 109/200, Validation Loss: 543.9129\n",
            "Epoch 110/200, Train Loss: 580.7052\n",
            "Epoch 110/200, Validation Loss: 542.8530\n",
            "Epoch 111/200, Train Loss: 578.3534\n",
            "Epoch 111/200, Validation Loss: 543.3682\n",
            "Epoch 112/200, Train Loss: 574.2288\n",
            "Epoch 112/200, Validation Loss: 542.7854\n",
            "Epoch 113/200, Train Loss: 584.4099\n",
            "Epoch 113/200, Validation Loss: 542.7445\n",
            "Epoch 114/200, Train Loss: 587.6157\n",
            "Epoch 114/200, Validation Loss: 542.8850\n",
            "Epoch 115/200, Train Loss: 580.8248\n",
            "Epoch 115/200, Validation Loss: 544.0655\n",
            "Epoch 116/200, Train Loss: 580.1290\n",
            "Epoch 116/200, Validation Loss: 549.7298\n",
            "Epoch 117/200, Train Loss: 579.3246\n",
            "Epoch 117/200, Validation Loss: 543.1978\n",
            "Epoch 118/200, Train Loss: 580.0933\n",
            "Epoch 118/200, Validation Loss: 543.5704\n",
            "Epoch 119/200, Train Loss: 585.3532\n",
            "Epoch 119/200, Validation Loss: 542.7948\n",
            "Epoch 120/200, Train Loss: 581.4664\n",
            "Epoch 120/200, Validation Loss: 543.3816\n",
            "Epoch 121/200, Train Loss: 584.4744\n",
            "Epoch 121/200, Validation Loss: 545.3594\n",
            "Epoch 122/200, Train Loss: 578.3739\n",
            "Epoch 122/200, Validation Loss: 542.6939\n",
            "Epoch 123/200, Train Loss: 583.0725\n",
            "Epoch 123/200, Validation Loss: 542.9726\n",
            "Epoch 124/200, Train Loss: 582.0477\n",
            "Epoch 124/200, Validation Loss: 543.0483\n",
            "Epoch 125/200, Train Loss: 588.8153\n",
            "Epoch 125/200, Validation Loss: 551.1947\n",
            "Epoch 126/200, Train Loss: 590.1735\n",
            "Epoch 126/200, Validation Loss: 543.3360\n",
            "Epoch 127/200, Train Loss: 584.6514\n",
            "Epoch 127/200, Validation Loss: 542.8264\n",
            "Epoch 128/200, Train Loss: 582.2574\n",
            "Epoch 128/200, Validation Loss: 547.3097\n",
            "Epoch 129/200, Train Loss: 583.2691\n",
            "Epoch 129/200, Validation Loss: 545.5636\n",
            "Epoch 130/200, Train Loss: 573.9930\n",
            "Epoch 130/200, Validation Loss: 543.5884\n",
            "Epoch 131/200, Train Loss: 583.5775\n",
            "Epoch 131/200, Validation Loss: 543.3160\n",
            "Epoch 132/200, Train Loss: 583.4353\n",
            "Epoch 132/200, Validation Loss: 542.8274\n",
            "Epoch 133/200, Train Loss: 579.2640\n",
            "Epoch 133/200, Validation Loss: 543.7381\n",
            "Epoch 134/200, Train Loss: 584.0064\n",
            "Epoch 134/200, Validation Loss: 557.2688\n",
            "Epoch 135/200, Train Loss: 585.4551\n",
            "Epoch 135/200, Validation Loss: 557.1122\n",
            "Epoch 136/200, Train Loss: 585.0350\n",
            "Epoch 136/200, Validation Loss: 544.1207\n",
            "Epoch 137/200, Train Loss: 579.5586\n",
            "Epoch 137/200, Validation Loss: 549.9611\n",
            "Epoch 138/200, Train Loss: 583.9009\n",
            "Epoch 138/200, Validation Loss: 546.9149\n",
            "Epoch 139/200, Train Loss: 583.2300\n",
            "Epoch 139/200, Validation Loss: 545.4446\n",
            "Epoch 140/200, Train Loss: 582.1952\n",
            "Epoch 140/200, Validation Loss: 544.6759\n",
            "Epoch 141/200, Train Loss: 580.9753\n",
            "Epoch 141/200, Validation Loss: 543.8308\n",
            "Epoch 142/200, Train Loss: 586.3785\n",
            "Epoch 142/200, Validation Loss: 545.7959\n",
            "Epoch 143/200, Train Loss: 582.2655\n",
            "Epoch 143/200, Validation Loss: 545.1024\n",
            "Epoch 144/200, Train Loss: 581.9214\n",
            "Epoch 144/200, Validation Loss: 544.0895\n",
            "Epoch 145/200, Train Loss: 580.4889\n",
            "Epoch 145/200, Validation Loss: 543.1321\n",
            "Epoch 146/200, Train Loss: 574.1938\n",
            "Epoch 146/200, Validation Loss: 543.8044\n",
            "Epoch 147/200, Train Loss: 577.0674\n",
            "Epoch 147/200, Validation Loss: 545.2973\n",
            "Epoch 148/200, Train Loss: 580.1150\n",
            "Epoch 148/200, Validation Loss: 543.8626\n",
            "Epoch 149/200, Train Loss: 580.8105\n",
            "Epoch 149/200, Validation Loss: 543.1466\n",
            "Epoch 150/200, Train Loss: 582.1659\n",
            "Epoch 150/200, Validation Loss: 543.0153\n",
            "Epoch 151/200, Train Loss: 575.7799\n",
            "Epoch 151/200, Validation Loss: 544.3330\n",
            "Epoch 152/200, Train Loss: 581.5591\n",
            "Epoch 152/200, Validation Loss: 546.8086\n",
            "Epoch 153/200, Train Loss: 581.1593\n",
            "Epoch 153/200, Validation Loss: 544.9029\n",
            "Epoch 154/200, Train Loss: 581.2597\n",
            "Epoch 154/200, Validation Loss: 543.3697\n",
            "Epoch 155/200, Train Loss: 581.8323\n",
            "Epoch 155/200, Validation Loss: 543.3754\n",
            "Epoch 156/200, Train Loss: 577.7689\n",
            "Epoch 156/200, Validation Loss: 542.9325\n",
            "Epoch 157/200, Train Loss: 581.8956\n",
            "Epoch 157/200, Validation Loss: 542.8468\n",
            "Epoch 158/200, Train Loss: 584.6272\n",
            "Epoch 158/200, Validation Loss: 546.2325\n",
            "Epoch 159/200, Train Loss: 581.9827\n",
            "Epoch 159/200, Validation Loss: 543.0651\n",
            "Epoch 160/200, Train Loss: 579.7627\n",
            "Epoch 160/200, Validation Loss: 542.7953\n",
            "Epoch 161/200, Train Loss: 579.0344\n",
            "Epoch 161/200, Validation Loss: 543.0714\n",
            "Epoch 162/200, Train Loss: 580.3534\n",
            "Epoch 162/200, Validation Loss: 542.7684\n",
            "Epoch 163/200, Train Loss: 580.9607\n",
            "Epoch 163/200, Validation Loss: 545.6575\n",
            "Epoch 164/200, Train Loss: 575.7865\n",
            "Epoch 164/200, Validation Loss: 543.7147\n",
            "Epoch 165/200, Train Loss: 585.1459\n",
            "Epoch 165/200, Validation Loss: 547.7688\n",
            "Epoch 166/200, Train Loss: 583.7722\n",
            "Epoch 166/200, Validation Loss: 542.7708\n",
            "Epoch 167/200, Train Loss: 581.1815\n",
            "Epoch 167/200, Validation Loss: 543.2028\n",
            "Epoch 168/200, Train Loss: 578.4904\n",
            "Epoch 168/200, Validation Loss: 544.0457\n",
            "Epoch 169/200, Train Loss: 578.7452\n",
            "Epoch 169/200, Validation Loss: 543.3446\n",
            "Epoch 170/200, Train Loss: 582.3220\n",
            "Epoch 170/200, Validation Loss: 547.6182\n",
            "Epoch 171/200, Train Loss: 577.8959\n",
            "Epoch 171/200, Validation Loss: 543.2096\n",
            "Epoch 172/200, Train Loss: 584.2389\n",
            "Epoch 172/200, Validation Loss: 543.4306\n",
            "Epoch 173/200, Train Loss: 582.5635\n",
            "Epoch 173/200, Validation Loss: 544.1367\n",
            "Epoch 174/200, Train Loss: 576.9493\n",
            "Epoch 174/200, Validation Loss: 543.9264\n",
            "Epoch 175/200, Train Loss: 580.2240\n",
            "Epoch 175/200, Validation Loss: 543.3703\n",
            "Epoch 176/200, Train Loss: 584.7878\n",
            "Epoch 176/200, Validation Loss: 542.7453\n",
            "Epoch 177/200, Train Loss: 588.9964\n",
            "Epoch 177/200, Validation Loss: 542.8514\n",
            "Epoch 178/200, Train Loss: 584.8035\n",
            "Epoch 178/200, Validation Loss: 544.8018\n",
            "Epoch 179/200, Train Loss: 578.4130\n",
            "Epoch 179/200, Validation Loss: 545.9608\n",
            "Epoch 180/200, Train Loss: 584.2362\n",
            "Epoch 180/200, Validation Loss: 543.0681\n",
            "Epoch 181/200, Train Loss: 576.6212\n",
            "Epoch 181/200, Validation Loss: 543.1523\n",
            "Epoch 182/200, Train Loss: 579.0873\n",
            "Epoch 182/200, Validation Loss: 542.9475\n",
            "Epoch 183/200, Train Loss: 581.7696\n",
            "Epoch 183/200, Validation Loss: 550.5164\n",
            "Epoch 184/200, Train Loss: 584.1372\n",
            "Epoch 184/200, Validation Loss: 548.5318\n",
            "Epoch 185/200, Train Loss: 578.0960\n",
            "Epoch 185/200, Validation Loss: 543.4394\n",
            "Epoch 186/200, Train Loss: 582.2395\n",
            "Epoch 186/200, Validation Loss: 542.7439\n",
            "Epoch 187/200, Train Loss: 580.0322\n",
            "Epoch 187/200, Validation Loss: 543.0411\n",
            "Epoch 188/200, Train Loss: 582.0206\n",
            "Epoch 188/200, Validation Loss: 544.2853\n",
            "Epoch 189/200, Train Loss: 579.9456\n",
            "Epoch 189/200, Validation Loss: 559.1140\n",
            "Epoch 190/200, Train Loss: 587.4812\n",
            "Epoch 190/200, Validation Loss: 544.9487\n",
            "Epoch 191/200, Train Loss: 583.5567\n",
            "Epoch 191/200, Validation Loss: 544.0199\n",
            "Epoch 192/200, Train Loss: 577.5824\n",
            "Epoch 192/200, Validation Loss: 543.3280\n",
            "Epoch 193/200, Train Loss: 577.9838\n",
            "Epoch 193/200, Validation Loss: 545.7798\n",
            "Epoch 194/200, Train Loss: 581.1661\n",
            "Epoch 194/200, Validation Loss: 546.8523\n",
            "Epoch 195/200, Train Loss: 585.8055\n",
            "Epoch 195/200, Validation Loss: 546.5297\n",
            "Epoch 196/200, Train Loss: 582.7473\n",
            "Epoch 196/200, Validation Loss: 552.4955\n",
            "Epoch 197/200, Train Loss: 583.1633\n",
            "Epoch 197/200, Validation Loss: 546.6989\n",
            "Epoch 198/200, Train Loss: 585.5638\n",
            "Epoch 198/200, Validation Loss: 544.5304\n",
            "Epoch 199/200, Train Loss: 583.7081\n",
            "Epoch 199/200, Validation Loss: 545.0541\n",
            "Epoch 200/200, Train Loss: 580.3561\n",
            "Epoch 200/200, Validation Loss: 543.0846\n"
          ]
        }
      ],
      "source": [
        "#entrenamos la red con la implementacion del pytorch\n",
        "#cargamos los datos1\n",
        "#num_samples=8000\n",
        "#x_train,y_train=get_dataset(num_samples)\n",
        "#x_train=np.expand_dims(x_train,-1)\n",
        "\n",
        "#probamos con los datos2\n",
        "\n",
        "num_samples=1000\n",
        "input_dim=1\n",
        "len_seq=3\n",
        "x_train=np.random.randint(0,100,(num_samples,len_seq))\n",
        "y_train=x_train[:,::-1].copy()\n",
        "\n",
        "x_train,y_train=np.expand_dims(x_train,-1),np.expand_dims(y_train,-1)\n",
        "\n",
        "x_train,y_train=torch.from_numpy(x_train).float() ,torch.from_numpy(y_train).float()\n",
        "\n",
        "\n",
        "#instanciamos el modelo\n",
        "d_k=64\n",
        "gc.collect()\n",
        "try:\n",
        "    del model\n",
        "except:\n",
        "    pass\n",
        "model = CustomNetwork3(input_dim, d_k,input_dim,num_heads=4)\n",
        "\n",
        "#creamos la funcion perdida y el optimizador\n",
        "loss_criterion=nn.MSELoss()\n",
        "optimizer=optim.Adam(model.parameters(),lr=0.01)\n",
        "#scheduler = ExponentialLR(optimizer, 0.95)\n",
        "\n",
        "\n",
        "#creamos los datasets y los dataloaders\n",
        "train_size = int(0.8 * num_samples)\n",
        "val_size = num_samples - train_size\n",
        "train_dataset, val_dataset = random_split(TensorDataset(x_train,y_train), [train_size, val_size])\n",
        "\n",
        "# Crear DataLoaders para entrenamiento y validación\n",
        "batch_size=64\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "epocas=200\n",
        "\n",
        "\n",
        "#hacemos un bucle para entrenamiento\n",
        "for epoca in range(epocas):\n",
        "    #establecemos el modelo en modo entrenamiento\n",
        "    model.train()\n",
        "\n",
        "    #establecemos en 0 el contador de la funcion de perdida\n",
        "    train_loss=0\n",
        "\n",
        "    #bucle para recorrer todos los batches\n",
        "    for batch_x_train,batch_y_train in train_dataloader:\n",
        "        #ponemos a zero los gradientes para el optimizador\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #predecimos\n",
        "\n",
        "        output = model(batch_x_train)\n",
        "\n",
        "        #calculamos la perdida\n",
        "        loss = loss_criterion(output,batch_y_train)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        #retropropagamos/calculamso lso gradientes\n",
        "        loss.backward()\n",
        "        #y modificamos los pesos\n",
        "        optimizer.step()\n",
        "\n",
        "    train_loss /= len(train_dataloader)\n",
        "    print(f\"Epoch {epoca + 1}/{epocas}, Train Loss: {train_loss:.4f}\")\n",
        "\n",
        "\n",
        "    # modo validacion, \n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_x_val,batch_y_val in val_dataloader:\n",
        "            outputs = model(batch_x_val)\n",
        "            loss = loss_criterion(outputs, batch_y_val)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_loss /= len(val_dataloader)\n",
        "    print(f\"Epoch {epoca + 1}/{epocas}, Validation Loss: {val_loss:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77zWEaSfy3Up",
        "outputId": "aa93e744-1230-4d52-c2dd-00e2ca1f2445"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[22.],\n",
            "         [57.],\n",
            "         [36.]]])\n",
            "tensor([[[39.5671],\n",
            "         [39.5671],\n",
            "         [39.5671]]], grad_fn=<ViewBackward0>)\n",
            "tensor([[[[1.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "          [1.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "          [1.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
            "\n",
            "         [[1.0000e+00, 1.2935e-32, 1.7567e-13],\n",
            "          [1.0000e+00, 0.0000e+00, 7.9328e-27],\n",
            "          [1.0000e+00, 0.0000e+00, 8.0649e-19]],\n",
            "\n",
            "         [[1.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "          [1.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "          [1.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
            "\n",
            "         [[0.0000e+00, 1.0000e+00, 2.9007e-43],\n",
            "          [0.0000e+00, 1.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 1.0000e+00, 0.0000e+00]]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "x=torch.randint(0,100,(1,len_seq,1)).float()\n",
        "print(x)\n",
        "y,att=model(x,return_attention=True,average_attn_weights=False)\n",
        "print(y)\n",
        "print(att)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrg8zXGyy3Up"
      },
      "source": [
        "### Entrenar CustomNetwork4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLRSkvKPy3Up",
        "outputId": "f41bba8d-4c0c-48aa-d32f-3c4a940ece9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30, Train Loss: 2000.9459\n",
            "Epoch 1/30, Validation Loss: 1126.8336\n",
            "Epoch 2/30, Train Loss: 814.2508\n",
            "Epoch 2/30, Validation Loss: 659.2357\n",
            "Epoch 3/30, Train Loss: 622.1192\n",
            "Epoch 3/30, Validation Loss: 598.9983\n",
            "Epoch 4/30, Train Loss: 596.1869\n",
            "Epoch 4/30, Validation Loss: 585.2652\n",
            "Epoch 5/30, Train Loss: 589.7725\n",
            "Epoch 5/30, Validation Loss: 585.2767\n",
            "Epoch 6/30, Train Loss: 587.5596\n",
            "Epoch 6/30, Validation Loss: 583.2285\n",
            "Epoch 7/30, Train Loss: 586.8403\n",
            "Epoch 7/30, Validation Loss: 583.2864\n",
            "Epoch 8/30, Train Loss: 586.0861\n",
            "Epoch 8/30, Validation Loss: 578.7184\n",
            "Epoch 9/30, Train Loss: 584.9948\n",
            "Epoch 9/30, Validation Loss: 578.2278\n",
            "Epoch 10/30, Train Loss: 585.1269\n",
            "Epoch 10/30, Validation Loss: 579.3866\n",
            "Epoch 11/30, Train Loss: 584.6913\n",
            "Epoch 11/30, Validation Loss: 582.3831\n",
            "Epoch 12/30, Train Loss: 584.7779\n",
            "Epoch 12/30, Validation Loss: 578.2818\n",
            "Epoch 13/30, Train Loss: 584.4605\n",
            "Epoch 13/30, Validation Loss: 578.9738\n",
            "Epoch 14/30, Train Loss: 584.1538\n",
            "Epoch 14/30, Validation Loss: 577.3259\n",
            "Epoch 15/30, Train Loss: 584.2117\n",
            "Epoch 15/30, Validation Loss: 577.0298\n",
            "Epoch 16/30, Train Loss: 584.5960\n",
            "Epoch 16/30, Validation Loss: 582.3690\n",
            "Epoch 17/30, Train Loss: 584.2105\n",
            "Epoch 17/30, Validation Loss: 577.0754\n",
            "Epoch 18/30, Train Loss: 584.0379\n",
            "Epoch 18/30, Validation Loss: 579.4734\n",
            "Epoch 19/30, Train Loss: 583.8626\n",
            "Epoch 19/30, Validation Loss: 578.7465\n",
            "Epoch 20/30, Train Loss: 584.3263\n",
            "Epoch 20/30, Validation Loss: 577.0467\n",
            "Epoch 21/30, Train Loss: 584.0615\n",
            "Epoch 21/30, Validation Loss: 577.1542\n",
            "Epoch 22/30, Train Loss: 584.1428\n",
            "Epoch 22/30, Validation Loss: 577.6174\n",
            "Epoch 23/30, Train Loss: 584.2982\n",
            "Epoch 23/30, Validation Loss: 577.3912\n",
            "Epoch 24/30, Train Loss: 584.3501\n",
            "Epoch 24/30, Validation Loss: 579.8684\n",
            "Epoch 25/30, Train Loss: 583.7644\n",
            "Epoch 25/30, Validation Loss: 576.8439\n",
            "Epoch 26/30, Train Loss: 584.3023\n",
            "Epoch 26/30, Validation Loss: 576.7302\n",
            "Epoch 27/30, Train Loss: 584.4029\n",
            "Epoch 27/30, Validation Loss: 576.9375\n",
            "Epoch 28/30, Train Loss: 583.7087\n",
            "Epoch 28/30, Validation Loss: 577.5865\n",
            "Epoch 29/30, Train Loss: 583.8452\n",
            "Epoch 29/30, Validation Loss: 577.0979\n",
            "Epoch 30/30, Train Loss: 583.9646\n",
            "Epoch 30/30, Validation Loss: 582.3861\n"
          ]
        }
      ],
      "source": [
        "#entrenamos la red con la implementacion del pytorch\n",
        "#cargamos los datos1\n",
        "#num_samples=8000\n",
        "#x_train,y_train=get_dataset(num_samples)\n",
        "#x_train=np.expand_dims(x_train,-1)\n",
        "\n",
        "#probamos con los datos2\n",
        "\n",
        "num_samples=30000\n",
        "input_dim=1\n",
        "len_seq=3\n",
        "x_train=np.random.randint(0,100,(num_samples,len_seq))\n",
        "y_train=x_train[:,::-1].copy()\n",
        "\n",
        "x_train,y_train=np.expand_dims(x_train,-1),np.expand_dims(y_train,-1)\n",
        "\n",
        "x_train,y_train=torch.from_numpy(x_train).float() ,torch.from_numpy(y_train).float()\n",
        "\n",
        "\n",
        "#instanciamos el modelo\n",
        "d_k=120\n",
        "gc.collect()\n",
        "try:\n",
        "    del model\n",
        "except:\n",
        "    pass\n",
        "model = CustomNetwork5(input_dim, d_k,input_dim,num_heads=4)\n",
        "\n",
        "#creamos la funcion perdida y el optimizador\n",
        "loss_criterion=nn.MSELoss()\n",
        "optimizer=optim.Adam(model.parameters(),lr=0.001)\n",
        "#scheduler = ExponentialLR(optimizer, 0.95)\n",
        "\n",
        "\n",
        "#creamos los datasets y los dataloaders\n",
        "train_size = int(0.8 * num_samples)\n",
        "val_size = num_samples - train_size\n",
        "train_dataset, val_dataset = random_split(TensorDataset(x_train,y_train), [train_size, val_size])\n",
        "\n",
        "# Crear DataLoaders para entrenamiento y validación\n",
        "batch_size=128\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "epocas=30\n",
        "\n",
        "\n",
        "#hacemos un bucle para entrenamiento\n",
        "for epoca in range(epocas):\n",
        "    #establecemos el modelo en modo entrenamiento\n",
        "    model.train()\n",
        "\n",
        "    #establecemos en 0 el contador de la funcion de perdida\n",
        "    train_loss=0\n",
        "\n",
        "    #bucle para recorrer todos los batches\n",
        "    for batch_x_train,batch_y_train in train_dataloader:\n",
        "        #ponemos a zero los gradientes para el optimizador\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #predecimos\n",
        "\n",
        "        output = model(batch_x_train)\n",
        "\n",
        "        #calculamos la perdida\n",
        "        loss = loss_criterion(output,batch_y_train)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        #retropropagamos/calculamso lso gradientes\n",
        "        loss.backward()\n",
        "        #y modificamos los pesos\n",
        "        optimizer.step()\n",
        "\n",
        "    train_loss /= len(train_dataloader)\n",
        "    print(f\"Epoch {epoca + 1}/{epocas}, Train Loss: {train_loss:.4f}\")\n",
        "\n",
        "\n",
        "    # modo validacion, \n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_x_val,batch_y_val in val_dataloader:\n",
        "            outputs = model(batch_x_val)\n",
        "            loss = loss_criterion(outputs, batch_y_val)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_loss /= len(val_dataloader)\n",
        "    print(f\"Epoch {epoca + 1}/{epocas}, Validation Loss: {val_loss:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "417BvtPly3Up",
        "outputId": "8dbdc2ac-28a4-4d11-df8e-e982df16d64c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[64.],\n",
            "         [ 4.]]])\n",
            "tensor([[[ 3.9368],\n",
            "         [65.4299]]], grad_fn=<ViewBackward0>)\n",
            "tensor([[[[0.0000e+00, 1.0000e+00],\n",
            "          [0.0000e+00, 1.0000e+00]],\n",
            "\n",
            "         [[1.8551e-28, 1.0000e+00],\n",
            "          [1.0000e+00, 4.5442e-11]],\n",
            "\n",
            "         [[9.9992e-01, 8.4177e-05],\n",
            "          [1.0000e+00, 1.6283e-09]],\n",
            "\n",
            "         [[0.0000e+00, 1.0000e+00],\n",
            "          [1.0000e+00, 2.3903e-08]]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "x=torch.randint(0,100,(1,len_seq,1)).float()\n",
        "print(x)\n",
        "y,att=model(x,return_attention=True,average_attn_weights=False)\n",
        "print(y)\n",
        "print(att)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IG3oTwl0y3Uq"
      },
      "source": [
        "### Custom positional encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUIPfK2Ry3Uq",
        "outputId": "4f43e26f-4d17-49ad-91e5-e8340b68821e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[[20],\n",
              "        [25],\n",
              "        [40]],\n",
              "\n",
              "       [[95],\n",
              "        [24],\n",
              "        [40]],\n",
              "\n",
              "       [[57],\n",
              "        [41],\n",
              "        [58]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[64],\n",
              "        [81],\n",
              "        [83]],\n",
              "\n",
              "       [[19],\n",
              "        [68],\n",
              "        [65]],\n",
              "\n",
              "       [[46],\n",
              "        [66],\n",
              "        [62]]])"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_samples=70000\n",
        "input_dim=2\n",
        "len_seq=3\n",
        "x_train=np.random.randint(0,100,(num_samples,len_seq))\n",
        "\n",
        "y_train=x_train[:,::-1].copy()\n",
        "\n",
        "x_train,y_train=np.expand_dims(x_train,-1),np.expand_dims(y_train,-1)\n",
        "x_train_pos=np.array([np.hstack((np.arange(len_seq)[...,np.newaxis],x)) for x in x_train])\n",
        "x_train=x_train_pos.sum(-1)[...,np.newaxis]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aADJewCNy3Uq",
        "outputId": "b9f00b32-07a5-4faf-eb7b-0ca0ba7d6557"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40, Train Loss: 0.7992\n",
            "Epoch 1/40, Validation Loss: 0.6420\n",
            "Epoch 2/40, Train Loss: 0.6304\n",
            "Epoch 2/40, Validation Loss: 0.6219\n",
            "Epoch 3/40, Train Loss: 0.6182\n",
            "Epoch 3/40, Validation Loss: 0.6143\n",
            "Epoch 4/40, Train Loss: 0.6135\n",
            "Epoch 4/40, Validation Loss: 0.6117\n",
            "Epoch 5/40, Train Loss: 0.6113\n",
            "Epoch 5/40, Validation Loss: 0.6098\n",
            "Epoch 6/40, Train Loss: 0.6100\n",
            "Epoch 6/40, Validation Loss: 0.6088\n",
            "Epoch 7/40, Train Loss: 0.6093\n",
            "Epoch 7/40, Validation Loss: 0.6081\n",
            "Epoch 8/40, Train Loss: 0.6087\n",
            "Epoch 8/40, Validation Loss: 0.6073\n",
            "Epoch 9/40, Train Loss: 0.6081\n",
            "Epoch 9/40, Validation Loss: 0.6066\n",
            "Epoch 10/40, Train Loss: 0.6075\n",
            "Epoch 10/40, Validation Loss: 0.6063\n",
            "Epoch 11/40, Train Loss: 0.6069\n",
            "Epoch 11/40, Validation Loss: 0.6053\n",
            "Epoch 12/40, Train Loss: 0.6058\n",
            "Epoch 12/40, Validation Loss: 0.6040\n",
            "Epoch 13/40, Train Loss: 0.6040\n",
            "Epoch 13/40, Validation Loss: 0.6025\n",
            "Epoch 14/40, Train Loss: 0.6014\n",
            "Epoch 14/40, Validation Loss: 0.5987\n",
            "Epoch 15/40, Train Loss: 0.5993\n",
            "Epoch 15/40, Validation Loss: 0.5975\n",
            "Epoch 16/40, Train Loss: 0.5980\n",
            "Epoch 16/40, Validation Loss: 0.5960\n",
            "Epoch 17/40, Train Loss: 0.5969\n",
            "Epoch 17/40, Validation Loss: 0.5953\n",
            "Epoch 18/40, Train Loss: 0.5957\n",
            "Epoch 18/40, Validation Loss: 0.5927\n",
            "Epoch 19/40, Train Loss: 0.5932\n",
            "Epoch 19/40, Validation Loss: 0.5900\n",
            "Epoch 20/40, Train Loss: 0.5903\n",
            "Epoch 20/40, Validation Loss: 0.5867\n",
            "Epoch 21/40, Train Loss: 0.5871\n",
            "Epoch 21/40, Validation Loss: 0.5833\n",
            "Epoch 22/40, Train Loss: 0.5839\n",
            "Epoch 22/40, Validation Loss: 0.5803\n",
            "Epoch 23/40, Train Loss: 0.5815\n",
            "Epoch 23/40, Validation Loss: 0.5779\n",
            "Epoch 24/40, Train Loss: 0.5797\n",
            "Epoch 24/40, Validation Loss: 0.5762\n",
            "Epoch 25/40, Train Loss: 0.5784\n",
            "Epoch 25/40, Validation Loss: 0.5752\n",
            "Epoch 26/40, Train Loss: 0.5775\n",
            "Epoch 26/40, Validation Loss: 0.5745\n",
            "Epoch 27/40, Train Loss: 0.5768\n",
            "Epoch 27/40, Validation Loss: 0.5739\n",
            "Epoch 28/40, Train Loss: 0.5764\n",
            "Epoch 28/40, Validation Loss: 0.5734\n",
            "Epoch 29/40, Train Loss: 0.5762\n",
            "Epoch 29/40, Validation Loss: 0.5759\n",
            "Epoch 30/40, Train Loss: 0.5756\n",
            "Epoch 30/40, Validation Loss: 0.5729\n",
            "Epoch 31/40, Train Loss: 0.5755\n",
            "Epoch 31/40, Validation Loss: 0.5725\n",
            "Epoch 32/40, Train Loss: 0.5750\n",
            "Epoch 32/40, Validation Loss: 0.5722\n",
            "Epoch 33/40, Train Loss: 0.5751\n",
            "Epoch 33/40, Validation Loss: 0.5744\n",
            "Epoch 34/40, Train Loss: 0.5752\n",
            "Epoch 34/40, Validation Loss: 0.5716\n",
            "Epoch 35/40, Train Loss: 0.5748\n",
            "Epoch 35/40, Validation Loss: 0.5740\n",
            "Epoch 36/40, Train Loss: 0.5746\n",
            "Epoch 36/40, Validation Loss: 0.5724\n",
            "Epoch 37/40, Train Loss: 0.5746\n",
            "Epoch 37/40, Validation Loss: 0.5709\n",
            "Epoch 38/40, Train Loss: 0.5744\n",
            "Epoch 38/40, Validation Loss: 0.5713\n",
            "Epoch 39/40, Train Loss: 0.5744\n",
            "Epoch 39/40, Validation Loss: 0.5730\n",
            "Epoch 40/40, Train Loss: 0.5743\n",
            "Epoch 40/40, Validation Loss: 0.5710\n"
          ]
        }
      ],
      "source": [
        "#entrenamos la red con la implementacion del pytorch\n",
        "#cargamos los datos1\n",
        "#num_samples=8000\n",
        "#x_train,y_train=get_dataset(num_samples)\n",
        "#x_train=np.expand_dims(x_train,-1)\n",
        "\n",
        "#probamos con los datos2\n",
        "\n",
        "num_samples=170000\n",
        "input_dim=1\n",
        "len_seq=3\n",
        "x_train=np.random.randn(num_samples,len_seq)\n",
        "\n",
        "y_train=x_train[:,::-1].copy()\n",
        "\n",
        "x_train,y_train=np.expand_dims(x_train,-1),np.expand_dims(y_train,-1)\n",
        "x_train_pos=np.array([np.hstack((np.arange(len_seq)[...,np.newaxis],x)) for x in x_train])\n",
        "x_train=x_train_pos.sum(-1)[...,np.newaxis]\n",
        "x_train,y_train=torch.from_numpy(x_train).float() ,torch.from_numpy(y_train).float()\n",
        "\n",
        "\n",
        "#instanciamos el modelo\n",
        "d_k=24\n",
        "gc.collect()\n",
        "try:\n",
        "    del model\n",
        "except:\n",
        "    pass\n",
        "model = CustomNetwork5(input_dim, d_k,1,num_heads=4)\n",
        "\n",
        "#creamos la funcion perdida y el optimizador\n",
        "loss_criterion=nn.MSELoss()\n",
        "optimizer=optim.Adam(model.parameters(),lr=0.0001)\n",
        "#scheduler = ExponentialLR(optimizer, 0.95)\n",
        "\n",
        "\n",
        "#creamos los datasets y los dataloaders\n",
        "train_size = int(0.8 * num_samples)\n",
        "val_size = num_samples - train_size\n",
        "train_dataset, val_dataset = random_split(TensorDataset(x_train,y_train), [train_size, val_size])\n",
        "\n",
        "# Crear DataLoaders para entrenamiento y validación\n",
        "batch_size=128\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "epocas=40\n",
        "\n",
        "\n",
        "#hacemos un bucle para entrenamiento\n",
        "for epoca in range(epocas):\n",
        "    #establecemos el modelo en modo entrenamiento\n",
        "    model.train()\n",
        "\n",
        "    #establecemos en 0 el contador de la funcion de perdida\n",
        "    train_loss=0\n",
        "\n",
        "    #bucle para recorrer todos los batches\n",
        "    for batch_x_train,batch_y_train in train_dataloader:\n",
        "        #ponemos a zero los gradientes para el optimizador\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #predecimos\n",
        "\n",
        "        output = model(batch_x_train)\n",
        "\n",
        "        #calculamos la perdida\n",
        "        loss = loss_criterion(output,batch_y_train)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        #retropropagamos/calculamso lso gradientes\n",
        "        loss.backward()\n",
        "        #y modificamos los pesos\n",
        "        optimizer.step()\n",
        "\n",
        "    train_loss /= len(train_dataloader)\n",
        "    print(f\"Epoch {epoca + 1}/{epocas}, Train Loss: {train_loss:.4f}\")\n",
        "\n",
        "\n",
        "    # modo validacion, \n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_x_val,batch_y_val in val_dataloader:\n",
        "            outputs = model(batch_x_val)\n",
        "            loss = loss_criterion(outputs, batch_y_val)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_loss /= len(val_dataloader)\n",
        "    print(f\"Epoch {epoca + 1}/{epocas}, Validation Loss: {val_loss:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJpwRgqZy3Uq",
        "outputId": "4b7ffcfc-8879-4b62-9464-8b5199bda71c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[38.],\n",
              "        [86.],\n",
              "        [85.]])"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x=torch.randint(0,100,(len_seq,1)).float()\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSaQGN1My3Ur",
        "outputId": "e1876a98-ad68-4265-94f0-cb2e21916992"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[ 0., 31.],\n",
            "         [ 1., 84.],\n",
            "         [ 2., 82.]]])\n",
            "tensor([[[84.7435],\n",
            "         [87.5554],\n",
            "         [32.0334]]], grad_fn=<ViewBackward0>)\n",
            "tensor([[[[0.9519, 0.0231, 0.0251],\n",
            "          [0.9113, 0.0509, 0.0377],\n",
            "          [0.6461, 0.1880, 0.1659]],\n",
            "\n",
            "         [[0.0828, 0.2635, 0.6537],\n",
            "          [0.2157, 0.2805, 0.5038],\n",
            "          [0.5074, 0.2192, 0.2734]],\n",
            "\n",
            "         [[0.9866, 0.0069, 0.0066],\n",
            "          [0.9088, 0.0650, 0.0262],\n",
            "          [0.7080, 0.2536, 0.0384]],\n",
            "\n",
            "         [[0.5075, 0.3043, 0.1882],\n",
            "          [0.2268, 0.3108, 0.4624],\n",
            "          [0.0910, 0.1999, 0.7092]]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "len_seq=3\n",
        "x=torch.randint(0,100,(1,len_seq,1)).float()\n",
        "x=np.array([np.hstack((np.arange(len_seq)[...,np.newaxis],i)) for i in x])\n",
        "x=torch.from_numpy(x).float() \n",
        "\n",
        "print(x)\n",
        "y,att=model(x,return_attention=True,average_attn_weights=False)\n",
        "print(y)\n",
        "print(att)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "th1DhMbJy3Ur"
      },
      "source": [
        "## Con positional encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJQqX30Ky3Ur"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CustomNetwork4_withPosEncoding(nn.Module):\n",
        "    def __init__(self, d_input, d_k, output_dim=1, num_heads=1, max_seq_length=200):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.d_input = d_input\n",
        "        self.d_k = d_k\n",
        "        self.dense = nn.Linear(d_input, d_k)\n",
        "        self.attention = nn.MultiheadAttention(d_k, num_heads=num_heads, batch_first=True)\n",
        "        self.norm1 = nn.LayerNorm(d_k)\n",
        "        self.norm2 = nn.LayerNorm(d_k)\n",
        "\n",
        "        self.fc = nn.Linear(d_k, d_k)\n",
        "        self.last_dense = nn.Linear(d_k, output_dim)\n",
        "\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.positional_encoding = self.generate_positional_encoding(self.max_seq_length, self.d_k)\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def generate_positional_encoding(self, max_seq_length, d_k):\n",
        "        positional_encoding = torch.zeros(max_seq_length, d_k)\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(-1)\n",
        "        div_term = torch.exp(torch.arange(0, d_k, 2).float() * -(math.log(10000.0) / d_k))\n",
        "        positional_encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "        positional_encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "        positional_encoding = positional_encoding.unsqueeze(0).transpose(0, 1)\n",
        "        return positional_encoding\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.dense.weight)\n",
        "        nn.init.xavier_uniform_(self.fc.weight)\n",
        "\n",
        "    def forward(self, vector_input, return_attention=False, average_attn_weights=False):\n",
        "        x = self.dense(vector_input)\n",
        "        \n",
        "        # Add positional encoding\n",
        "        x = x + self.positional_encoding[:x.size(0), :]\n",
        "\n",
        "        values, attention = self.attention(x, x, x, average_attn_weights=average_attn_weights)\n",
        "        values = self.norm1(x + values)\n",
        "        output = self.last_dense(values)\n",
        "\n",
        "        if return_attention:\n",
        "            return output, attention\n",
        "        else:\n",
        "            return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KolxrLLBy3Ur",
        "outputId": "e09f0eff-66c4-42ef-9873-5e9a180d3f98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/130, Train Loss: 1.0497\n",
            "Epoch 1/130, Validation Loss: 0.7196\n",
            "Epoch 2/130, Train Loss: 0.7063\n",
            "Epoch 2/130, Validation Loss: 0.6702\n",
            "Epoch 3/130, Train Loss: 0.6837\n",
            "Epoch 3/130, Validation Loss: 0.6632\n",
            "Epoch 4/130, Train Loss: 0.6792\n",
            "Epoch 4/130, Validation Loss: 0.6605\n",
            "Epoch 5/130, Train Loss: 0.6788\n",
            "Epoch 5/130, Validation Loss: 0.6585\n",
            "Epoch 6/130, Train Loss: 0.6755\n",
            "Epoch 6/130, Validation Loss: 0.6619\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 69\u001b[0m\n\u001b[1;32m     66\u001b[0m train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     68\u001b[0m \u001b[39m#retropropagamos/calculamso lso gradientes\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     70\u001b[0m \u001b[39m#y modificamos los pesos\u001b[39;00m\n\u001b[1;32m     71\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
            "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
            "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "#entrenamos la red con la implementacion del pytorch\n",
        "#cargamos los datos1\n",
        "#num_samples=8000\n",
        "#x_train,y_train=get_dataset(num_samples)\n",
        "#x_train=np.expand_dims(x_train,-1)\n",
        "\n",
        "#probamos con los datos2\n",
        "\n",
        "num_samples=6000\n",
        "input_dim=1\n",
        "len_seq=3\n",
        "x_train=np.random.randn(num_samples,len_seq)\n",
        "y_train=x_train[:,::-1].copy()\n",
        "\n",
        "x_train,y_train=np.expand_dims(x_train,-1),np.expand_dims(y_train,-1)\n",
        "\n",
        "x_train,y_train=torch.from_numpy(x_train).float() ,torch.from_numpy(y_train).float()\n",
        "\n",
        "\n",
        "#instanciamos el modelo\n",
        "d_k=10\n",
        "gc.collect()\n",
        "try:\n",
        "    del model\n",
        "except:\n",
        "    pass\n",
        "model = CustomNetwork4_withPosEncoding(input_dim, d_k,input_dim,num_heads=2)\n",
        "\n",
        "#creamos la funcion perdida y el optimizador\n",
        "loss_criterion=nn.MSELoss()\n",
        "optimizer=optim.Adam(model.parameters(),lr=0.001)\n",
        "#scheduler = ExponentialLR(optimizer, 0.95)\n",
        "\n",
        "\n",
        "#creamos los datasets y los dataloaders\n",
        "train_size = int(0.8 * num_samples)\n",
        "val_size = num_samples - train_size\n",
        "train_dataset, val_dataset = random_split(TensorDataset(x_train,y_train), [train_size, val_size])\n",
        "\n",
        "# Crear DataLoaders para entrenamiento y validación\n",
        "batch_size=32\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "epocas=130\n",
        "\n",
        "\n",
        "#hacemos un bucle para entrenamiento\n",
        "for epoca in range(epocas):\n",
        "    #establecemos el modelo en modo entrenamiento\n",
        "    model.train()\n",
        "\n",
        "    #establecemos en 0 el contador de la funcion de perdida\n",
        "    train_loss=0\n",
        "\n",
        "    #bucle para recorrer todos los batches\n",
        "    for batch_x_train,batch_y_train in train_dataloader:\n",
        "        #ponemos a zero los gradientes para el optimizador\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #predecimos\n",
        "\n",
        "        output = model(batch_x_train)\n",
        "\n",
        "        #calculamos la perdida\n",
        "        loss = loss_criterion(output,batch_y_train)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        #retropropagamos/calculamso lso gradientes\n",
        "        loss.backward()\n",
        "        #y modificamos los pesos\n",
        "        optimizer.step()\n",
        "\n",
        "    train_loss /= len(train_dataloader)\n",
        "    print(f\"Epoch {epoca + 1}/{epocas}, Train Loss: {train_loss:.4f}\")\n",
        "\n",
        "\n",
        "    # modo validacion, \n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_x_val,batch_y_val in val_dataloader:\n",
        "            outputs = model(batch_x_val)\n",
        "            loss = loss_criterion(outputs, batch_y_val)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_loss /= len(val_dataloader)\n",
        "    print(f\"Epoch {epoca + 1}/{epocas}, Validation Loss: {val_loss:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-n5EhnNwy3Us",
        "outputId": "c1e440ac-17f6-4255-a16b-92ebab84e84e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[1.1172],\n",
            "         [0.7123]]])\n",
            "tensor([[[0.7095],\n",
            "         [1.1009]]], grad_fn=<ViewBackward0>)\n",
            "tensor([[[[0.4725, 0.5275],\n",
            "          [0.4732, 0.5268]],\n",
            "\n",
            "         [[0.5407, 0.4593],\n",
            "          [0.5401, 0.4599]]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "x=torch.randn(1,len_seq,1).float()\n",
        "print(x)\n",
        "y,att=model(x,return_attention=True,average_attn_weights=False)\n",
        "print(y)\n",
        "print(att)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LutZzp2y3Us",
        "outputId": "0e79ecf8-0f01-4fca-842c-9c3f9d04c0fe"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "Invalid shape (200, 1, 10) for image data",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[200], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[39m.\u001b[39;49mimshow(model\u001b[39m.\u001b[39;49mpositional_encoding)\n",
            "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.10/site-packages/matplotlib/_api/deprecation.py:454\u001b[0m, in \u001b[0;36mmake_keyword_only.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m name_idx:\n\u001b[1;32m    449\u001b[0m     warn_deprecated(\n\u001b[1;32m    450\u001b[0m         since, message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPassing the \u001b[39m\u001b[39m%(name)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%(obj_type)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    451\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpositionally is deprecated since Matplotlib \u001b[39m\u001b[39m%(since)s\u001b[39;00m\u001b[39m; the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mparameter will become keyword-only \u001b[39m\u001b[39m%(removal)s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    453\u001b[0m         name\u001b[39m=\u001b[39mname, obj_type\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 454\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.10/site-packages/matplotlib/pyplot.py:2623\u001b[0m, in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2617\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mimshow)\n\u001b[1;32m   2618\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimshow\u001b[39m(\n\u001b[1;32m   2619\u001b[0m         X, cmap\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, norm\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, aspect\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, interpolation\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   2620\u001b[0m         alpha\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, vmin\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, vmax\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, origin\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, extent\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m,\n\u001b[1;32m   2621\u001b[0m         interpolation_stage\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, filternorm\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, filterrad\u001b[39m=\u001b[39m\u001b[39m4.0\u001b[39m,\n\u001b[1;32m   2622\u001b[0m         resample\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, url\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 2623\u001b[0m     __ret \u001b[39m=\u001b[39m gca()\u001b[39m.\u001b[39;49mimshow(\n\u001b[1;32m   2624\u001b[0m         X, cmap\u001b[39m=\u001b[39;49mcmap, norm\u001b[39m=\u001b[39;49mnorm, aspect\u001b[39m=\u001b[39;49maspect,\n\u001b[1;32m   2625\u001b[0m         interpolation\u001b[39m=\u001b[39;49minterpolation, alpha\u001b[39m=\u001b[39;49malpha, vmin\u001b[39m=\u001b[39;49mvmin,\n\u001b[1;32m   2626\u001b[0m         vmax\u001b[39m=\u001b[39;49mvmax, origin\u001b[39m=\u001b[39;49morigin, extent\u001b[39m=\u001b[39;49mextent,\n\u001b[1;32m   2627\u001b[0m         interpolation_stage\u001b[39m=\u001b[39;49minterpolation_stage,\n\u001b[1;32m   2628\u001b[0m         filternorm\u001b[39m=\u001b[39;49mfilternorm, filterrad\u001b[39m=\u001b[39;49mfilterrad, resample\u001b[39m=\u001b[39;49mresample,\n\u001b[1;32m   2629\u001b[0m         url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m({\u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m: data} \u001b[39mif\u001b[39;49;00m data \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m {}),\n\u001b[1;32m   2630\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2631\u001b[0m     sci(__ret)\n\u001b[1;32m   2632\u001b[0m     \u001b[39mreturn\u001b[39;00m __ret\n",
            "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.10/site-packages/matplotlib/_api/deprecation.py:454\u001b[0m, in \u001b[0;36mmake_keyword_only.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m name_idx:\n\u001b[1;32m    449\u001b[0m     warn_deprecated(\n\u001b[1;32m    450\u001b[0m         since, message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPassing the \u001b[39m\u001b[39m%(name)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%(obj_type)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    451\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpositionally is deprecated since Matplotlib \u001b[39m\u001b[39m%(since)s\u001b[39;00m\u001b[39m; the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mparameter will become keyword-only \u001b[39m\u001b[39m%(removal)s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    453\u001b[0m         name\u001b[39m=\u001b[39mname, obj_type\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 454\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.10/site-packages/matplotlib/__init__.py:1423\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1420\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m   1421\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(ax, \u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1422\u001b[0m     \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1423\u001b[0m         \u001b[39mreturn\u001b[39;00m func(ax, \u001b[39m*\u001b[39;49m\u001b[39mmap\u001b[39;49m(sanitize_sequence, args), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1425\u001b[0m     bound \u001b[39m=\u001b[39m new_sig\u001b[39m.\u001b[39mbind(ax, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1426\u001b[0m     auto_label \u001b[39m=\u001b[39m (bound\u001b[39m.\u001b[39marguments\u001b[39m.\u001b[39mget(label_namer)\n\u001b[1;32m   1427\u001b[0m                   \u001b[39mor\u001b[39;00m bound\u001b[39m.\u001b[39mkwargs\u001b[39m.\u001b[39mget(label_namer))\n",
            "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.10/site-packages/matplotlib/axes/_axes.py:5604\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5596\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_aspect(aspect)\n\u001b[1;32m   5597\u001b[0m im \u001b[39m=\u001b[39m mimage\u001b[39m.\u001b[39mAxesImage(\u001b[39mself\u001b[39m, cmap\u001b[39m=\u001b[39mcmap, norm\u001b[39m=\u001b[39mnorm,\n\u001b[1;32m   5598\u001b[0m                       interpolation\u001b[39m=\u001b[39minterpolation, origin\u001b[39m=\u001b[39morigin,\n\u001b[1;32m   5599\u001b[0m                       extent\u001b[39m=\u001b[39mextent, filternorm\u001b[39m=\u001b[39mfilternorm,\n\u001b[1;32m   5600\u001b[0m                       filterrad\u001b[39m=\u001b[39mfilterrad, resample\u001b[39m=\u001b[39mresample,\n\u001b[1;32m   5601\u001b[0m                       interpolation_stage\u001b[39m=\u001b[39minterpolation_stage,\n\u001b[1;32m   5602\u001b[0m                       \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 5604\u001b[0m im\u001b[39m.\u001b[39;49mset_data(X)\n\u001b[1;32m   5605\u001b[0m im\u001b[39m.\u001b[39mset_alpha(alpha)\n\u001b[1;32m   5606\u001b[0m \u001b[39mif\u001b[39;00m im\u001b[39m.\u001b[39mget_clip_path() \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   5607\u001b[0m     \u001b[39m# image does not already have clipping set, clip to axes patch\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.10/site-packages/matplotlib/image.py:710\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    706\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A[:, :, \u001b[39m0\u001b[39m]\n\u001b[1;32m    708\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m    709\u001b[0m         \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39min\u001b[39;00m [\u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m]):\n\u001b[0;32m--> 710\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid shape \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m for image data\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    711\u001b[0m                     \u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mshape))\n\u001b[1;32m    713\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[1;32m    714\u001b[0m     \u001b[39m# If the input data has values outside the valid range (after\u001b[39;00m\n\u001b[1;32m    715\u001b[0m     \u001b[39m# normalisation), we issue a warning and then clip X to the bounds\u001b[39;00m\n\u001b[1;32m    716\u001b[0m     \u001b[39m# - otherwise casting wraps extreme values, hiding outliers and\u001b[39;00m\n\u001b[1;32m    717\u001b[0m     \u001b[39m# making reliable interpretation impossible.\u001b[39;00m\n\u001b[1;32m    718\u001b[0m     high \u001b[39m=\u001b[39m \u001b[39m255\u001b[39m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39missubdtype(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype, np\u001b[39m.\u001b[39minteger) \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n",
            "\u001b[0;31mTypeError\u001b[0m: Invalid shape (200, 1, 10) for image data"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.imshow(model.positional_encoding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGMyj1I2y3Us"
      },
      "source": [
        "### Implementación seq2seq con GRU y attencion Bahnaudsdfsasd "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BF3T0tgxy3Us"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        outputs, hidden = self.gru(embedded)\n",
        "        return outputs, hidden\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Attention, self).__init__()\n",
        "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        attn_weights = torch.sum(hidden * encoder_outputs, dim=2)\n",
        "        attn_weights = F.softmax(attn_weights, dim=1).unsqueeze(1)\n",
        "        context = attn_weights.bmm(encoder_outputs)\n",
        "        return context, attn_weights\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_size, hidden_size, num_layers=1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.output_size = output_size  # Agrega esta línea\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size * 2, hidden_size, num_layers, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x, hidden, context):\n",
        "        x = self.embedding(x)\n",
        "        x = torch.cat((x, context), dim=2)\n",
        "        output, hidden = self.gru(x, hidden)\n",
        "        output = self.softmax(self.out(output.squeeze(1)))\n",
        "        return output, hidden\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, attention, decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.attention = attention\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        batch_size = src.size(0)\n",
        "        max_len = trg.size(1)\n",
        "        trg_vocab_size = self.decoder.output_size\n",
        "\n",
        "        outputs = torch.zeros(batch_size, max_len, trg_vocab_size).to(src.device)\n",
        "\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "\n",
        "        x = trg[:, 0]\n",
        "\n",
        "        for t in range(1, max_len):\n",
        "            context, _ = self.attention(hidden, encoder_outputs)\n",
        "            output, hidden = self.decoder(x.unsqueeze(1), hidden, context)\n",
        "            outputs[:, t] = output\n",
        "\n",
        "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
        "            x = (trg[:, t] if teacher_force else output.argmax(1))\n",
        "\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBWdVXpqy3Ut",
        "outputId": "d2bf0102-c20d-4e64-f184-76cf8c43d23c"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (32) must match the size of tensor b (10) at non-singleton dimension 1",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[74], line 67\u001b[0m\n\u001b[1;32m     64\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss(ignore_index\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m---> 67\u001b[0m     train_loss \u001b[39m=\u001b[39m train(seq2seq, train_dataloader, optimizer, criterion, device)\n\u001b[1;32m     68\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m02\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m | Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
            "Cell \u001b[0;32mIn[74], line 30\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     27\u001b[0m src, trg \u001b[39m=\u001b[39m src\u001b[39m.\u001b[39mto(device), trg\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     29\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 30\u001b[0m output \u001b[39m=\u001b[39m model(src, trg)\n\u001b[1;32m     32\u001b[0m output \u001b[39m=\u001b[39m output[:, \u001b[39m1\u001b[39m:]\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, output\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m     33\u001b[0m trg \u001b[39m=\u001b[39m trg[:, \u001b[39m1\u001b[39m:]\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
            "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "Cell \u001b[0;32mIn[73], line 70\u001b[0m, in \u001b[0;36mSeq2Seq.forward\u001b[0;34m(self, src, trg, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     67\u001b[0m x \u001b[39m=\u001b[39m trg[:, \u001b[39m0\u001b[39m]\n\u001b[1;32m     69\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, max_len):\n\u001b[0;32m---> 70\u001b[0m     context, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(hidden, encoder_outputs)\n\u001b[1;32m     71\u001b[0m     output, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(x\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m), hidden, context)\n\u001b[1;32m     72\u001b[0m     outputs[:, t] \u001b[39m=\u001b[39m output\n",
            "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "Cell \u001b[0;32mIn[73], line 26\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden, encoder_outputs):\n\u001b[0;32m---> 26\u001b[0m     attn_weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(hidden \u001b[39m*\u001b[39;49m encoder_outputs, dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m     27\u001b[0m     attn_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(attn_weights, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m     28\u001b[0m     context \u001b[39m=\u001b[39m attn_weights\u001b[39m.\u001b[39mbmm(encoder_outputs)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (32) must match the size of tensor b (10) at non-singleton dimension 1"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class VectorInversionDataset(Dataset):\n",
        "    def __init__(self, vector_length, dataset_size):\n",
        "        self.vector_length = vector_length\n",
        "        self.dataset_size = dataset_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.randint(1, 10, (self.vector_length,))\n",
        "        y = torch.flip(x, [0])\n",
        "        return x, y\n",
        "\n",
        "# Ajusta la línea en la clase Seq2Seq\n",
        "\n",
        "# Modifica la función de entrenamiento\n",
        "def train(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for src, trg in dataloader:\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, trg)\n",
        "\n",
        "        output = output[:, 1:].reshape(-1, output.shape[-1])\n",
        "        trg = trg[:, 1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(dataloader)\n",
        "\n",
        "\n",
        "vector_length = 10\n",
        "dataset_size = 1000\n",
        "batch_size = 32\n",
        "epochs = 50\n",
        "\n",
        "input_size = output_size = 10  # Rango de valores en el vector (1 a 10)\n",
        "hidden_size = 256\n",
        "num_layers = 1\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_dataset = VectorInversionDataset(vector_length, dataset_size)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "encoder = Encoder(input_size, hidden_size, num_layers).to(device)\n",
        "attention = Attention(hidden_size).to(device)\n",
        "decoder = Decoder(output_size, hidden_size, num_layers).to(device)\n",
        "seq2seq = Seq2Seq(encoder, attention, decoder).to(device)\n",
        "\n",
        "optimizer = optim.Adam(seq2seq.parameters())\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_loss = train(seq2seq, train_dataloader, optimizer, criterion, device)\n",
        "    print(f'Epoch: {epoch+1:02} | Loss: {train_loss:.3f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oVoGV922y3Ut"
      },
      "outputs": [],
      "source": [
        "#implementacion de un seq2seq con atencion\n",
        "\n",
        "class EncoderRNN(nn.Module):\n",
        "    #esto nos va a devolver un output y un hidden state\n",
        "    #tenemos tambien un init_hidden para inicializar el hidden state\n",
        "    #nos interesa el tamaño del input y del hidden state (el output tiene tamaño del hidden)\n",
        "    def __init__(self,d_input,d_hidden):\n",
        "        super().__init__()\n",
        "        self.d_input=d_input\n",
        "        self.d_hidden=d_hidden\n",
        "        #vamos a crear un embeding propio con una densa antes de la RNN \n",
        "\n",
        "        self.dense1=nn.Linear(d_input,d_hidden)\n",
        "        self.rnn=nn.GRU(d_hidden,d_hidden,batch_first=True) #como no ponemos batch_first=True todo sera de dimension dim: len x hidden size\n",
        "\n",
        "    def forward(self,x,hidden):\n",
        "        x=self.dense1(x) # pasamos B x 1 x input y obtenemos B x 1 x hidden_dim\n",
        "        output,hidden_out=self.rnn(x,hidden) #obtenemos B x 1 x hidden_dim y 1 x B x hidden_dim \n",
        "        return output,hidden_out #obtenemos B x 1 x hidden_dim y 1 x B x hidden_dim\n",
        "    \n",
        "    def init_hidden(self,batch_size=1):\n",
        "        return torch.zeros(1,batch_size,self.d_hidden)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zL8c12Gqy3Ut",
        "outputId": "8a8ca5a2-4527-43e6-cdfc-979db2925c7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([5, 1, 15]) torch.Size([1, 5, 15])\n"
          ]
        }
      ],
      "source": [
        "#provemos lo anterior\n",
        "encoder=EncoderRNN(1,15)\n",
        "test_vector=torch.randn(5,1,1)\n",
        "output,attn=encoder(test_vector,encoder.init_hidden(5))\n",
        "print(output.shape,attn.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "aaV4mL_Oy3Ut"
      },
      "outputs": [],
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    #este decoder va a tener implementada la atencion\n",
        "    #empezaremos el decoder con un hidden_state 0 igual que antes\n",
        "    #el hidden state previo se le va a meter a modo de query al modelo de atencion, que va a recibir tambien los values, que \n",
        "    #son los estados hidden del encoder, de modo que lo ejecutamos en el init de aqui, el encoder\n",
        "    def __init__(self,input_dim,hidden_size):\n",
        "        super().__init__()\n",
        "        self.d_input=input_dim\n",
        "        self.d_output=input_dim\n",
        "        self.d_hidden=hidden_size\n",
        "        #un enmbeding para el input y una densa para el output\n",
        "        self.dense_in=nn.Linear(input_dim,hidden_size)\n",
        "        self.dense_out1=nn.Linear(hidden_size*2,hidden_size)\n",
        "        self.dense_out2=nn.Linear(hidden_size,input_dim) # asumienod dim_input = dim_output\n",
        "\n",
        "        self.rnn=nn.GRU(hidden_size,hidden_size,batch_first=True)#como no ponemos batch_first=True todo sera de dimension dim: len x hidden size\n",
        "\n",
        "        #el mecanismo de atencion y los hidden states del encoder\n",
        "        self.attn=Attention()\n",
        "\n",
        "    def forward(self,prev_input,hidden_state,encoder_outputs):\n",
        "        #hacer forward de esto es partiendo del hidden state anterior, sacar el nuevo\n",
        "        #y con ese calcular la atencion y el context vector, y luego juntarlo con el hidden state y meterlo en la dense y obtener el nuedo hidden\n",
        "\n",
        "        #el input debe de pasar por el embeding\n",
        "        x=self.dense_in(prev_input) # entra B x 1 x input_dim y sale B x 1 x Hidden_dim, el 1 es porque estamos procesando un tocken solo\n",
        "        #ahora calculamos el hidden state\n",
        "        _,new_hidden_state=self.rnn(x,hidden_state) #entran y_(t-1)=B x 1 x Hidden_dim y h_(t-1)= 1 x Batch_size x Hidden_dim y salen y_t = B x 1 x Hidden_dim y h_t = 1 x B x Hidden_dim\n",
        "\n",
        "        #con este hidden state,calculamos la attencion, se calcula en cada dimension batch independiente\n",
        "        #si el encoder_outputs es:  Batch xlen_seq x hidden_dim y el hidden state que usamo de Q es 1 x B x Hidden_dim, devolvemos un context que es B x 1 x hidden_dim  y attn_weights B x 1 x len_seq\n",
        "        context,attn_weigths=self.attn(new_hidden_state,encoder_outputs) # return  B x 1 x hidden_dim y B x 1 x len_seq \n",
        "        new_hidden_state_transpose=new_hidden_state.transpose(0,1)\n",
        "        #con este context y con el hiden state creamos un vector para meter en una dense\n",
        "        out=self.dense_out1(torch.cat((new_hidden_state_transpose,context),dim=2))# le metemos B x 1 x 2*hidden_dim y sacamos B x 1 x hidden_dim\n",
        "        out=self.dense_out2(out) #metemos B x 1 x hidden_dim y sacamos B x 1 x input_dim\n",
        "\n",
        "        return out,new_hidden_state, attn_weigths #B x 1 x input_dim , 1 x B x Hidden_dim y B x 1 x len_seq\n",
        "    \n",
        "    def init_hidden(self,batch_size=1):\n",
        "        return torch.zeros(1,batch_size,self.d_hidden)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jO3JDU8ay3Ut",
        "outputId": "7cc11240-10e0-4a88-c188-dded3e8a8467"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[[-0.1685]]], grad_fn=<ViewBackward0>),\n",
              " tensor([[[-0.3516, -0.1402, -0.3676, -0.5432, -0.8043,  0.0096, -1.0972,\n",
              "            0.3669, -0.8808,  0.4729,  0.8387, -0.0459,  0.7664,  0.5486,\n",
              "           -0.7034]]], grad_fn=<StackBackward0>),\n",
              " tensor([[[0.0062, 0.2421, 0.0382, 0.0009, 0.7126]]],\n",
              "        grad_fn=<SoftmaxBackward0>))"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "decoder=DecoderRNN(1,15)\n",
        "encoder_outputs=torch.randn(1,5,15)\n",
        "hidden=torch.randn(1,1,15)\n",
        "prev_input=torch.randn(1,1,1)\n",
        "\n",
        "decoder(prev_input,hidden,encoder_outputs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0eNmV2tLy3Uu"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    #este es el mecanismo de atencion, el dot product basicamente\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "    def forward(self,hidden_state,encoder_outputs):\n",
        "        #si el encoder_outputs es:  Batch x len_seq x hidden_dim y el hidden state que usamo de Q es 1 x B x Hidden_dim, devolvemos un context que es B x 1 x hidden_dim  y attn_weights B x 1 x len_seq\n",
        "\n",
        "        #transponemos el hidden state para hacer las multiplicaciones sobre los batches\n",
        "\n",
        "        hidden_state_transpose=hidden_state.transpose(0,1) # ahora es B x 1 x hidden\n",
        "        #el objetivo es multiplicarlo por el encoder_outputs que es  Batch x len_seq x hidden_dim, y para ello lo tenemos que transponer\n",
        "        encoder_outputs_transpose=encoder_outputs.transpose(1,2) # Batch x  hidden_dim x len_seq \n",
        "        #aqui calculamos el alignement\n",
        "\n",
        "        e=torch.bmm(hidden_state_transpose,encoder_outputs_transpose) # B x 1 x len_seq\n",
        "\n",
        "        att_weights=F.softmax(e,dim=2) # B x 1 x len_seq y suman 1 sobre los elementos en esa dimension\n",
        "        context=torch.bmm(att_weights,encoder_outputs) # hacemos la multiplicacion para obtener B x 1 x hidden_dim\n",
        "        return context, att_weights # return  B x 1 x hidden_dim y B x 1 x len_seq \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkKUF-VBy3Uu"
      },
      "source": [
        "Tenemos un modulo que toma in input y podemos obtener de ahi un vector de sus estados ocultos.\n",
        "\n",
        "Luego tenemos un modulo que toma un hidden state, un input y un nos calcula la attencion y nos termina dando un output, un hidden state y unas atenciones calculadas sobre el vector de entrada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zwu3Tp4xy3Uu"
      },
      "outputs": [],
      "source": [
        "#funcion que nos da los datos \n",
        "\n",
        "class FlipData(Dataset):\n",
        "    def __init__(self,size_dataset,len_seq,d_input):\n",
        "        super().__init__()\n",
        "        self.size_dataset=size_dataset\n",
        "        self.len_seq=len_seq\n",
        "        self.d_input=d_input\n",
        "\n",
        "    def __getitem__(self,indx):\n",
        "        #cada vez que la llamamos nos devuelve un \n",
        "        x=torch.randint(0,100,(self.len_seq,self.d_input)).float()\n",
        "        y=x.flip(0)\n",
        "        return x, y \n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.size_dataset\n",
        "\n",
        "dataset=FlipData(2000,10,1)\n",
        "dataloader=DataLoader(dataset,batch_size=32,shuffle=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "p78Wn_9Oy3Uu"
      },
      "outputs": [],
      "source": [
        "#podemos crear un datasetIterable tbn\n",
        "class IterFlipData(IterableDataset):\n",
        "    def __init__(self,len_seq,max_size):\n",
        "        super().__init__()\n",
        "        self.len_seq=len_seq\n",
        "        self.max_size=max_size\n",
        "\n",
        "    def __iter__(self):\n",
        "        for _ in range(self.max_size):\n",
        "            x=torch.randint(0,100,(self.len_seq,1))\n",
        "            y=x.flip(0)\n",
        "            yield x,y    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkdBvJimy3Uu"
      },
      "outputs": [],
      "source": [
        "dataset=IterFlipData(10,200)\n",
        "dataloader=DataLoader(dataset,batch_size=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sq_3V7Sy3Uv"
      },
      "outputs": [],
      "source": [
        "encoder=EncoderRNN(d_input=1,d_hidden=15)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WCskCiay3Uv",
        "outputId": "ae5e4810-5407-4134-e08a-405b9f61b2b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 10, 15]) torch.Size([1, 1, 15])\n"
          ]
        }
      ],
      "source": [
        "a=torch.randn(1,10,1)\n",
        "o,h=encoder(a,encoder.init_hidden(1))\n",
        "print(o.shape,h.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKzKnfVdy3Uv",
        "outputId": "51ad411f-b134-44e4-8246-31c9351476ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 :  541.5908813476562\n",
            "10 :  447.4613342285156\n",
            "20 :  461.4779052734375\n",
            "30 :  402.0994873046875\n"
          ]
        }
      ],
      "source": [
        "#entrenamiento\n",
        "N=40\n",
        "epochs=np.arange(N)\n",
        "\n",
        "len_seq=10\n",
        "batch_dim=32\n",
        "d_input=1\n",
        "dataset=FlipData(10000,len_seq,d_input)\n",
        "hidden_dim=32\n",
        "dataloader=DataLoader(dataset,batch_size=batch_dim,shuffle=True)\n",
        "\n",
        "#creamos los modelos\n",
        "encoder=EncoderRNN(d_input=d_input,d_hidden=hidden_dim)\n",
        "decoder=DecoderRNN(input_dim=d_input,hidden_size=hidden_dim)\n",
        "#primero inicializamos todo lo de los optimizadores y mierdas\n",
        "loss_criterion=nn.MSELoss()\n",
        "\n",
        "encoder_optim=optim.Adam(encoder.parameters(),lr=0.01)\n",
        "decoder_optim=optim.Adam(decoder.parameters(),lr=0.01)\n",
        "#clip_grad_norm_(encoder.parameters(), 1)\n",
        "#clip_grad_norm_(decoder.parameters(), 1)\n",
        "\n",
        "for epoch in epochs:\n",
        "    #para cada epoca entrenamos para todos los datos que tengamos disponibles\n",
        "    #los datos vienen de un dataloader(dataset)\n",
        "    for indx,(input_tensor,target_tensor) in enumerate(dataloader):\n",
        "        #primero pasamos el input por el encoder\n",
        "        batch_size,len_seq,input_size=input_tensor.size()\n",
        "        encoder_outputs=torch.zeros(batch_size,len_seq,hidden_dim)\n",
        "\n",
        "        aux_hidden_state=encoder.init_hidden(batch_size=batch_size)\n",
        "        #lo podenemos en modo entrenar\n",
        "\n",
        "        with torch.set_grad_enabled(True):\n",
        "            encoder_optim.zero_grad()\n",
        "            decoder_optim.zero_grad()\n",
        "            loss=torch.tensor(0.0)\n",
        "            #ahora un bucle para sacar todos los hidden states del encoder\n",
        "            for jndx in range(len_seq):\n",
        "                _,encoder_hidden_aux=encoder(input_tensor[:,[jndx]],aux_hidden_state) # metemos B x 1 x input_dim y 1 x B x hidden_dim, y obtenemos B x 1 x hidden_dim y 1 x B x hidden_dim\n",
        "                encoder_outputs[:,[jndx],:]=encoder_hidden_aux.transpose(0,1) #tenemos que meter encoder_hidden_aux = 1 x B x hidden_dim en un vecto de batch_size x len_seq x hidden_dim\n",
        "                aux_hidden_state=encoder_hidden_aux\n",
        "    \n",
        "        #antes de \n",
        "    \n",
        "        #una vez tenemos esto calculado, podemos proceder con el proceso autoregresivo de la prediccion\n",
        "        #el proceso es el siguiente, hiden state vacio en el decoder, y \n",
        "            #el primer input será la el GO Token y el hidden vector el 0\n",
        "            aux_hidden_state_decoder=decoder.init_hidden(batch_size=batch_size)\n",
        "            prev_input=torch.zeros(batch_size,1,input_size)\n",
        "            _,len_seq_target,output_size=target_tensor.size()\n",
        "            for kndx in range(len_seq_target):\n",
        "                out,new_hidden_state,attn_weigths=decoder(prev_input,aux_hidden_state_decoder,encoder_outputs)# B x 1 x input , 1 x B x hidden , B x len x hidden\n",
        "                a=loss_criterion(out,target_tensor[:,[kndx]])\n",
        "                loss+=a\n",
        "                prev_input=target_tensor[:,[kndx]]\n",
        "\n",
        "            #una vez que hemos calculado el los sobre todo un batch, hacemos backprop\n",
        "            loss.backward()\n",
        "            decoder_optim.step()\n",
        "            encoder_optim.step()\n",
        "    if (epoch % 10)==0:\n",
        "        print(epoch, \": \",loss.item()/batch_size)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "CmlzK3rqytWt"
      },
      "outputs": [],
      "source": [
        "def infer(encoder,decoder,x):\n",
        "    result=[]\n",
        "    batch_size,len_seq,input_size=x.size()\n",
        "    hidden_size=encoder.d_hidden\n",
        "\n",
        "    encoder_outputs=torch.zeros(batch_size,len_seq,hidden_size)\n",
        "\n",
        "    aux_hidden_state=encoder.init_hidden(batch_size=batch_size)\n",
        "\n",
        "    with torch.set_grad_enabled(False):\n",
        "        #ahora un bucle para sacar todos los hidden states del encoder\n",
        "        for jndx in range(len_seq):\n",
        "            _,encoder_hidden_aux=encoder(x[:,[jndx]],aux_hidden_state) # metemos B x 1 x input_dim y 1 x B x hidden_dim, y obtenemos B x 1 x hidden_dim y 1 x B x hidden_dim\n",
        "            encoder_outputs[:,[jndx],:]=encoder_hidden_aux.transpose(0,1) #tenemos que meter encoder_hidden_aux = 1 x B x hidden_dim en un vecto de batch_size x len_seq x hidden_dim\n",
        "            aux_hidden_state=encoder_hidden_aux\n",
        "            #sacamos los vectores ocultos del encoder\n",
        "\n",
        "        aux_hidden_state_decoder=decoder.init_hidden(batch_size=batch_size)\n",
        "        prev_input=torch.ones(batch_size,1,input_size)\n",
        "        for kndx in range(len_seq):\n",
        "            out,new_hidden_state,attn_weigths=decoder(prev_input,aux_hidden_state_decoder,encoder_outputs)# B x 1 x input , 1 x B x hidden , B x len x hidden\n",
        "            result.append(out.detach().numpy().item())\n",
        "            prev_input=out\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "84j8nD1pnFae"
      },
      "outputs": [],
      "source": [
        "x=torch.randint(0,100,(1,3,1)).float()\n",
        "y=infer(encoder,decoder,x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jrjal0751DsT",
        "outputId": "bae4e9fe-2f90-4d23-d9ad-4f2e23246585"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[34. 10.  2.]\n"
          ]
        }
      ],
      "source": [
        "print(x.detach().numpy().squeeze())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehP-BloK1SNt",
        "outputId": "9dc5f9fc-f2fa-435f-dce6-3e491fa9b37a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.5889452695846558, -0.021090185269713402, 30.18843650817871]"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "niYdgk1Cy3Uw"
      },
      "outputs": [],
      "source": [
        "N=1\n",
        "epochs=np.arange(N)\n",
        "\n",
        "len_seq=10\n",
        "batch_dim=5\n",
        "d_input=1\n",
        "dataset=FlipData(2000,len_seq,d_input)\n",
        "hidden_dim=15\n",
        "dataloader=DataLoader(dataset,batch_size=batch_dim,shuffle=True)\n",
        "\n",
        "#creamos los modelos\n",
        "encoder=EncoderRNN(d_input=d_input,d_hidden=hidden_dim)\n",
        "decoder=DecoderRNN(input_dim=d_input,hidden_size=hidden_dim)\n",
        "#primero inicializamos todo lo de los optimizadores y mierdas\n",
        "loss_criterion=nn.MSELoss()\n",
        "\n",
        "for indx,(input_tensor,target_tensor) in enumerate(dataloader):\n",
        "    #primero pasamos el input por el encoder\n",
        "    batch_size,len_seq,input_size=input_tensor.size()\n",
        "    encoder_outputs=torch.zeros(batch_size,len_seq,hidden_dim)\n",
        "\n",
        "    aux_hidden_state=encoder.init_hidden(batch_size=batch_size)\n",
        "    #lo podenemos en modo entrenar\n",
        "\n",
        "    loss=torch.tensor(0.0)\n",
        "\n",
        "    with torch.set_grad_enabled(True):\n",
        "        #ahora un bucle para sacar todos los hidden states del encoder\n",
        "        for jndx in range(len_seq):\n",
        "            \n",
        "            _,encoder_hidden_aux=encoder(input_tensor[:,[jndx]],aux_hidden_state) # metemos B x 1 x input_dim y 1 x B x hidden_dim, y obtenemos B x 1 x hidden_dim y 1 x B x hidden_dim\n",
        "            encoder_outputs[:,[jndx],:]=encoder_hidden_aux.transpose(0,1) #tenemos que meter encoder_hidden_aux = 1 x B x hidden_dim en un vecto de batch_size x len_seq x hidden_dim\n",
        "            aux_hidden_state=encoder_hidden_aux\n",
        "\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBZslSb_RdEw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "5274204275de82dbc48cd1392c949d64560abc714158f13f8296624993604764"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
